[
  {
    "text": "## Company Intel to Reference Casually\n  Raptor Maps Background\n  Who Their Customers Are\n  The Problems They're Solving\n  How Raptor Solves These Problems\n  Raptor Solar Sentry (Robotics Integration)\n  Raptor Comply (NERC Compliance Automation)\n  Market Context & Tailwinds\n",
    "source_id": "2d3a97f0-fe30-803c-a242-ea3f2fbe859a",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "First interview with Peter (1)"
    ],
    "title": "First interview with Peter (1)"
  },
  {
    "text": "## Your 3-Minute Pitch (More Detailed)\n  Starter:\n  \"I'm a entrepreneurial product manager with 9+ years building 0-to-1 products in AI and asset management. Most recently, I founded Kunik, an AI-powered data marketplace serving institutional investors and hedge funds.\"\n  Key points to weave in:\n  Kunik specifics: Built real-time dashboards tracking usage, revenue by query/source, data lineage, SOC 2 audit logs\n  Closed $100K+ in paid pilots and secured 2 LOIs from data providers after private beta\n  Warner context: Managed enterprise DAM platform and content delivery infrastructure serving 500+ users across 40+ countries\n  Implemented ML-powered workflows for media asset management at scale\n  Why now: Looking for 0-to-1 contract role in climate tech where asset management expertise compounds\n  Raptor fit: Asset management at Warner translates directly to energy asset workflows; founder mentality means I can ship without handholdinglibrary\n",
    "source_id": "2d3a97f0-fe30-803c-a242-ea3f2fbe859a",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "First interview with Peter (1)"
    ],
    "title": "First interview with Peter (1)"
  },
  {
    "text": "## STAR Stories (Pick 2-3 Based on Conversation)\n  Story 1: Building Growth Infrastructure from Scratch\n  Story 2: Scaling Through Customer Feedback Loops\n  Story 3: Building AI-Native Workflows\n  Story 4: Product-Led Sales Execution\n  Story 5: Managing Integrations at Enterprise Scale\n  New STAR Story: Growth Execution at Fox/Imbr\n",
    "source_id": "2d3a97f0-fe30-803c-a242-ea3f2fbe859a",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "First interview with Peter (1)"
    ],
    "title": "First interview with Peter (1)"
  },
  {
    "text": "## Addressing Your Gaps (With Confidence)\n  \"You don't have a Growth PM title\"\n  \"You have no solar/energy experience\"\n  \"Limited marketing execution at scale\"\n",
    "source_id": "2d3a97f0-fe30-803c-a242-ea3f2fbe859a",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "First interview with Peter (1)"
    ],
    "title": "First interview with Peter (1)"
  },
  {
    "text": "## Why Raptor Maps (Full Version)\n  Starter:\n  \"I'm genuinely excited about this opportunity. Raptor Comply is solving a real problem\u2014compliance teams drowning in manual work\u2014and you're at the exact stage where I thrive.\"\n  I've spent my career building products that solve real problems, but I want to work on problems that matter at scale. Climate is the defining challenge of our generation, and energy infrastructure is at the heart of it. Raptor Comply is solving operational pain for the people actually managing our energy transition. That's work I can get behind. Plus, this is the perfect stage for me: 0-to-1 product, tight customer feedback loops, and a chance to learn a new domain where my asset management expertise transfers directly.\"\n  Key points:\n  0-to-1 stage: Tight iteration cycles, product-GTM integration, hypothesis-driven experimentationlibrary\n  Mission-driven: Climate tech solving real operational pain for energy operators\n  Direct skill transfer: Asset management at Warner (media) translates to energy asset workflowslibrary\n  Founder experience: Built SOC 2 compliance, activation dashboards, customer feedback loops\u2014exactly what this role needs\n  Contract structure: Flexibility to focus on high-impact problems without multi-year commitment\n  AI-native: Raptor is building AI-augmented workflows; I've built AI-powered products from scratch\n",
    "source_id": "2d3a97f0-fe30-803c-a242-ea3f2fbe859a",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "First interview with Peter (1)"
    ],
    "title": "First interview with Peter (1)"
  },
  {
    "text": "## Your Questions for Them (Strategic)\nWhat would make this contract role successful enough that you\u2019d want to extend or convert it to full-time?\u201d\nActivation deep-dive:\n  \"What does activation success look like for Raptor Comply's first 100 customers? Are we optimizing for time-to-first-evidence-upload, feature adoption breadth, or something else?\"\nProduct-GTM integration:\n  \"How tightly coupled are product validation and GTM right now? Should I expect to be running experiments, closing pilots, and feeding insights back into roadmap prioritization simultaneously?\"\nCustomer surprise:\n  \"What's been the biggest surprise in customer feedback so far? Where have compliance teams' pain points differed from your initial hypotheses?\"\nAI-native operating model:\n  \"How do you see the AI-native operating model evolving here? Are we building agent-augmented workflows for internal ops, customer-facing features, or both?\"library\nSuccess criteria:\n  \"What would make this contract role successful enough that you'd want to extend or convert it to full-time?\"\n",
    "source_id": "2d3a97f0-fe30-803c-a242-ea3f2fbe859a",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "First interview with Peter (1)"
    ],
    "title": "First interview with Peter (1)"
  },
  {
    "text": "## Strong Closing Statement\nWhen they ask \"Any final thoughts?\":\n\"I want to reiterate why I think this is a great fit. You need someone who can move fast with limited resources, talk to customers, build the growth engine, and validate product-market fit. That's exactly what I did at Kunik\u2014$100K in pilots, 2 LOIs, built the entire infrastructure from scratch. My Warner experience gives me the asset management foundation, my founder experience gives me the autonomy and bias-for-action, and I've literally built SOC 2 compliance systems before. I want this contract structure because it lets me focus on high-impact work without a multi-year commitment, and you get someone who can start shipping immediately. I'd love to help you scale Raptor Comply.\"library\n",
    "source_id": "2d3a97f0-fe30-803c-a242-ea3f2fbe859a",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "First interview with Peter (1)"
    ],
    "title": "First interview with Peter (1)"
  },
  {
    "text": "## Negotiation (Post-Offer Only)\nTarget: $11K/month (from $8-10K range)\nScript:\n\"I really appreciate the offer. Based on the scope\u2014I'll be owning product feedback, running growth experiments, and executing marketing simultaneously\u2014and my 9+ years of PM experience including two 0-to-1 startups, I was hoping for $11K/month. I'm bringing founder-level autonomy and the ability to ship without handholding, which accelerates your timeline to product-market fit. Does that work?\"\nFallback if they hold firm:\n\"I understand the budget constraints. Would you be open to a performance milestone? For example, if I hit activation targets within the first 90 days, we bump to $11K for the remainder of the contract?\"\nPractice bullets out loud 2-3 times. Pick your strongest 2 stories based on what they care about.\nhttps://www.perplexity.ai/search/2c448445-51fb-432c-9ae4-4fb0fa1ce05a\n",
    "source_id": "2d3a97f0-fe30-803c-a242-ea3f2fbe859a",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "First interview with Peter (1)"
    ],
    "title": "First interview with Peter (1)"
  },
  {
    "text": "Slide 1: Trial Onboarding & Activation Plan\nActivation Metric: Compliance Exposure Score calculated within 7 days\nDay-1 Onboarding Flow:\nMinute 0-2: Facility Profile Quiz (5 questions \u2192 segment into Low/Medium/Category 2 IBR)\nMinute 2-5: Instant Exposure Report (applicable standards, deadlines, gap estimate, penalty exposure)\nMinute 5-15: Guided Asset Inventory (progressive disclosure, Quick Start defaults available)\nDays 2-3: Policy Configuration with pre-selected recommendations\nDays 4-7: First Task Assignment + Team Invitation\nTop 3 Drop-Off Risks:\nSlide 2: 30-Day Growth Execution Plan\nPre-Budget Move: Targeted outreach to Raptor Maps' existing customers managing NERC-registered assets (zero cost, warmest leads)\n3 Marketing Experiments ($5K Budget):\nWeek-by-Week Plan:\nMetrics Dashboard (Funnel Order):\nImpressions by Channel\nClick-Through Rate by Channel\nQuiz Completions\nTrial Signups (Target: 150+)\nCost per Trial Signup (Target: <$35)\nActivation Rate (Target: 40%+)\nTime to Activation (Target: <2 days median)\nTrial to Paid Conversion Rate (establish tracking)\nSlide 3: LinkedIn Post\nPost Copy:\nIn 150 days, NERC CIP enforcement expands to 1,000+ renewable energy facilities that have never faced federal cybersecurity mandates.\nIf you operate solar, wind, or battery storage assets between 20-75 MVA, you're likely in scope for Category 2 IBR requirements starting May 2026.\nThe question isn't whether you need a compliance program. It's whether you have time to build one.\nMost utilities spend $110K+ and 6+ months standing up a NERC CIP program from scratch. That timeline doesn't work when you have 150 days.\nWe built a 5-minute Compliance Exposure Calculator to help you understand exactly where you stand:\n\u2192 Are you subject to the new IBR requirements?\n\u2192 Which CIP standards apply to your facility type?\n\u2192 What's your current compliance gap?\n\u2192 What deadlines are you racing against?\nNo sales pitch. No commitment. Just clarity on what May 2026 means for your operations.\n[Link: See Your Compliance Exposure \u2192]\nCreative Brief:\nStatic image with countdown timer showing \"150 DAYS\" prominently. Subhead: \"Until NERC CIP expands to 1,000+ renewable facilities.\" Simple checklist below: \"Am I in scope? \u2713 | Which standards apply? \u2713 | What's my gap? \u2713\" with CTA \"Get your exposure score in 5 minutes.\" Brand colors (blues/greens), 1200 x 627 pixels.\nPaste that into Gamma. It should generate 3 clean slides. Then your Notion doc is your talk-over script where you explain the thinking behind each decision.\n",
    "source_id": "2d3a97f0-fe30-80c0-8396-e1a00f386c13",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Slide content (1)"
    ],
    "title": "Slide content (1)"
  },
  {
    "text": "Raptor Comply represents a strategic expansion by MIT-founded solar software company Raptor Maps into the NERC CIP compliance market, launching October 30, 2025 to address a regulatory inflection point: May 2026 enforcement deadlines that will bring over 1,000 previously exempt renewable energy facilities under federal cybersecurity mandates. The platform targets small-to-medium power generators (20-1500+ MW) struggling with fragmented compliance tools, offering purpose-built automation at $839-$2,490/month versus enterprise GRC platforms requiring months of implementation and significantly higher costs.\n",
    "source_id": "2d3a97f0-fe30-8162-bb59-da3338daccc0",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Raptor Comply: Strategic Context Brief"
    ],
    "title": "Raptor Comply: Strategic Context Brief"
  },
  {
    "text": "## Company origins trace from MIT aerospace labs to solar dominance\nRaptor Maps was founded in 2015 by three MIT aerospace engineers\u2014Nikhil Vadhavkar (CEO, PhD candidate in bioastronautics), Eddie Obropta (CTO, B.S./M.S. in Aeronautics and Astronautics), and Forrest Meyen\u2014in the MIT Man-Vehicle Laboratory. The company won the MIT $100K Entrepreneurship Competition grand prize in May 2015, initially targeting precision agriculture through drone-based thermal imaging.\nThe pivot to solar came organically when solar farm operators discovered the platform's thermal imaging capabilities. Working from Greentown Labs (North America's largest cleantech incubator), the founders recognized stronger product-market fit in solar O&M, where operators were purchasing $20,000-30,000 drones but lacking software to analyze inspection data at scale. The company graduated from Y Combinator (Summer 2016) and has since raised approximately $62-65 million across multiple rounds:\nSeries A (August 2020): $5M led by Blue Bear Capital and Data Point Capital\nSeries B (April 2022): $22M led by MacKinnon, Bennett & Co., with participation from Microsoft Climate Innovation Fund\nSeries C (December 2024): $35M led by Maverix Private Equity\nToday Raptor Maps monitors approximately 125 GW of solar assets across 45+ countries, covering roughly 200 million solar panels (~2% of global installations). Major customers include Tesla, SOLV Energy, Enel Green Power, and Cypress Creek Renewables. The company employs 119-156 people, primarily based at Greentown Labs in Somerville, Massachusetts.\n",
    "source_id": "2d3a97f0-fe30-8162-bb59-da3338daccc0",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Raptor Comply: Strategic Context Brief"
    ],
    "title": "Raptor Comply: Strategic Context Brief"
  },
  {
    "text": "## Regulatory expansion creates urgent compliance demand\nFERC Order 901 and subsequent NERC rule changes have fundamentally expanded which power generation facilities must comply with Critical Infrastructure Protection (CIP) cybersecurity standards. The core threshold change dropped mandatory registration requirements from \u226575 MVA at \u2265100 kV to \u226520 MVA at \u226560 kV for inverter-based resources (IBRs), capturing over 1,000 previously exempt renewable energy facilities.\nThe new Category 2 IBR classification encompasses solar, wind, and battery storage facilities between 20-75 MVA\u2014assets that historically operated without NERC oversight. NERC begins identifying these facilities for registration in May 2025, with full enforcement commencing May 2026. Additional enforcement milestones include:\nCIP-015-1 (Internal Network Security Monitoring): Effective September 2, 2025\nCIP-003-9 (Security Management Controls): Enforcement begins April 1, 2026\nFull FERC Order 901 implementation: January 1, 2030\nThese regulatory changes respond directly to grid reliability incidents caused by IBR performance failures. Since 2016, over 15,000 MW of unexpected generation loss has occurred from IBR-related disturbances. The June 2022 Odessa II event\u2014triggered by a lightning arrestor fault\u2014caused 2,555 MW of generation loss from 14 different solar sites, nearly reaching ERCOT's largest allowable event threshold. Root causes included improper inverter protection settings, inadequate voltage ride-through performance, and poor model accuracy.\nEnforcement has intensified correspondingly. The Duke Energy $10 million settlement (January 2019) remains the reference case: 127 CIP violations between 2015-2018, including 13 classified as serious risk\u2014failures to revoke terminated employee access, firewalls permitting unencrypted remote access, and deficient security patch programs. NERC penalties increased 20% in 2024 compared to the prior year, with maximum penalties now reaching $1.54 million per day per violation.\n",
    "source_id": "2d3a97f0-fe30-8162-bb59-da3338daccc0",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Raptor Comply: Strategic Context Brief"
    ],
    "title": "Raptor Comply: Strategic Context Brief"
  },
  {
    "text": "## Target customers face tool fragmentation and resource constraints\nRaptor Comply targets two primary segments: small-to-medium power generation owners/operators (typically 20-1500+ MW) who manage NERC compliance internally, and Managed Security Service Providers (MSSPs) seeking a technology backbone for compliance delivery. The platform serves solar, wind, battery storage, natural gas, and hydro facilities.\nThree distinct user personas experience compliance pain differently:\nCompliance Managers struggle with audit preparation across evolving standards. Their workflow involves building compliance calendars, maintaining program coverage across all requirements, and coordinating evidence collection\u2014often described as a \"monumental task for even experienced utilities.\" One Compliance Manager noted: \"That's a very nice feature\u2014I haven't seen anything like that\u2014a step by step that builds you a whole compliance calendar.\"\nOT Engineering Managers bear the operational burden of evidence documentation. As one stated: \"Right now, it's all very manual\u2014taking screenshots and then emailing our team.\" They duplicate effort between operational execution and compliance documentation, with no automated capture of evidence as work occurs.\nIT/Security teams face integration challenges across fragmented systems. A Technology Executive summarized: \"The whole approach is a gap. We are using a hodgepodge of solutions.\"\nIndustry statistics underscore the problem's scale:\n92% of compliance teams use 3+ different tools\n96% report struggling to keep pace with compliance demands\n5+ hours spent weekly on manual evidence collection\n$62,500 estimated annual cost of manual evidence collection\n$110,000+ FERC-estimated cost to establish a Low Impact NERC CIP program\n$379,000+ for Medium Impact entities\nCIP-004 (Personnel and Training) ranks among the most violated standards due to strict 24-hour timelines for revoking terminated employee access\u2014a process particularly error-prone with spreadsheet-based tracking. CIP-007 (System Security Management) logged 108 serious/moderate risk noncompliance filings in 2022 alone.\n",
    "source_id": "2d3a97f0-fe30-8162-bb59-da3338daccc0",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Raptor Comply: Strategic Context Brief"
    ],
    "title": "Raptor Comply: Strategic Context Brief"
  },
  {
    "text": "## Competitive positioning emphasizes speed and multi-team collaboration\nRaptor Comply differentiates against enterprise GRC platforms (ServiceNow, MetricStream, LogicGate) on three dimensions: implementation speed, price accessibility, and purpose-built architecture for NERC CIP specifically.\nImplementation timeline: The platform claims 2-3 weeks to operational readiness versus the 6-12 months typical of enterprise deployments. Marketing materials state: \"Traditional compliance tools take months to set up because you start with a blank slate. Our platform comes ready to use\u2014with NERC CIP task schedules, templates based on what actually works, and workflows you can start using right away.\"\nPricing structure targets the underserved middle market:\nAnnual billing provides 12% savings; 30-day refund guarantees apply to Starter and Scale tiers.\nMulti-team collaboration addresses the fundamental workflow gap. Most compliance software serves either compliance administrators (program management, evidence collection) or IT/OT operations teams (technical implementation)\u2014never both. CEO Vadhavkar stated: \"Generator operators shouldn't have to choose between compliance and operational efficiency as they architect for scale.\" The platform connects both teams in a unified system where evidence generates automatically as operations teams complete work, eliminating duplicate effort and email-based coordination.\n",
    "source_id": "2d3a97f0-fe30-8162-bb59-da3338daccc0",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Raptor Comply: Strategic Context Brief"
    ],
    "title": "Raptor Comply: Strategic Context Brief"
  },
  {
    "text": "## Product capabilities span the NERC CIP compliance lifecycle\nThe platform architecture centers on five core capabilities designed to automate compliance workflows while maintaining SOC 2 Type II certification:\nAsset Inventory Management organizes cyber systems and cyber assets across multi-facility fleets, enabling assignment to policies, tasks, and access requests. This supports the foundational CIP-002 requirement for BES Cyber System categorization.\nSmart Policies digitize compliance programs using a library of 60+ internal controls. Multiple-choice configuration flows guide users through requirement coverage, with custom detail capabilities for organization-specific needs. Marketing claims 100% program coverage for supported standards.\nAuto-Task Scheduling generates NERC CIP tasks automatically based on policy and program configuration. The compliance calendar builds programmatically, addressing the manual calendar maintenance burden compliance managers identified as a primary pain point.\nAccess Control Workflows provide configurable flows for physical perimeters, electronic perimeters, Transient Cyber Asset (TCA) access, and removable media\u2014the access management categories where CIP-004 violations most commonly occur. Sharable secure links enable rapid field access for technicians.\nEvidence Automation integrates evidence collection directly with task completion. When operations teams perform actions, evidence generates automatically\u2014eliminating the screenshot-and-email workflows that consume 5+ hours weekly for many teams.\nThe platform supports Low Impact, Medium Impact, and High Impact facility classifications, with Category 2 IBR support specifically addressing the May 2026 enforcement deadline. Technical architecture is API-based with single-tenancy database deployment, role-based access control, and SSO support.\n",
    "source_id": "2d3a97f0-fe30-8162-bb59-da3338daccc0",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Raptor Comply: Strategic Context Brief"
    ],
    "title": "Raptor Comply: Strategic Context Brief"
  },
  {
    "text": "## Strategic constraints define best-fit customer profiles\nSeveral factors bound Raptor Comply's optimal positioning:\nBest-fit customers operate multi-facility portfolios in the 20-1500+ MW range, lack dedicated compliance technology infrastructure, and face the Category 2 IBR May 2026 deadline. Organizations already invested in enterprise GRC platforms like ServiceNow likely fall outside the target market. The sweet spot appears to be operators who have historically managed compliance through spreadsheets and fragmented tools, now facing regulatory requirements that demand more systematic approaches.\nStandards coverage focuses primarily on Low Impact and Category 2 IBR requirements at lower tiers, with Medium and High Impact support in the Enterprise tier. The platform emphasizes CIP-003 (Security Management Controls) given its April 2026 enforcement date for version 9. Operations & Planning (O&P) task support is listed as planned rather than currently available.\nIntegration requirements include API-based connectivity and Active Directory integration (Enterprise tier). The single-tenancy architecture and SOC 2 Type II certification address security concerns, though organizations with complex existing IT landscapes should evaluate integration depth.\nProduct maturity considerations warrant attention: Raptor Comply launched October 30, 2025\u2014less than one month ago. No independent reviews exist on G2, Capterra, or similar platforms. Customer testimonials on the website use role titles rather than named companies or individuals. While parent company Raptor Maps has established credibility with major solar operators (Tesla, SOLV Energy, Enel Green Power), the compliance product itself lacks track record. Organizations evaluating the platform should request reference calls with pilot customers and validate implementation timeline claims against actual deployments.\nTeam sizing for prospects should include both compliance and operations personnel, given the multi-team collaboration value proposition. Organizations where compliance and IT/OT teams operate entirely separately may see reduced benefit from the unified platform approach.\n",
    "source_id": "2d3a97f0-fe30-8162-bb59-da3338daccc0",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Raptor Comply: Strategic Context Brief"
    ],
    "title": "Raptor Comply: Strategic Context Brief"
  },
  {
    "text": "## Conclusion\nRaptor Comply enters the market at a regulatory inflection point\u2014the May 2026 Category 2 IBR deadline creates urgency for over 1,000 renewable energy facilities that have never faced NERC CIP compliance requirements. The platform's $839-$2,490/month pricing and 2-3 week implementation claims directly target the cost and complexity barriers that have historically limited compliance software adoption among smaller generators.\nThe strategic logic of Raptor Maps' expansion makes sense: deep existing relationships with solar/renewable operators, proven software development capabilities, and timing aligned with regulatory expansion affecting their core customer base. However, the October 2025 launch means the product lacks the deployment history and customer evidence that would validate marketing claims. Organizations facing imminent compliance deadlines must weigh the apparent product-market fit against the inherent risks of adopting newly launched software for regulatory obligations carrying $1.54 million per day penalty exposure.\n",
    "source_id": "2d3a97f0-fe30-8162-bb59-da3338daccc0",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Raptor Comply: Strategic Context Brief"
    ],
    "title": "Raptor Comply: Strategic Context Brief"
  },
  {
    "text": "Raptor Comply occupies a compelling white space at the intersection of urgent regulatory deadlines, enterprise tool fatigue, and an underserved mid-market segment. With May 2026 enforcement of Category 2 IBR requirements creating immediate demand, the product's positioning on cost reduction (30%+ savings on $110K+ compliance setup costs), rapid deployment (2-3 weeks vs. months), and multi-team collaboration addresses documented pain points that incumbent solutions have failed to solve. Strategic expansion opportunities exist across adjacent regulatory frameworks, international markets where Raptor Maps already operates, and emerging AI-powered compliance capabilities that competitors have yet to deploy effectively.\n",
    "source_id": "2d3a97f0-fe30-813d-9b8e-e93427167f35",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Strategic Growth Opportunities for Raptor Comply"
    ],
    "title": "Strategic Growth Opportunities for Raptor Comply"
  },
  {
    "text": "## The competitive landscape favors focused challengers\nEnterprise GRC platforms have priced themselves out of the mid-market. MetricStream's licensing starts at $750,000+ annually, Archer requires hiring dedicated full-time experts for customization, and ServiceNow implementations extend months with significant IT involvement. One PeerSpot reviewer captured the market reality: \"Archer is expensive compared to other GRC tools... the cost would be prohibitive for a small or medium-scale company.\"\nThe specialized NERC CIP tools (Industrial Defender, Tripwire, Network Perception) solve point problems\u2014asset management, change detection, network analysis\u2014but none delivers complete program management with the rapid time-to-value smaller utilities desperately need. No NERC CIP-specific vendor currently positions prominently on fast implementation, despite compliance teams reporting that audit preparation \"can take a compliance team's full attention and time for weeks.\"\nThe most violated NERC CIP standards reveal where software can create immediate value: CIP-007 (System Security) accounts for 54% of violations, followed by CIP-010 (Configuration Management) at 27.5% and CIP-004 (Personnel & Training) at 18.5%. Common causes include missed 35-day patch windows, configuration drift detection failures, and the strict 24-hour access revocation requirement for terminated employees. These are precisely the workflows where automation delivers measurable ROI\u2014automating patch management alone saves an estimated two hours per month per asset.\n",
    "source_id": "2d3a97f0-fe30-813d-9b8e-e93427167f35",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Strategic Growth Opportunities for Raptor Comply"
    ],
    "title": "Strategic Growth Opportunities for Raptor Comply"
  },
  {
    "text": "## Adjacent regulatory frameworks offer natural expansion paths\nThree adjacent compliance domains share substantial workflow overlap with NERC CIP and present viable expansion opportunities:\nEU NIS2 Directive (effective October 2024) is the most direct international equivalent, classifying energy infrastructure as \"high criticality\" and requiring incident reporting within 24 hours, supply chain security assessments, and management-level accountability. Penalties reach \u20ac10 million or 2% of global turnover. The control categories\u2014access management, risk assessment, incident response\u2014map directly to NERC CIP requirements, enabling significant platform reuse with localized compliance content.\nAustralian AESCSF (Australian Energy Sector Cyber Security Framework) was built on the same DOE ES-C2M2 foundation as U.S. frameworks, creating particularly strong alignment. Raptor Maps already operates in 50+ countries including Australia, providing existing customer relationships and market presence to leverage for compliance product expansion.\nBattery storage compliance (NFPA 855, UL 9540) represents an adjacent vertical with growing documentation requirements. As solar-plus-storage deployments accelerate, compliance workflows for safety certification, grid interconnection (IEEE 1547/2800), and fire code documentation could extend Raptor Comply's value proposition to the same customer base.\nFERC Order 2222 for distributed energy resources creates additional opportunity, though implementation timelines vary significantly by RTO\u2014CAISO completed in November 2024, while SPP won't finish until Q2 2030. DER aggregators face coordination requirements between RTOs, distribution utilities, and individual resources that software could streamline.\n",
    "source_id": "2d3a97f0-fe30-813d-9b8e-e93427167f35",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Strategic Growth Opportunities for Raptor Comply"
    ],
    "title": "Strategic Growth Opportunities for Raptor Comply"
  },
  {
    "text": "## AI capabilities in GRC remain nascent\u2014creating first-mover potential\nThe GRC industry is amid an AI transformation, but implementation remains early-stage. Gartner predicts over 50% of major enterprises will use AI for continuous regulatory compliance checks by 2025, up from less than 10% in 2021. Organizations implementing AI-assisted security automation report 62% improvement in compliance efficiency, yet 60% of GRC professionals still manage with manual processes.\nCurrent AI applications in leading platforms cluster around four capabilities: predictive risk scoring (MetricStream's AiSPIRE identifies patterns in historical violations), automated evidence collection (Secureframe reports 80-90% evidence reuse across overlapping frameworks), natural language policy search (enabling queries by intent rather than keywords), and regulatory change monitoring (Archer's Compliance.ai acquisition tracks regulatory updates and auto-maps to internal controls).\nCritical infrastructure cybersecurity monitoring is advancing rapidly. Research demonstrates LSTM neural networks combined with Random Forest algorithms achieving 99.8% accuracy in power system anomaly detection. The SANS 2024 ICS/OT Survey found incident detection time improved from \"days\" in 2019 to \"hours\" in 2024, while OT-specific monitoring adoption grew from 33% to 52%. Industrial Defender has spent 10+ years automating NERC CIP compliance with OT asset management, positioning as the established player in the OT-compliance intersection.\nThe differentiation opportunity for Raptor Comply lies in NERC CIP-specific AI\u2014most GRC platforms are horizontal, applying generic AI to compliance broadly rather than training models on energy sector-specific patterns, violation histories, and audit outcomes. Predictive capabilities for upcoming CIP requirement changes (like CIP-015-1 for internal network security monitoring) could provide proactive compliance guidance that reactive tools cannot match.\n",
    "source_id": "2d3a97f0-fe30-813d-9b8e-e93427167f35",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Strategic Growth Opportunities for Raptor Comply"
    ],
    "title": "Strategic Growth Opportunities for Raptor Comply"
  },
  {
    "text": "## Raptor Maps' Series C positioned growth, but Raptor Comply was separate\nRaptor Maps' $35 million Series C (December 2024, led by Maverix Private Equity) funded three stated priorities: product development acceleration for solar automation, team expansion in software engineering and data science, and scaling Raptor Solar Sentry for O&M orchestration. Notably, Raptor Comply was not mentioned in the Series C announcement\u2014it launched independently in October 2025, suggesting it may have been developed as a strategic extension of the platform's data infrastructure.\nCEO Nikhil Vadhavkar's public statements reveal the strategic logic: \"We've been working on the data model to apply AI to solar for many, many years... A system of record is a necessity for adopting AI in the renewable industry.\" The company has digitized 150+ GW of solar capacity across 50+ countries, creating the data foundation for compliance intelligence. CTO Eddie Obropta emphasized the platform approach: \"Our platform embeds years of compliance expertise directly into operations. Teams don't need to start from scratch\u2014they can implement proven security frameworks from day one.\"\nThe current team size is estimated at 75-115 employees across three continents, operating from headquarters at Greentown Labs in Somerville, Massachusetts, with a remote-first culture. Named customers span the solar value chain: Enel Green Power, Luminace (Brookfield's 1,400+ site distributed solar portfolio), Cypress Creek Renewables, and EPCs including McCarthy, SOLV Energy, and Rosendin Electric.\nRaptor Comply's pricing is published and transparent\u2014uncommon in enterprise GRC: Starter at $839/month (1 control center, 5 Low Impact facilities), Scale at $2,490/month (expanded capacity), and custom Enterprise pricing. This positions against enterprise platforms that hide pricing behind sales processes and professional services requirements.\n",
    "source_id": "2d3a97f0-fe30-813d-9b8e-e93427167f35",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Strategic Growth Opportunities for Raptor Comply"
    ],
    "title": "Strategic Growth Opportunities for Raptor Comply"
  },
  {
    "text": "## Four strategic growth vectors emerge from the research\nVector 1: Capture IBR deadline urgency (now through May 2026). The Category 2 IBR enforcement deadline creates a 205-day window of heightened demand. FERC estimates $379,000+ for Medium Impact entities to establish NERC CIP programs and $110,000+ for Low Impact. Raptor Comply's claim of 30%+ cost reduction and 2-3 week deployment directly addresses the time-compressed buying cycle. Marketing should emphasize countdown urgency and deployment speed guarantees.\nVector 2: Enable the \"land with compliance, expand to operations\" motion. Early customer feedback indicates cross-sell potential: \"What you're building here, it works beyond NERC CIP and it works beyond solar.\" Raptor Comply can serve as the entry point for customer relationships that expand into Raptor Solar Sentry for O&M automation. Conversely, existing Raptor Solar customers managing NERC-registered assets represent warm leads for compliance upsell.\nVector 3: Build MSSP channel partnerships. The target customer segment includes Managed Security Service Providers as a delivery platform. MSSPs serving multiple smaller utilities could standardize on Raptor Comply, creating recurring channel revenue and reducing customer acquisition costs. This channel strategy mirrors successful patterns in cybersecurity software distribution.\nVector 4: Prepare international regulatory modules. NIS2 enforcement across EU member states (23 countries received infringement notices for transposition delays) and AESCSF in Australia represent the highest-alignment international opportunities. Product investment in localized compliance content could leverage existing Raptor Maps international presence (50+ countries, 150+ GW digitized) to open new markets before competitors adapt.\n",
    "source_id": "2d3a97f0-fe30-813d-9b8e-e93427167f35",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Strategic Growth Opportunities for Raptor Comply"
    ],
    "title": "Strategic Growth Opportunities for Raptor Comply"
  },
  {
    "text": "## Messaging should attack documented competitor weaknesses\nCompetitive positioning should exploit specific, documented pain points:\nAgainst enterprise GRC platforms: \"Enterprise power without enterprise complexity and cost.\" Cite the MetricStream $750K+ annual licensing, the Archer requirement for \"hiring a full-time dedicated expert,\" and the ServiceNow \"extended implementation timelines.\" Position transparent pricing and self-service configuration as antidotes.\nAgainst point solutions: \"Complete NERC CIP program management, not just monitoring.\" Tools like Tripwire (configuration), Network Perception (network analysis), and Netwrix (change auditing) solve narrow problems. Position Raptor Comply as the unified platform that eliminates multi-tool sprawl\u2014particularly relevant given that 92% of compliance teams use 3+ tools.\nAgainst generic GRC platforms: \"Built specifically for power industry compliance.\" Vanta, Sprinto, and Drata automate SOC 2 and similar frameworks but lack NERC CIP-specific workflows. The pre-built 60+ internal controls, CIP-specific task scheduling, and evidence templates create differentiation.\nThe \"quick time-to-value\" positioning is genuinely differentiated\u2014no NERC CIP vendor currently claims rapid deployment prominently. The \"2-3 weeks\" claim should be validated with early customer data and used aggressively in competitive situations where enterprise tools require months.\n",
    "source_id": "2d3a97f0-fe30-813d-9b8e-e93427167f35",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Strategic Growth Opportunities for Raptor Comply"
    ],
    "title": "Strategic Growth Opportunities for Raptor Comply"
  },
  {
    "text": "## Conclusion: Strategic priorities for Growth PM consideration\nThe research reveals Raptor Comply entering a market with genuine unmet needs rather than manufactured differentiation. The compliance burden on smaller utilities is real ($110K+ setup costs), the enterprise tools are genuinely ill-suited (months-long implementations, $55K-$750K+ costs), and the regulatory timeline is fixed (May 2026 IBR enforcement).\nNear-term growth priorities should focus on capturing the IBR deadline window, building reference customers across the three violated standards (CIP-007, CIP-010, CIP-004), and proving the time-to-value claims with published case studies. Medium-term opportunities include MSSP channel development, cross-sell integration with Raptor Solar Sentry, and international expansion to NIS2/AESCSF markets.\nThe product-market fit signals are strong: early customer testimonials validate core value propositions, the pricing is aligned with mid-market budgets, and the parent company's data infrastructure provides a technical moat. The Growth PM challenge is execution\u2014converting market opportunity into customer acquisition velocity before the May 2026 deadline passes and urgency dissipates.\n",
    "source_id": "2d3a97f0-fe30-813d-9b8e-e93427167f35",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)",
      "Strategic Growth Opportunities for Raptor Comply"
    ],
    "title": "Strategic Growth Opportunities for Raptor Comply"
  },
  {
    "text": "## Raptor Comply: Research Synthesis\n",
    "source_id": "2d3a97f0-fe30-80f2-a5b4-cfb938a2c629",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)"
    ],
    "title": "Raptor Comply research (1)"
  },
  {
    "text": "## The Opportunity in One Sentence\nRaptor Comply enters a market where 1,000+ renewable facilities face mandatory NERC CIP compliance by May 2026, yet existing solutions either cost $110K-$750K+ annually or take 6-12 months to implement\u2014creating a clear gap for a fast, affordable alternative.\n",
    "source_id": "2d3a97f0-fe30-80f2-a5b4-cfb938a2c629",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)"
    ],
    "title": "Raptor Comply research (1)"
  },
  {
    "text": "## Company Context\nRaptor Maps is an MIT-founded solar software company (Y Combinator S16) that monitors 125 GW of solar assets across 45+ countries. The company raised a $35M Series C in December 2024, employs 75-115 people, and counts Tesla, Enel Green Power, and Cypress Creek Renewables as customers. Raptor Comply launched October 30, 2025\u2014less than one month old with ~10 pilots and no independent reviews yet.\nPricing is transparent and mid-market focused: $839/month (Starter), $2,490/month (Scale), custom Enterprise. This undercuts enterprise GRC platforms by 10-100x while offering purpose-built NERC CIP functionality.\nPrimary target: Small-to-medium power generators (20-1500+ MW) managing NERC compliance internally \u2014 solar, wind, battery storage, natural gas, and hydro facilities. Specifically those facing the new Category 2 IBR requirements who've never had to comply before.\nSecondary target: Managed Security Service Providers (MSSPs) who could use Raptor Comply as their technology backbone to serve multiple smaller utility clients.\nNamed Raptor Maps customers (parent company, not Raptor Comply specifically): Tesla, SOLV Energy, Enel Green Power, Cypress Creek Renewables, Luminace (Brookfield's 1,400+ site distributed solar portfolio). These existing relationships are warm leads for the compliance product upsell.\nWho's NOT the target: Enterprises already invested in ServiceNow, MetricStream, or Archer \u2014 they've sunk cost into those platforms and won't switch for a new product.\n",
    "source_id": "2d3a97f0-fe30-80f2-a5b4-cfb938a2c629",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)"
    ],
    "title": "Raptor Comply research (1)"
  },
  {
    "text": "## The Regulatory Catalyst\nFERC Order 901 dropped mandatory registration thresholds from \u226575 MVA to \u226520 MVA for inverter-based resources (solar, wind, battery). Key enforcement milestones create urgency:\nMay 2025: NERC begins identifying Category 2 IBR facilities for registration\nApril 2026: CIP-003-9 (Security Management Controls) enforcement begins\nMay 2026: Full Category 2 IBR enforcement deadline\nJanuary 2030: Complete FERC Order 901 implementation\nPenalties have teeth: Duke Energy paid $10M for 127 violations, and maximum penalties now reach $1.54M per day per violation.\n",
    "source_id": "2d3a97f0-fe30-80f2-a5b4-cfb938a2c629",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)"
    ],
    "title": "Raptor Comply research (1)"
  },
  {
    "text": "## Customer Pain Points (Validated by Research)\nCost burden: FERC estimates $110K+ to establish a Low Impact program, $379K+ for Medium Impact entities.\nTool fragmentation: 92% of compliance teams use 3+ disconnected tools; one executive described their approach as \"a hodgepodge of solutions.\"\nManual drudgery: 5+ hours weekly spent on manual evidence collection, primarily \"taking screenshots and emailing our team.\"\nExpertise scarcity: Compliance calendars are described as a \"monumental task for even experienced utilities.\"\n",
    "source_id": "2d3a97f0-fe30-80f2-a5b4-cfb938a2c629",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)"
    ],
    "title": "Raptor Comply research (1)"
  },
  {
    "text": "## Competitive Landscape\nEnterprise GRC platforms (MetricStream, Archer, ServiceNow): Priced at $55K-$750K+ annually, require 6-12 months implementation, need dedicated experts for customization. One review noted Archer's cost is \"prohibitive for small or medium-scale companies.\"\nNERC-specific point solutions (Industrial Defender, Tripwire, Network Perception): Solve narrow problems (asset management, change detection, network analysis) but don't deliver complete program management or fast time-to-value.\nWhite space Raptor Comply owns: No competitor prominently positions on rapid deployment (<3 weeks) combined with transparent mid-market pricing and multi-team collaboration (compliance + IT/OT in one system).\n",
    "source_id": "2d3a97f0-fe30-80f2-a5b4-cfb938a2c629",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)"
    ],
    "title": "Raptor Comply research (1)"
  },
  {
    "text": "## Most Violated Standards = Product Opportunity\nCIP-007 (System Security Management): 54% of violations\u2014missed 35-day patch windows, inadequate security controls.\nCIP-010 (Configuration Management): 27.5% of violations\u2014configuration drift, change detection failures.\nCIP-004 (Personnel & Training): 18.5% of violations\u2014the strict 24-hour access revocation requirement for terminated employees is particularly error-prone with spreadsheet tracking.\n",
    "source_id": "2d3a97f0-fe30-80f2-a5b4-cfb938a2c629",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)"
    ],
    "title": "Raptor Comply research (1)"
  },
  {
    "text": "## Adjacent Growth Opportunities\nInternational expansion: EU NIS2 Directive (October 2024, \u20ac10M penalties) and Australian AESCSF share substantial control overlap with NERC CIP. Raptor Maps already operates in 50+ countries.\nAdjacent verticals: Battery storage compliance (NFPA 855, UL 9540), DER aggregator coordination (FERC Order 2222), EV charging infrastructure security.\nMSSP channel: Managed Security Service Providers serving multiple smaller utilities could standardize on Raptor Comply\u2014this represents a potential distribution channel beyond direct sales.\n",
    "source_id": "2d3a97f0-fe30-80f2-a5b4-cfb938a2c629",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)"
    ],
    "title": "Raptor Comply research (1)"
  },
  {
    "text": "## AI/Automation Opportunity\nThe GRC industry is early in AI adoption: Gartner predicts 50%+ of enterprises will use AI for compliance by 2025, up from <10% in 2021. Current AI applications focus on predictive risk scoring, automated evidence collection, and regulatory change monitoring. No competitor has deployed NERC CIP-specific AI\u2014training models on energy sector violation patterns and audit outcomes represents a differentiation opportunity.\n",
    "source_id": "2d3a97f0-fe30-80f2-a5b4-cfb938a2c629",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)"
    ],
    "title": "Raptor Comply research (1)"
  },
  {
    "text": "## Strategic Implications for the Case Study\nThe 205-day countdown is your hook. Every touchpoint should reference the May 2026 deadline creating urgency for facilities that have never faced NERC CIP requirements.\n\"Time-to-value in minutes, not months\" is the competitive wedge. Enterprise tools can't claim fast implementation. Position the exposure assessment and 2-3 week deployment as proof points.\nMulti-team collaboration is differentiated but harder to demonstrate in a trial. Design onboarding that delivers value to a solo compliance manager while creating a path to invite IT/OT colleagues.\nThe product is unproven. No case studies, no G2 reviews, only role-titled testimonials. Growth strategy must generate reference customers and validation data simultaneously with lead generation.\n",
    "source_id": "2d3a97f0-fe30-80f2-a5b4-cfb938a2c629",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep",
      "Raptor Comply research (1)"
    ],
    "title": "Raptor Comply research (1)"
  },
  {
    "text": "Before I dive in, I want to flag that I've made some assumptions where I didn't have internal data \u2014 things like whether certain product capabilities exist, baseline metrics, and channel performance. I'll call those out as I go, and I have fallback recommendations if any assumptions don't hold.\u201d\nStrategic framing: The May 2026 deadline creates a 150-day window where urgency is our ally. My strategy is built around one core bet: that demonstrating time-to-value in minutes\u2014not months\u2014is the competitive moat Raptor Comply should own.\"\n",
    "source_id": "2d3a97f0-fe30-80d8-8dc6-c87e928a9016",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep"
    ],
    "title": "Raptor Comply Prep"
  },
  {
    "text": "## Strategic Recommendations: Raptor Comply 30-Day Growth Case Study\nThis is a resource-constrained launch into a deadline-driven market. The strategic through-line I'm building: demonstrate time-to-value so fast it becomes your competitive moat. Every onboarding decision, every experiment, every metric should ladder up to proving that Raptor Comply delivers compliance clarity faster than any alternative\u2014including doing nothing.\nLet me walk you through each decision with the reasoning behind it.\n",
    "source_id": "2d3a97f0-fe30-80d8-8dc6-c87e928a9016",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep"
    ],
    "title": "Raptor Comply Prep"
  },
  {
    "text": "## PART 1: Onboarding & Activation\n  The Activation Metric Decision\n  I evaluated four candidate activation metrics against three criteria: predictive of conversion, achievable in <7 days, and demonstrates core value proposition.\n  Option A: First Policy Configured. Shows engagement but not value delivery. A user could configure a policy incorrectly and still \"activate\"\u2014this measures effort, not outcome. Weak predictor of conversion because it doesn't prove the product works.\n  Option B: First Compliance Task Auto-Scheduled. Better\u2014this proves the automation is functioning. But scheduling a task isn't the same as understanding your compliance posture. The user still hasn't experienced the \"aha moment\" of seeing their exposure.\n  Option C: First Evidence Artifact Generated. This requires the user to complete a task and have the system capture evidence automatically. High-value signal, but takes too long\u2014likely 2+ weeks given task scheduling cadences. Fails the <7 day requirement.\n  Option D: Compliance Exposure Score Calculated. This means the user has entered their facility profile, asset inventory, and policy selections\u2014and the system has returned a quantified view of their compliance gaps, upcoming deadlines, and risk exposure. This is the \"aha moment.\"\n  Before I walk through my activation metric recommendation, I want to flag that this assumes a product capability \u2014 the ability to generate a compliance exposure score from minimal input. If that feature exists or can be built, here's my recommendation. If not, I have a fallback metric I can discuss.\n  My recommendation: Option D\u2014Compliance Exposure Score Calculated within 5 minutes of substantive input.\nHere's my reasoning. The documented pain point is that compliance teams face a \"monumental task\" just understanding where they stand. FERC estimates $110K+ just to establish a Low Impact program\u2014much of that cost is assessment and planning before any actual compliance work. If Raptor Comply can show a user their exposure in minutes instead of weeks, that's the value demonstration that drives conversion.\n  This also directly attacks competitor weakness. Enterprise GRC platforms require months of implementation before delivering any insight. Point solutions like Tripwire or Network Perception show technical data but don't translate it into compliance posture. \"See your exposure in 5 minutes\" is a claim no competitor can match.\n  The tradeoff: this metric requires the user to input meaningful data (facility type, MW capacity, asset categories). If the input friction is too high, you'll see drop-off before activation. That's why the onboarding flow design is critical.\n  The Onboarding Flow Design\n  I considered four structural approaches:\n  Approach 1: Linear Configuration Wizard. Walk users through asset inventory \u2192 policy selection \u2192 task scheduling step-by-step. Problem: this front-loads effort before value. Users invest 30+ minutes before seeing any return. High abandonment risk.\n  Approach 2: Template-Based Quick Start. \"You're a 50 MW solar facility in ERCOT\u2014here's your pre-configured compliance program.\" Problem: too presumptive. Compliance managers need to trust that the system understands their specific situation. Templates feel generic.\n  Approach 3: Import-and-Analyze. \"Upload your existing asset spreadsheet and we'll map it.\" Problem: assumes users have clean data to import. The research shows they're using \"hodgepodge solutions\"\u2014their data is scattered and inconsistent.\n  Approach 4: Assessment-First (Diagnose Before Prescribe). Start with 5-7 questions that establish facility profile and compliance scope. Show the exposure score immediately. Then offer to \"fix\" the identified gaps through guided configuration.\n  My recommendation: Approach 4\u2014Assessment-First onboarding with immediate exposure scoring.\n  The specific flow I'd design:\n  Day 1, Minute 0-2: Facility Profile Quiz. Five multiple-choice questions: What's your total MW capacity? (dropdown: <20 MVA, 20-75 MVA, 75+ MVA). What's your interconnection voltage? (dropdown ranges). What generation type? (solar/wind/battery/gas/hydro). Are you currently NERC-registered? (yes/no/unsure). What's your current compliance approach? (spreadsheets/no formal program/enterprise GRC/consultant-managed).\n  These questions take 90 seconds and accomplish two things: they segment the user into Low Impact vs. Medium Impact vs. Category 2 IBR, and they reveal their current pain level.\n  Day 1, Minute 2-5: Instant Exposure Report. Based on those five answers, generate an exposure summary: \"Based on your 45 MVA solar facility at 69kV, you fall under Category 2 IBR requirements effective May 2026. Here's what that means...\" Show a visual dashboard with: applicable CIP standards (CIP-003-9, etc.), key deadlines (May 2026 registration, April 2026 CIP-003-9 enforcement), estimated compliance gap (percentage of requirements currently addressed based on their \"current approach\" answer), and penalty exposure range.\n  This is the activation moment. The user now understands their situation in a way they didn't before signing up.\n  Day 1, Minute 5-15: Guided Asset Inventory. Now that they've seen value, prompt them to add detail: \"To refine your compliance roadmap, let's identify your BES Cyber Assets. How many inverters do you have? Do you have SCADA systems? Remote access points?\" Use progressive disclosure\u2014start with the minimum viable asset inventory (the categories required for CIP-002 classification), not a complete technical inventory.\n  Day 2-3: Policy Configuration with Recommendations. Based on the asset inventory, pre-select recommended policies from the 60+ internal controls. \"For Category 2 IBR facilities with remote access, we recommend these 12 policies. Review and customize, or accept defaults to continue.\" This leverages the \"comes ready to use\" positioning from their marketing.\n  Day 4-7: First Task Assignment and Team Invitation. Generate the first compliance tasks based on policy configuration. Prompt user to invite their IT/OT counterpart: \"Compliance works best when operations teams are connected. Invite your OT Engineering Manager to handle technical evidence collection.\"\n  Top 3 Drop-Off Risks and Mitigations\n  Risk 1: Asset Inventory Paralysis (Day 1, Minute 5-15).\nThe research shows compliance managers struggle because \"it's all very manual\u2014taking screenshots and emailing our team.\" They don't have clean asset data readily available. When the quiz ends and the system asks for asset details, users may stall because they need to gather information from other systems or colleagues.\n  Mitigation: Offer a \"Quick Start\" path that uses facility-type defaults. \"Don't have your asset inventory handy? We'll use typical assets for a 45 MW solar facility. You can refine later.\" This preserves momentum while allowing progressive refinement. Also provide a \"save and continue later\" with email reminder at 24 and 72 hours.\n  Risk 2: Multi-Stakeholder Coordination Stall (Day 4-7).\nRaptor Comply's value proposition depends on connecting compliance managers and IT/OT teams. But the compliance manager who signs up for the trial may not have authority to invite colleagues, or may face internal friction getting buy-in. This creates a ceiling on activation\u2014single-user trials can't demonstrate the multi-team collaboration value.\n  Mitigation: Design a single-user activation path that still delivers value. The exposure score and task scheduling work for solo users. Frame team invitation as \"unlock advanced features\" rather than a gate. Provide a \"stakeholder briefing\" one-pager the compliance manager can share internally: \"Here's what I found using Raptor Comply\u2014here's why we should trial it together.\"\n  Risk 3: \"I'll Come Back Later\" Syndrome (Day 1, Any Point).\nCompliance professionals are busy. They sign up with good intentions, get interrupted, and never return. The research cites \"5+ hours weekly on manual evidence collection\"\u2014these are time-starved users.\n  Mitigation: The 5-minute exposure score is the primary defense\u2014deliver value before they leave. Secondary defense: behavioral email sequence triggered by drop-off points. If they complete the quiz but don't start asset inventory within 24 hours, send: \"Your compliance exposure report is waiting. You're 83% ready for May 2026\u2014here's what's missing.\" If they start asset inventory but don't finish, send: \"You've identified 12 cyber assets. Add 3 more to complete your baseline and unlock task scheduling.\"\n  Quantify progress and create completion momentum rather than generic \"come back\" nudges.\n",
    "source_id": "2d3a97f0-fe30-80d8-8dc6-c87e928a9016",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep"
    ],
    "title": "Raptor Comply Prep"
  },
  {
    "text": "## PART 2: 30-Day Growth Execution Plan\n  Strategic Constraints That Shape Experiment Selection\n  Before picking experiments, let me name the constraints that eliminate certain options:\n  $5K eliminates paid search. NERC CIP compliance keywords are expensive (enterprise GRC buyers) and Raptor Comply's brand awareness is zero. You'd burn budget on clicks from enterprises who aren't the target market.\n  30 days eliminates SEO/content marketing. Content takes months to rank. You might publish content for credibility, but it won't drive trial signups in this window.\n  Early stage means limited social proof. You can't run campaigns built on case studies and logos you don't have yet.\n  May 2026 deadline creates urgency positioning. This is the hook\u2014every experiment should reference the countdown.\n  Before I walk through the three marketing experiments, I want to call out a zero-cost opportunity: Raptor Maps has existing relationships with solar operators like Tesla, Enel, and Cypress Creek. My first move would be working with sales to identify which of those customers manage NERC-registered assets and run a targeted outreach offering early access to Raptor Comply. This is our warmest, lowest-CAC channel and could generate reference customers before we spend a dollar on paid acquisition. Now, for the $5K budget...\u201d\n  The Three Experiments I'd Run\n  Experiment 1: LinkedIn \"Compliance Exposure Calculator\" Lead Magnet ($2,000)\n  Channel: LinkedIn Ads targeting compliance managers at power generation companies.\n  Tactic: Promoted post driving to a gated \"NERC CIP Compliance Exposure Calculator\"\u2014essentially a standalone version of the Day 1 assessment quiz. User answers 5 questions, gets an exposure report with their Category 2 IBR status, applicable standards, and May 2026 deadline. Report ends with: \"Want to see your full compliance roadmap? Start your free trial of Raptor Comply.\"\n  Why this experiment: It tests the core hypothesis that \"see your exposure fast\" resonates with the target audience. If compliance managers won't even complete a 5-question quiz for a free report, they won't complete trial onboarding. This is a lower-commitment way to validate the value proposition before optimizing the full trial flow.\n  Budget allocation: $1,500 on LinkedIn ad spend (targeting: Job titles containing \"compliance,\" \"NERC,\" \"reliability\" at companies in Utilities, Renewable Energy, Power Generation industries; company size 51-1000; US geography). $500 reserved for creative iteration based on Week 1 performance. \n  Success criteria: 500+ quiz completions at <$3 cost per completion. 15%+ conversion from quiz completion to free trial signup (75+ trial signups from this channel). Track which quiz answers correlate with trial signup to refine targeting. \n  Experiment 2: Industry Newsletter Sponsorship ($1,500)\n  Channel: Sponsored content in NERC-focused or renewable energy compliance newsletters.\n  Tactic: Identify 2-3 newsletters that reach compliance professionals at power generators. Options include: Utility Dive's compliance coverage, POWER Magazine's newsletter, or regional reliability coordinator newsletters. Purchase a sponsored post or newsletter inclusion featuring the May 2026 deadline angle.\n  Why this experiment: Newsletter audiences are pre-qualified\u2014they've opted in to receive compliance content. This is \"rented audience\" that doesn't require building from scratch. It also tests whether the message resonates with an audience actively seeking compliance information (high intent) versus LinkedIn's broader professional audience (lower intent but larger scale).\n  Budget allocation: $1,500 split across 2 newsletter placements in Weeks 2-3. Negotiate for inclusion of a trackable link to the free trial landing page.\n  Success criteria: 3%+ click-through rate from newsletter to landing page (newsletters average 2-5% CTR for sponsored content). 25%+ conversion from click to trial signup. If one newsletter dramatically outperforms, reallocate remaining budget to double down.\n  Experiment 3: \"May 2026 Readiness\" Webinar with MSSP Partner ($1,500)\n  Channel: Co-marketed webinar with an MSSP or compliance consultant who serves the target market.\n  Tactic: Partner with a managed security service provider or NERC compliance consulting firm to co-host a webinar: \"May 2026 Category 2 IBR Deadline: What You Need to Know and How to Prepare.\" The MSSP provides credibility and brings their client list as attendees. Raptor Comply provides the platform demo as part of the \"how to prepare\" content.\n  Why this experiment: This tests the MSSP channel hypothesis from the research. If MSSPs are willing to co-market, they're signaling potential for deeper partnership (reselling, referrals). It also generates higher-quality leads\u2014webinar attendees who watch a 45-minute session have demonstrated genuine interest. The cost-per-lead will be higher, but conversion rates should be significantly better.\n  Budget allocation: $500 for webinar platform/promotion. $500 for MSSP incentive (co-marketing fee or lead share agreement). $500 for post-webinar follow-up sequence and landing page optimization.\n  Success criteria: 100+ registrations (co-marketing should bring 50+ from MSSP's network). 40%+ attendance rate (40+ live attendees). 25%+ conversion from attendee to free trial signup (10+ high-quality trial signups). Secondary goal: validate MSSP channel interest for future partnership development.\n  Week-by-Week Execution Plan\n  Week 1: Foundation and Experiment 1 Launch\n  Days 1-2: Finalize the Compliance Exposure Calculator quiz and results page. This is the asset that powers Experiment 1 and serves as the activation mechanism within the trial. Build the landing page with May 2026 countdown prominently featured.\n  Days 3-4: Launch LinkedIn campaign with 3 creative variants. Variant A emphasizes deadline urgency (\"150 days until NERC CIP enforcement expands\"). Variant B emphasizes cost savings (\"Stop spending $110K on compliance setup\"). Variant C emphasizes speed (\"See your compliance exposure in 5 minutes\").\n  Days 5-7: Monitor initial performance. Pause underperforming variants, increase spend on winners. Begin outreach to newsletters for Experiment 2 placements.\n  Week 2: Experiment 2 Launch and Experiment 1 Optimization\n  Days 8-10: Confirm newsletter placements for Week 2-3. Finalize sponsored content copy\u2014use learnings from Week 1 LinkedIn performance to inform messaging.\n  Days 11-14: First newsletter placement goes live. Simultaneously, begin MSSP partner outreach for Experiment 3. Target 5-10 MSSPs with personalized outreach offering co-marketing partnership.\n  End of Week 2 checkpoint: Review Experiment 1 data. Are quiz completions converting to trial signups? If <10% conversion, the trial onboarding flow needs work before scaling spend. If >20%, increase LinkedIn budget using the $500 reserve.\n  Week 3: Experiment 3 Launch and Optimization\n  Days 15-17: Confirm MSSP partner and webinar date (schedule for Day 21-24). Create webinar registration page and promotional assets.\n  Days 18-21: Second newsletter placement goes live. Promote webinar through LinkedIn organic posts and to existing trial signups who haven't activated (re-engagement opportunity).\n  Days 22-24: Host webinar. Capture registrant data and engagement metrics (questions asked, poll responses, demo section watch time).\n  End of Week 3 checkpoint: Compare cost-per-trial-signup across all three experiments. You should have enough data to identify which channel is most efficient.\n  Week 4: Double Down and Document\n  Days 25-28: Reallocate remaining budget to best-performing channel. If LinkedIn is winning, increase spend and test new audiences (expand to job titles like \"reliability coordinator,\" \"grid compliance\"). If newsletter is winning, book additional placements for post-trial-launch period. If webinar is winning, schedule second webinar and explore deeper MSSP partnership.\n  Days 29-30: Compile learnings for case study presentation. Document what worked, what didn't, and hypotheses for next 30 days. Prepare metrics dashboard for ongoing monitoring.\n  The Metrics Dashboard (5-8 Key Metrics)\n  I'm organizing these in a funnel structure because that's how you'll diagnose problems:\n  Top of Funnel (Awareness to Interest):\n  Impressions by Channel \u2014 Are your experiments reaching the target audience?\n  Click-Through Rate by Channel \u2014 Is the messaging resonating?\n  Middle of Funnel (Interest to Trial):\n3. Quiz Completions (Experiment 1 specific) \u2014 Are people engaging with the exposure calculator?\n4. Trial Signups \u2014 The primary volume metric. Target: 150+ in 30 days across all channels.\n5. Cost per Trial Signup by Channel \u2014 Efficiency metric for budget allocation decisions. Target: <$35 average (allows 150 signups on $5K budget).\n  Bottom of Funnel (Trial to Activation):\n6. Activation Rate (% of trial signups who reach Compliance Exposure Score within 7 days) \u2014 Target: 40%+. This is the leading indicator of trial-to-paid conversion.\n7. Time to Activation \u2014 How many days/hours from signup to exposure score? Shorter is better. Target: median <2 days.\n  Lagging Indicator:\n8. Trial to Paid Conversion Rate \u2014 This won't be measurable in 30 days (trials need time to convert), but establish the tracking now. Industry benchmark for B2B SaaS free trials: 15-25%.\n  One bonus metric I'd track informally: Qualitative feedback on the \"aha moment.\" Add a single question after the exposure score: \"How valuable was this compliance assessment?\" (1-5 scale). This helps you understand if the activation mechanism is actually delivering perceived value or just checking a box.\n",
    "source_id": "2d3a97f0-fe30-80d8-8dc6-c87e928a9016",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep"
    ],
    "title": "Raptor Comply Prep"
  },
  {
    "text": "## PART 3: LinkedIn Post\n  Pick one of your marketing experiments. Write the actual LinkedIn post you'd publish to promote the free trial to compliance professionals at energy companies. Include post copy and any accompanying creative brief (image/video description if relevant).\n  Which Experiment to Promote\n  I'm recommending the LinkedIn post promote Experiment 1: the Compliance Exposure Calculator.\n  Here's my reasoning: The calculator is the lowest-commitment entry point. It validates interest before asking for a trial signup. A LinkedIn post asking people to sign up for a free trial has more friction than a post offering a free assessment tool. Once they've completed the quiz and seen their exposure score, the trial signup becomes a natural next step.\n  The webinar (Experiment 3) would also make a strong LinkedIn post, but it requires scheduling commitment and has a fixed date\u2014limiting reach. The calculator is evergreen and can be promoted continuously.\n  The LinkedIn Post\n  Post Copy:\n  In 150 days, NERC CIP enforcement expands to 1,000+ renewable energy facilities that have never faced federal cybersecurity mandates.\n  If you operate solar, wind, or battery storage assets between 20-75 MVA, you're likely in scope for Category 2 IBR requirements starting May 2026.\n  The question isn't whether you need a compliance program. It's whether you have time to build one.\n  Most utilities spend $110K+ and 6+ months standing up a NERC CIP program from scratch. That timeline doesn't work when you have 150 days.\n  We built a 5-minute Compliance Exposure Calculator to help you understand exactly where you stand:\n  \u2192 Are you subject to the new IBR requirements?\n\u2192 Which CIP standards apply to your facility type?\n\u2192 What's your current compliance gap?\n\u2192 What deadlines are you racing against?\n  No sales pitch. No commitment. Just clarity on what May 2026 means for your operations.\n  [Link: See Your Compliance Exposure \u2192]\n  Creative Brief:\n  Format: Single static image (LinkedIn's algorithm currently favors text-heavy posts with simple images over carousels or videos for B2B content).\n  Visual concept: A countdown timer showing \"150 DAYS\" prominently, with the subhead \"Until NERC CIP expands to 1,000+ renewable facilities.\" Below the countdown, a simple checklist graphic with three items: \"Am I in scope? \u2713\" \"Which standards apply? \u2713\" \"What's my gap? \u2713\" with the CTA \"Get your exposure score in 5 minutes.\"\n  Color palette: Use Raptor Comply's brand colors (likely blues/greens given climate tech positioning). Avoid red/alarm colors\u2014compliance professionals see enough fear-based marketing. Position this as clarity, not panic.\n  Image dimensions: 1200 x 627 pixels (LinkedIn's recommended aspect ratio for link preview images).\n",
    "source_id": "2d3a97f0-fe30-80d8-8dc6-c87e928a9016",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep"
    ],
    "title": "Raptor Comply Prep"
  },
  {
    "text": "## How These Parts Connect as a System\n  Let me make explicit how these decisions reinforce each other:\n  The activation metric (Compliance Exposure Score) directly shapes the lead magnet (Calculator). By defining activation as \"seeing your exposure,\" I can test that value proposition before users even enter the trial. If the calculator converts well, the activation mechanism is validated. If it doesn't, I know to rethink the core value prop before spending more on acquisition.\n  Week 1-2 results determine Week 3-4 strategy. If the LinkedIn calculator outperforms, I'd shift the webinar content to be calculator-focused (\"See your live exposure score demo\"). If newsletter outperforms LinkedIn, I'd reduce paid spend and invest in creating newsletter-worthy content for earned placements. The experiments are designed to generate learnings, not just leads.\n  The MSSP webinar (Experiment 3) tests a channel hypothesis that affects post-30-day strategy. If MSSPs engage enthusiastically with co-marketing, the growth playbook shifts toward channel partnerships. If they're lukewarm, direct acquisition becomes the priority. This experiment is as much about strategic intelligence as lead generation.\n  Drop-off mitigations connect to the metrics dashboard. The \"Asset Inventory Paralysis\" risk would show up as high quiz completion but low activation rate\u2014users get the exposure score but stall on the next step. The \"I'll Come Back Later\" risk would show up as long time-to-activation. The dashboard is designed to diagnose which risk is actually occurring.\n",
    "source_id": "2d3a97f0-fe30-80d8-8dc6-c87e928a9016",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep"
    ],
    "title": "Raptor Comply Prep"
  },
  {
    "text": "## Post presentation q&a\n  Strategic Judgment Questions\n  \"Why these three experiments and not others?\"\nBe ready to explain what you didn't pick and why. You deprioritized paid search (expensive, wrong audience), SEO/content (too slow for 30 days), and events/conferences (logistically complex, budget doesn't support). Show you evaluated options, not just grabbed the first ideas.\n  \"What would you do differently with $20K? With $50K?\"\nThey're testing if you can scale thinking. With $20K, you might add paid search for high-intent keywords, sponsor a second webinar, or hire a freelance content writer for SEO groundwork. With $50K, you might test a small field marketing event or regional utility conference sponsorship.\n  \"What's your biggest risk in this plan?\"\nHave a clear answer. Mine would be: \"The biggest risk is that the fast time-to-value positioning doesn't resonate \u2014 if compliance managers don't care about speed and only care about cost or thoroughness, our messaging misses. That's why Experiment 1 tests the value prop before we scale spend.\"\n  \"What would make you abandon this strategy entirely?\"\nShows you're not precious about your ideas. Answer: \"If Week 1-2 data shows the exposure calculator gets completions but zero trial conversions, that tells me the value prop isn't compelling enough to drive action. I'd pause paid spend and do customer interviews to understand what's actually blocking them.\"\n  Execution & Prioritization Questions\n  \"It's Day 15 and you've only generated 20 trial signups. What do you do?\"\nThey want to see how you diagnose and respond. Answer: \"First I'd look at where the funnel is breaking \u2014 are we getting impressions but no clicks (messaging problem), clicks but no quiz completions (landing page problem), or completions but no trial signups (value prop problem). The fix depends on where the drop-off is. If it's top-of-funnel, I'd test new creative. If it's bottom-of-funnel, I'd revisit the trial signup CTA and friction.\"\n  \"How would you prioritize if you only had 2 weeks instead of 4?\"\nAnswer: \"I'd cut Experiment 3 (webinar) entirely \u2014 it requires partner coordination and has a longer lead time. I'd focus all budget on Experiment 1 (LinkedIn calculator) and Experiment 2 (newsletter), which can launch faster and generate learnings quickly.\"\n  \"Who do you need to execute this plan?\"\nShow you've thought about cross-functional dependencies. Answer: \"I'd need design support for the landing page and LinkedIn creative, engineering to build or validate the exposure calculator capability, sales to identify warm leads from Raptor Maps' customer base, and marketing ops to set up tracking and the email sequences.\"\n  Product & Customer Questions\n  \"How do you know compliance managers actually care about speed?\"\nThis is where your research pays off. Answer: \"The research shows compliance teams spend 5+ hours weekly on manual evidence collection and describe audit prep as a 'monumental task.' FERC estimates 6+ months to stand up a program. The pain is time, not just money. Speed directly addresses that.\"\n  \"What if our target customer doesn't use LinkedIn?\"\nFair challenge. Answer: \"That's a hypothesis I'm testing with Experiment 1. If LinkedIn CTR is poor despite good targeting, that's a signal to shift budget to industry newsletters or direct outreach channels where compliance professionals are more active. The experiment is designed to learn, not just acquire.\"\n  \"How would you handle a customer who activates but then churns after the trial?\"\nShows you're thinking beyond acquisition. Answer: \"I'd want to understand why \u2014 was the product missing a key feature, did they hit a technical blocker, or did they just not have time to evaluate properly? I'd build a brief exit survey into the trial expiration flow and flag churned-but-activated users for a personal follow-up call.\"\n  Metrics & Measurement Questions\n  \"How do you know if 40% activation rate is good or bad?\"\nBe honest that it's a benchmark. Answer: \"40% is an industry benchmark for B2B SaaS free trials. Without Raptor Comply baseline data, I'd treat this as a starting hypothesis. If we hit 25%, that's still signal \u2014 and I'd dig into where users are dropping to improve it. The first 30 days are about establishing baselines, not hitting arbitrary targets.\"\n  \"What's the most important metric in your dashboard?\"\nPick one and defend it. Answer: \"Activation rate \u2014 it's the leading indicator that predicts everything downstream. High trial signups but low activation means we're acquiring the wrong users or the onboarding is broken. High activation suggests the product is delivering value and conversion will follow.\"\n  \"How would you attribute conversions across experiments?\"\nShows you've thought about measurement rigor. Answer: \"Each experiment has a unique entry point \u2014 LinkedIn drives to the calculator, newsletters drive to a trial landing page with UTM parameters, webinar attendees are tracked by registration. I'd use UTM tagging throughout and build a simple attribution view in the dashboard. For a 30-day sprint, first-touch attribution is good enough.\"\n  Questions About You / Working Style\n  \"How did you approach this case study?\"\nBe honest about your process. You did deep research on the market, competitive landscape, and regulatory context before developing strategy. You considered multiple options for each decision and made explicit tradeoffs. You pressure-tested assumptions.\n  \"What would you want to learn in your first 30 days on the job?\"\nAnswer: \"I'd want to understand the current trial flow and where users are dropping off, talk to 5-10 pilot customers about what made them convert or hesitate, and understand engineering capacity for onboarding changes. My strategy is based on research \u2014 I'd want to validate it against internal data and customer reality.\"\n  \"What's a time you had to drive growth with limited resources?\"\nThis is your Kunik story. You reduced time-to-value from 3 weeks to 5 days, built activation systems, and ran $100K pilots. Connect it directly: \"At Kunik, I faced a similar challenge \u2014 early-stage product, limited budget, needed to prove value fast. I focused on reducing time-to-value because that was the unlock for conversion. I'd bring that same lens here.\"\n  Curveball Questions\n  \"We actually already tried LinkedIn ads and they didn't work. Now what?\"\nDon't get defensive. Answer: \"I'd want to understand what didn't work \u2014 was it targeting, creative, landing page, or the offer itself? If LinkedIn is proven dead for this audience, I'd reallocate that $2K to doubling down on newsletter sponsorships and testing a direct outreach campaign to Raptor Maps' existing customer base.\"\n  \"What if we told you the free trial is launching in 15 days, not 30?\"\nShows you can adapt under pressure. Answer: \"I'd simplify to one experiment \u2014 the LinkedIn calculator \u2014 and treat the other two as fast-follows for post-launch. I'd also prioritize the Raptor Maps customer outreach since that requires no creative production and can launch immediately.\"\n  \"Is there anything in our product or market you think we're getting wrong?\"\nCareful here \u2014 be constructive, not critical. Answer: \"Based on my research, the multi-team collaboration value prop is strong but hard to demonstrate in a single-user trial. I'd want to explore how to give solo evaluators a taste of that value without requiring them to invite colleagues during the trial.\"\n  Want me to do a mock Q&A with you where I play the interviewer and throw some of these at you?\n",
    "source_id": "2d3a97f0-fe30-80d8-8dc6-c87e928a9016",
    "path": [
      "Interview Prep",
      "Raptor Comply Prep"
    ],
    "title": "Raptor Comply Prep"
  },
  {
    "text": "## Kunik Infra Map\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## WEB APPS\nDeveloper Portal\nWhat: Self-service testing, dashboard, API management\nKey metric: 5-minute signup to first query (vs 6-month Bloomberg)\nDecision: Separate portals vs unified (chose UX over maintenance)\nPublisher Portal\nWhat: License control, revenue tracking (70% share)\nMetric: 2 design partners on demand, 3 design partners on supply\nWhy separate: Publishers need different features than developers\nKunik Internal Tooling\nWhat: Admin overrides, analytics, dispute resolution\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## API LAYER\nAuthentication & Key Management\nWhat: Validates users, manages API keys\nTech: JWT tokens, 1-hour expiry\nTrade-off: Short expiry for security vs user convenience\nAPI Gateway\nWhat: Rate limiting, load balancing, geo-blocking\nRate limit: 60 req/min (tested - higher = abuse, lower = frustration)\nCircuit breaker: Opens at 50% error rate \u2192 serves cache\nScale: 10K req/sec capacity (current: 3K/day)\nDecision: AWS vs Kong \u2192 chose AWS (DDoS protection built-in)\nEndpoints\n/search: Main query endpoint\n/catalog: Browse data sources\nWhy REST: Simple, well-understood\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## DATA INGESTION PIPELINE\nInput Connectors\nWhat: Custom parsers for each data type\nCritical decision: Build vs buy (Fivetran = $30K/month)\nResult: 3-month build saved $360K/year\nPDF parser: 40% better table retention (critical for financial data)\nData Validation & Preprocessing\nWhat: Check format, remove duplicates, score quality\nMetric: Catches 15% malformed data before it enters system\nETL Processing\nWhat: Extract metadata, chunk text, clean artifacts\nChunk size: 700-1000 chars (optimal for LLM context)\nVolume: 50GB daily, 1,600-2,500 documents\nPipeline Orchestration\nWhat: Airflow manages workflow, retries, scheduling\nScale trigger: At 1M docs/day \u2192 switch to Kafka streaming\nLoading and Indexing\nWhat: Write to MongoDB (raw), index in OpenSearch\nParallel processing: Reduces ingestion time by 60%\nFeedback Loop\nWhat: Analyzes what users search for, adjusts priorities\nMetric: 50K queries analyzed daily\nImpact: Popular queries get fresher data\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## DATA ACCESS LAYER\nData Catalogue (PostgreSQL)\nWhat: Metadata, licensing, pricing info\nWhy PostgreSQL: ACID compliance for catalog consistency\nBottleneck: 10K connections (current: 2K)\nVector Database (OpenSearch)\nWhat: Stores embeddings + handles keyword search\nTHE KEY DECISION: OpenSearch vs alternatives\n  Pinecone: Vector-only, need second system for keywords\n  Elasticsearch: $2K/month + licensing issues\n  OpenSearch: Both search types, $400/month\nPerformance: p95 1200ms \u2192 900ms (parallel search)\nScale limit: 50K QPS (current: 5K)\nDocument Store (MongoDB)\nWhat: 50TB raw documents, flexible schema\nWhy MongoDB: Documents change format weekly\nTrade-off: Eventually consistent but flexible\nKnowledge Graphs (Neo4j)\nWhat: Entity relationships (Company \u2192 CEO \u2192 Products)\nWhy: Enables \"Show me all companies led by former Apple execs\"\nObject Store (S3)\nWhat: PDFs, images, large files\nStandard choice: Reliable, presigned URLs for direct access\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## SEARCH & RANKING\nQuery Planning\nWhat: Understands intent, routes to data sources\nDecision: Semantic vs keyword vs both\nRetrieval Connectors\nWhat: Specialized fetchers for each data type\nParallel execution: All sources queried simultaneously\nRanking Functions\nWhat: Rerank 100 results \u2192 top 8\nWhy 8: 60% token reduction, negligible quality loss\nAlgorithm: Cross-encoder for relevance\nResults Synthesis\nWhat: Combines multi-source results with attribution\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## ORCHESTRATION\nPrompts, Chains and Workflows (LangGraph)\nWhat: Handles multi-step queries (\"Compare Apple to Microsoft\")\nWhy LangGraph: Better than LangChain for complex flows\nTools Repository\nWhat: Calculators, formatters for LLM use\nState Management & Memory\nWhat: Tracks conversation context within session\nInference Input and Output Guardrails\nWhat: Blocks harmful queries, validates responses\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## WORKER MANAGEMENT\nJob Scheduling (Celery)\nWhat: Background document processing\nExample: New PDF ingestion happens async\nMessage Queue (Redis)\nWhat: Queue for background tasks\nCapacity: 100K operations/second\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## DATABASE CONTROLLERS\nCRUD Operations\nWhat: Standard create, read, update, delete\nCache Management (Redis)\nWhat: Stores frequent query results\nTTL: 5 minutes (the big trade-off)\n  Publishers wanted: real-time (<1 min)\n  We chose: 5 min for 73% hit rate\n  Saves: $1,200/month\nImpact: 73% queries served from cache (fast + cheap)\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## IAM - THE CONTROVERSIAL DECISION\nUser and Publisher Profiles\nWhat: Who can access what data\nRole Based Access Controls\nWhat: Admin, Publisher, Developer, User permissions\nTHE BIG DECISION: Synchronous permission checks\nCost: +100ms on EVERY query\nBreakdown:\n  20ms: User auth\n  30ms: License check\n  30ms: Document permissions\n  20ms: Section restrictions\nWhy worth it: Compliance approval 6 months \u2192 30 days\nWhat we rejected: Async (risked $50K fines)\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## BILLING & REVENUE\nPayment Service Provider Integration\nWhat: Stripe for payments\nStandard choice: Reliable, well-documented\nPublisher Revenue System\nWhat: Tracks usage, calculates 70% share\nMetric: Monthly payouts, transparent reporting\nCustomer Credits Management\nWhat: Prepaid model, real-time deduction\nPricing: $0.01 public, $0.44 licensed, $6.00 premium\nWhy real-time: Enables self-service model\nSecurity & Compliance\nWhat: PCI compliance, audit logs\nEvery query logged: Complete attribution trail\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## RIGHT SIDE INFRASTRUCTURE\nRealtime Data Sources\nWhat: Live feeds beyond stored documents\nExamples: News APIs, stock prices\nFallback: Cached last-known-good when APIs fail\nMonitoring & Observability\nLogging: CloudWatch/Datadog\nSystem Monitoring: Prometheus/Grafana\nLLM Tracing: LangFuse for debugging\nWhy separate: Different tools for different needs\nAnalytics\nPublisher Performance: Which content gets queried\nUser Behavior: Conversion, usage patterns\nAPI Usage: Per-customer tracking\nModel Inference\nModel Gateway: Routes to appropriate model\nFoundational Models:\n  Claude Sonnet: Accurate (premium queries)\n  Claude Haiku: Fast (public queries)\nEmbedding Models: Ada-002, FinBERT (financial)\nRanking Models: Cross-encoders\n8-chunk limit: 60% cost reduction\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## KEY SYSTEM METRICS\nScale: 3,000 queries/day (ready for 30K)\nLatency: p50 350ms, p95 900ms\nSuccess: 96-99%\nCache hit: 73%\nCost per query: $0.01-$6.00\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "## BOTTLENECKS IN ORDER\nPostgreSQL at 10K connections\nOpenSearch at 50K QPS\nLLM costs without chunk limits\nThis annotation follows your EXACT infrastructure map with the critical details you need for each component.\n",
    "source_id": "2d3a97f0-fe30-80f6-befb-cd11114a879a",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Kunik Infrastructure Overview - to review  (1)"
    ],
    "title": "Kunik Infrastructure Overview - to review  (1)"
  },
  {
    "text": "Hi Heba,\nGreat chatting with you today!\nAs mentioned, here are some helpful resources for the upcoming interviews. Let me know if you have any questions or want to hop on another call before your interviews. You got this!\nInterview Logistics:\nYour virtual on-site is\u00a0\n  five independent 1 hour interviews.\nThe first 5 minutes of each interview is usually making introductions. If you\u2019re going back to back with another interview, you asked to take a 5 minute break if you\u2019d like one.\nThe next 50 or so minutes covers a combination of our Leadership Principle and Functional Competency questions.\nWith any time left, you\u2019ll have the opportunity to ask the interviewer questions.\nhttps://amazon.jobs/content/en/how-we-hire/interview-loop\nhttps://amazon.jobs/content/en/how-we-hire/pm-t-interview-prep\nGetting The Job:\nCriteria #1. Raise our hiring bar for Senior Product Manager - Technical. All data points collected in your interview index to better than 50% of Senior PMTs already at Amazon.\nA bar raising example is\u00a0complex, large in\u00a0scope, somewhat or highly\u00a0ambiguous, and your decisions result in high impact for the business/ your customers.\nDemonstrating\u00a0high judgement\u00a0can raise the bar. (Ie. When making product feature tradeoffs, you decisively advocate for what is best for customers, while simultaneously factoring in risk mitigation for your business.)\nCriteria #2. Demonstrate you can grow within the role throughout your examples.\nInterviewers look for an emergent pattern of being vocally self-critical.\nThis is covered by discussing your\u00a0learnings. You can share them proactively or interviewers will frequently follow up asking what you learned from anecdotes you\u2019ve shared throughout your interview (see STAR-LMethod below).\nInterview Data Collection:\nInterviewers will be typing the whole time and entering notes into a system where all other interviewers can read what they write.\nThe above tip means leadership principle data is supplemental. This means you do NOT have to guess the LP behind the question! If I ask you a \u201cCustomer Obsession\u201d question and don\u2019t get sufficient data, but another interviewer does while questioning another leadership principle, they will tag this in their notes and I can read about it later! All interviewers also discuss counter points during a robust debrief conversation.\nContent Overview:\n~80%\u00a0of your interview will be responding to predictive behavioral questions rooted in our\u00a0leadership principles. There are 16 Leadership principles in total. Only 12 will be covered in your virtual on-site (see competency list below).\n~15%\u00a0will be covering the Functional Competencies.\n~5%\u00a0will be asking the interviewers questions at the end of your interview. Good questions demonstrate that you\u2019re already envisioning yourself within the position.\nInterview Competency Focus Areas\nLeadership Principles (LPs):\nCustomer Obsession\nLearn and Be Curious\nEarn\u00a0Trust\nAre Right, A Lot\nInvent and Simplify\nOwnership\nHave Backbone; Disagree and Commit\nDeliver Results\nInsist on the Highest Standards\nThink Big\nBias for Action\nDive Deep\nEx. Questions aligned to each LP:\nDealing with Ambiguity- Have you ever worked on a project / task whereby you have had minimal information and or minimal direction?\nAre Right A Lot -\u00a0Tell me about a time when you made a bad decision and the learning from the experience enabled you to make a good decision later.\nEarns Trust -\u00a0Give me an example of a tough or critical piece of feedback you received\nHave Backbone -Tell me about a time that you strongly disagreed with your manager on something you deemed to be very important to the business.\nDeliver Results -\u00a0Tell me about a time where you not only met a goal but considerably exceeded expectations.\nCustomer Obsession - Can you give me an example of when you\u2019ve been able to see around the corner to meet a customer need or\nInsist on Highest Standards -\u00a0Tell me about a time when you have been unsatisfied with the status quo.\u00a0 What did you do to change it?\u00a0 Were you successful?\nThink Big -\u00a0Tell me about time you were working on an initiative or goal and saw an opportunity to do something much bigger than the initial focus.\nBias for Action-\u00a0Tell me about a time when you had to analyze facts quickly, define key issues, and respond immediately to a situation. What was the outcome?\nDive Deep -\u00a0Tell me about a time you were trying to understand a problem on your team and you had to go down several layers to figure it out. \u00a0Who did you talk with and what information proved most valuable? \u00a0How did you use that information to help solve the problem?\nOwnership -\u00a0Tell me about a time when you took on something significant outside your area of responsibility\nInvent and Simplify -\u00a0Tell me about a time when you were able to make something simpler for customers.\u00a0What drove you to implement this change? What was the impact?\nLearn and Be Curious -\u00a0What is the coolest thing you have learned on your own that has helped you better perform your job?\nFunctional:\nPM Skills (Strategy/Vision & Product Roadmaps)\nPM Tech Skills\nWriting Sample\nDeal with Ambiguity (not assigned to your interviews, but wanted to share prep just in case)\nInterview\u00a0Verbal and Written Communication\u00a0Prep:\nDo:\n\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Be detail oriented in response. Stay connected to the customer/data in tradeoff explanations and decisions.\n\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Check in with the interviewer, \u201cDoes that answer your question?\u201d is a good way to keep the conversation flowing.\n\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Answer questions directly. Think crisp, plain, business\u00a0communication.\nDo not:\n\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Use jargon or buzzwords.\n\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Stay too high level.\n\u00b7\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Overshare beyond the scope of a question [See STAR Method].\nBest method to answer & common mistakes.\nLeadership Principle (LP) questions will always target specific stories from your past. They often start with \u201cTell me about a time when\u2026.\u201d Or \u201cGive me an example of\u2026\u201d. For example, an LP question targeting Customer Obsession will go \u201cTell me about a time you received negative customer feedback.\u201d What was it? What did you learn from this?\u201d\nSTAR-L Method\u00a0to answer.\nS \u2013 Situation\nSet the stage. Put the interviewer in your shoes. They\u2019ve likely been at Amazon for years and it\u2019s not obvious to them how your organization is structured or who your customers are. Not to mention yellow and red tape you need to navigate due to process and organizational constraints to product planning and execution.\nTalk up complexity, scope, and ambiguity. This directly correlates with how \u201cbar raising\u201d your example is.\nT \u2013 Task\nClearly explain what you were\u00a0trying\u00a0to accomplish.\nA \u2013 Action\nDelineate \u201cI\u201d vs \u201cWe. Interviewers are talking notes on you, and not your team. So please be sensitive when you say \u201cI\u201d or \u201cWe\u201d when describing actions that occurred in relation to the task that you set out to accomplish. For example, \u201cMy team came up with the original idea to do X and I thought to add YZ because of the customer data\u2026\u201d.\nR \u2013 Results\nQuantifiable data \u2013 using numbers or metrics to communicate your results is often the most compelling and clear way to communicate how you/ your team performed in relation to a Task. Be ready to discuss any weaknesses of KPIs/metrics if prompted. I recommend having key figures\u00a0prepared\u00a0to reference within your interview. Please be careful not to read too much from pre-prepared notes.\nAnecdotal evidence \u2013 It\u2019s OK to not have numbers to share for every question you\u2019re asked. For example, an Earn Trust question can be, \u201cTell me about a time you did not see eye to eye with a stakeholder.\u201d Be\u00a0prepared\u00a0to share how you listened first, then influenced positive change based finding and moving towards common goal (ie. What\u2019s best for the customer, data driven product roadmap, cost savings, etc.)\nL \u2013\u00a0Learnings\nVolunteering\u00a0learnings\u00a0and demonstrating the capacity of being vocally self-critical can satisfy hiring criteria #2.\nA common follow up question is, \u201cIf you had to do it all over again, what would you have done differently?\u201d More bluntly, candidates are frequently asked, \u201cDid you learn anything from this?\u201d\nFeel free to share a\u00a0learning\u00a0or two if you feel it\u2019s relevant. Maybe your team fell short of a goal or late to a deadline. Whatever you do, take ownership, be humble, and never point the finger at anyone else. I like to say, you\u2019re either winning or you\u2019re\u00a0learning!\nFunctional:\nPM Skills\u00a0- This is usually a deep dive into a past product you\u2019ve launched. Be ready to discuss product definition, prioritization, feature tradeoffs, and stakeholder management. Be connected to long term product strategy and how you\u2019ve experimented and measured impact.\nYou will receive questions around your end to end product experience- so be prepared to talk about Internal tools used \u2013 who were your customers, what was the scope, how the scope changed, what is your impact to customers, revenue impact of the product; it\u2019s Scalability - how you scaled the impact of your product to 1 to 2 to 20, how do you iterate?\u00a0 Talk about roadmaps; your ability to deal with ambiguity.\u00a0 How do you identify risks, how do you foresee an unknown risk or mitigate risk?\u00a0 How do you influence other teams, how do you communicate with cross-functional teams, your stakeholders, leadership, how do you get your buy-in or earn trust from other teams?\nTech Skills\u00a0- need to demonstrate you\u2019re connected to the technical details and can drive the right customer outcomes through interactions with your technical stakeholders.\nCommon questions: \u201cTell me about the biggest technical challenge you\u2019ve ever handled\u201d. \u201cTell me about a time you pushed back on an engineering decision.\u201d \u201cWalk me through the technical design choices of a product you\u2019ve launched.\u201d\u00a0Prepare\u00a0to discuss how you balanced needs for internal customers and end users (external customers). Ideally you can explain a high level system design and how it affects customer behavior.\nUnderstands the reasons behind product/feature architecture decisions\u00a0(Q. Why did we build it this way? What business or technical assumptions were made? Do we need to build something else\u2013 if so why?).\nCapable of holding an engineering team to a high standard for product solutions (similar to a technical manager). Recognize when technical design of a feature limits a product\u2019s performance, ability to scale, and/or our ability innovate for customers in the future.\nAble to discern which solutions require additional technical guidance (e.g., from senior engineers or subject matter experts).\nUnderstands the impact a technical backlog has on the future of a product and the ability for development teams to execute. Knows how best to prioritize operational excellence work alongside feature delivery on a roadmap.\nAble to assess emerging technologies and how to apply them to a product/feature use cases.\nAdditional\u00a0considerations:\nDependency management\nPain points post launch?\nTech details of product in relation to the customer experience (latency, caching, scaling..)\nArticulate tech tradeoffs and reason prioritization for customers.\nWriting Sample\nAttack one of two prompts. Both weighted the same. Make sure you address the entire prompt.\nHow you write is just as important as what you write.\n1 - 1.5 pages is perfectly long enough. Don\u2019t worry that the prompt asks for more.\nPlain business language is perfect.\nMake sure paragraphs flow well into each other.\nStructure matters, the STAR method is a good framework to use.\nGood sentence syntax and no typos.\nExpect to be asked follow up questions by the hiring manager about what you write.\nUse the strongest example you can.\nDEALING WITH AMBIGUITY\nOperates effectively in unstructured environment\nAble to make decisions when all the information is not available\nDrives toward problem resolution despite ambiguity or uncertainty\nUnderstands how to get things done\nSeeks clarity where necessary\nDemonstrates flexibility\nDrives forward initiatives\nAbility to multi-task\nSample Ambiguity Questions\nHave you ever worked in a job with a high level of ambiguity? (*Can you describe what that was like? Can you give me an example of how you handled the situation?)\nWhat has been your experience in working with conflicting, delayed or ambiguous information? (*What did you do to make the most of the situation? )\nTell me about a time when you have needed to quickly change direction? What was the situation?\nHave you ever worked on a project / task whereby you have had minimal information and or minimal direction? How did you manage the situation?\nTell me about a time when a decision was made or a course of action was taken that you did not agree with. (*How did it affect your planning? What were the other, better alternatives? Why weren\u2019t they explored or implemented?)\nDescribe a time when you had a plan in place and circumstances changed requiring you to take a different approach. (*How did you adjust your plan to respond to the new requirements? What were the costs associated with making the change? How could the situation have been avoided?)\nMiscellaneous Tips\nYou can use the\u00a0same example up to two (2) times. If you use the same example again,\u00a0\u00a0Interviewers will read about it anyway.\n  call out you already used it!\nInterviewers don\u2019t comment on your responses. They\u2019re trained to do this as to not bias the interview. We\u2019re not robots. Don\u2019t let this throw you off!\n",
    "source_id": "2d3a97f0-fe30-80a8-8fc4-f8605ba74822",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Email from Devon (1)"
    ],
    "title": "Email from Devon (1)"
  },
  {
    "text": "  10/7 10-11am: Colby Lavin - Leadership - https://www.linkedin.com/in/colbylavin/\n  10/7 11am-12pm: Sai Sundar - Product Vision/Strategy - https://www.linkedin.com/in/snsundar/\n  10/7 2:30-3:330pm: Sundeep Patath - Product Roadmap - https://www.linkedin.com/in/sundeep-patath/\n  10/10 10-11am: Kam Bancil - Tech - https://www.linkedin.com/in/kam-bancil-93460038/\n  10/10 1-2pm: Ushma Bhatia - Leadership - https://www.linkedin.com/in/ushma-bhatia-1a09084/\n",
    "source_id": "2d3a97f0-fe30-8087-b6a3-dd8a81f05c69",
    "path": [
      "Interview Prep",
      "Amazon Interview"
    ],
    "title": "Amazon Interview"
  },
  {
    "text": "## Creating Usage-Based Infrastructure for Proprietary Financial Data\nDuring my tenure as Entrepreneur in Residence at Temasek's venture studio, working within a $400 billion sovereign wealth fund, I created the first marketplace that enabled AI developers to access proprietary financial data on a pay-per-query basis with embedded compliance tracking. This wasn't another data aggregator or API wrapper. It was infrastructure that solved both sides of a broken market: developers forced to purchase complete data catalogs at $500,000 annually when they only needed specific subsets, and publishers watching their content get consumed by AI without compensation or control.\nThe problem emerged through customer discovery with twenty-five AI development teams. Quantitative trading firms were purchasing entire data catalogs from traditional providers when they only needed specific segments, then dedicating 80% of engineering resources to preprocessing that data for AI consumption. Meanwhile, publishers watched AI companies scrape their paywalled content with no attribution or compensation. The traditional licensing model was designed for human analysts and fundamentally mismatched how AI systems actually consume data.\nTemasek recruited me because they recognized that rights management principles from music could transform financial data monetization. Having built royalty payment systems at Warner Chappell Music that processed $2 billion in annual payments, I understood how to track, attribute, and compensate for granular usage at scale. The same infrastructure that enabled streaming services to pay artists fractions of pennies per stream could enable AI applications to pay data providers fractions of dollars per query.\nThe market had fragmented solutions but no integrated approach. Platforms offered usage-based pricing for public data only, semantic search for generic content, or publisher monetization without financial data partnerships. The critical gap was proprietary, paywalled financial content with embedded compliance. I recognized that solving proprietary licensing with compliance tracking would become critical infrastructure for how AI systems access regulated data in financial services.\nI designed Kunik to operate as infrastructure, not just a product. Leading a four-person team, I built enough conviction in the vision that all four members committed to leaving the venture studio to join the independent company when we spun out. I orchestrated three parallel workstreams: securing data partnerships with established financial and business publishers, building technical systems that made heterogeneous data AI-ready, and creating unit economics that worked for both sides of the marketplace. The core innovation was recognizing that developers didn't need another search engine - they needed a compliance layer that made it safe to consume proprietary data programmatically. By embedding attribution, audit trails, and licensing enforcement directly into the API, I eliminated the six-month legal review that typically blocked AI teams from accessing premium content.\nThe most critical technical decision involved compliance architecture. My engineering lead advocated for asynchronous permission checking to optimize for sub-100ms response times. I mandated synchronous validation, adding 100 milliseconds to every request to check user permissions, data licenses, territorial restrictions, and audit requirements before returning results. The team argued this would hurt performance, but I held firm. For financial services customers, compliance isn't optional - it's existential. The audit trail I built reduced our enterprise compliance approval timeline from six months to thirty days, enabling faster sales cycles with regulated institutions.\nThe second critical decision involved our go-to-market strategy. The Temasek partners argued nobody would pay for data that was technically free through EDGAR and wanted to wait until we had multiple proprietary sources. I pushed back, explaining that developers weren't paying for the data itself but for the preprocessing, search capabilities, and unified access. Our quantitative fund design partner validated this. While they could get SEC filings through Bloomberg, they couldn't test specific datasets before purchasing entire catalogs. Starting with enriched public data would prove our infrastructure value while we built proprietary partnerships. During our two-month pilot, we processed 3,000 queries daily, with developers specifically citing the value of plug-and-play SEC access for backtesting.\nFor data providers, I created transparency and control. Publishers could see exactly how their content was being used, set granular permissions, and receive seventy percent of revenue generated from their content. They could finally monetize specialized financial content that was valuable for specific AI applications but didn't justify enterprise subscriptions.\nBuilding custom data connectors saved $360,000 annually while giving us precise control over data quality. Implementing hybrid search solved the core problem where financial professionals needed both exact document matching and conceptual discovery.\nThe early validation exceeded expectations. We processed 3,000 test queries daily from design partners. The quantitative trading firm valued the ability to test datasets before committing and appreciated the unified API. An investor intelligence platform used our private market data integration to access information that previously required a $50,000 annual subscription. However, we encountered the classic two-sided marketplace challenge: developers wanted more proprietary sources before committing, while publishers wanted proven developer demand.\nSeven months in, as we prepared to spin out from the venture studio, my co-founder and I faced a critical decision point. The entire team had committed to joining the spin-out and we had validated demand through our pilot. However, the market landscape was shifting. Anthropic had announced Model Context Protocol in November 2024, a standardized way to connect AI systems to data sources. While MCP didn't provide the compliance tracking I'd built, it would make accessing enriched public SEC data significantly easier. Our strategy had been to prove value with public data while building proprietary partnerships, but publishers still needed 12 to 18 months to develop frameworks for programmatic AI licensing. Combined with new leadership at the fund questioning our timeline, we recognized that our window for the wedge strategy was closing faster than the publisher market was developing. As we sat down to finalize equity agreements, we weren't aligned on the risk under these shifting conditions. Rather than force a founding partnership without clear stakeholder backing, we jointly decided to pause.\nThe core lesson was about infrastructure adoption timing and go-to-market sequencing. I underestimated how quickly standardized protocols like MCP would gain adoption. What I thought was an 18-month window compressed to 6 months. More critically, infrastructure plays serving regulated industries need an anchor sponsor: a major financial institution willing to adopt early and validate the model. Publishers required minimum revenue guarantees, which meant I needed committed buyer demand before I could secure supply. In retrospect, securing a tier-one financial institution as a launch partner who could commit to minimum spend would have unlocked publisher partnerships. The market has since validated the need. I've tracked three venture-backed teams building similar infrastructure, confirming the innovation was sound but the sequencing needed refinement.\n",
    "source_id": "2d3a97f0-fe30-80b1-8646-c7ec5e1c95a9",
    "path": [
      "Interview Prep",
      "Amazon Interview",
      "Written assessment - submitted (2)"
    ],
    "title": "Written assessment - submitted (2)"
  },
  {
    "text": "## 90-SECOND OPENER (Same for All 3)\n\"I build infrastructure for complex, regulated domains. At Warner Chappell, I managed the royalty platform processing $2B annually for 80,000+ songwriters - I brought ML-powered matching that improved accuracy from 80% to 95% and drove writer portal adoption from 30% to 75%.\nAt Imbr, I reimagined the entire royalty model - eliminating intermediaries so songwriters could get paid in 120 days instead of 730 days and keep 90% of royalties instead of 47%.\nMost recently at Temasek, I built Kunik - an AI-powered marketplace for financial data with embedded compliance. We processed 3,000 queries daily and closed a $50K pilot with a quant firm.\nI'm excited about Amazon Music because you're solving the exact problems I've built at scale: modernizing legacy royalty infrastructure while building for the future of AI-generated content. This role combines everything I've done at enterprise scale.\"\n",
    "source_id": "2d3a97f0-fe30-8087-b6a3-dd8a81f05c69",
    "path": [
      "Interview Prep",
      "Amazon Interview"
    ],
    "title": "Amazon Interview"
  },
  {
    "text": "## What I Built at Kunik - Eval Framework One-Sheet\n  The Problem I Was Solving\n  We had a RAG system serving financial data to developers. Early accuracy was 78% - meaning 1 in 5 queries returned bad results. For customers making trading decisions in regulated industries, that\u2019s not good enough. I needed a way to measure quality systematically so we could improve.\n  What a RAG System Actually Does (Two Steps)\n  Step 1: Retrieval - User asks a question, system searches the database and pulls back relevant documents. This is finding the right information.\n  Step 2: Generation - The retrieved documents get passed to an LLM (like Claude) which reads them and generates an answer. This is synthesizing the information.\n  When something goes wrong, it\u2019s either a retrieval problem (wrong docs, bad ranking) or a generation problem (LLM hallucinated or misunderstood good docs).\n  What an Eval Harness Is\n  A script that automates quality testing. Instead of manually reviewing every query, you run 50-100 queries at once, get a score, make a change, run again, see if the score improved.\n  It has three parts: a test set (queries + expected answers), a runner (code that executes queries and captures outputs), and scoring logic (compares outputs to expected answers and calculates metrics).\n  What I Spec\u2019d (My Job as PM)\n  The test queries - Real questions from customer interviews. \u201cWhat\u2019s the valuation of X?\u201d \u201cWhat are revenue trends for Y?\u201d Categorized by public vs private markets.\n  The expected answers (ground truth) - I researched and documented correct answers for each query. Verified against source data. This is where I became the domain expert.\n  Scoring criteria - Defined what \u201ccorrect\u201d means. Is a partial answer acceptable? How recent does data need to be? What\u2019s the tolerance for numerical accuracy? Documented these rules so scoring was consistent.\n  Metadata to capture on failures - The query, expected answer, actual response, which documents were retrieved, ranking order, latency. This let me diagnose WHY something failed.\n  Success threshold - \u201cWe need 90%+ accuracy before enterprise pilots.\u201d Based on what regulated industry customers would accept.\n  The Workflow\n  Step 1: I created the test set - queries and correct answers in a spreadsheet.\n  Step 2: Engineer ran the harness - script loops through queries, hits RAG system, captures responses, compares to expected.\n  Step 3: I got a failure report - 78% accuracy, here are the 11 that failed with metadata.\n  Step 4: I analyzed failures - looked at each one, tagged it as retrieval problem, ranking problem, or generation problem.\n  Step 5: I prioritized for engineering - \u201c7 of these are ranking issues, 3 are retrieval misses, 1 is hallucination. Ranking is highest priority.\u201d\n  Step 6: Engineering fixed, we re-ran - they adjusted hybrid search weighting, re-ran harness, accuracy improved to 85%. Repeat until target.\n  How I Diagnosed Failures\n  Retrieval problem: Wrong documents came back entirely. Asked about Tesla stock, got Tesla Energy. Fix is on the search side.\n  Ranking problem: Right documents exist but ranked too low. Answer was result #47 instead of #1, or surfaced 2023 data when 2024 exists. Fix is tuning the hybrid search balance.\n  Generation problem: Right documents retrieved but LLM misinterpreted. Hallucinated, pulled wrong number from table, or gave generic response instead of answering. Fix is on the prompt side.\n  Most of our issues were ranking - we had trustworthy sources (connected to publisher APIs) but weren\u2019t surfacing the most relevant/recent data first.\n  Hybrid Search (Why It Matters)\n  We balanced two search methods:\n  Semantic search - Finds conceptually similar content. Good for understanding intent. \u201cTesla valuation\u201d might match \u201cTSLA market cap\u201d even though words are different.\n  Keyword search (BM25) - Finds exact matches. Good for specific terms like company names, ticker symbols, dates.\n  If keyword weighted too heavily, you miss conceptual matches. If semantic weighted too heavily, results are vague. We tuned this balance based on eval results.\n  The Feedback Loop\n  When customers flagged bad responses in production, we\u2019d add that query to the test set. Now the harness tests for that case going forward. Every real-world failure becomes training data for the next iteration.\n  Metrics I Used\n  Precision - Of the results returned, how many were actually relevant?\n  Recall - Of all relevant results that exist, how many did we find?\n  Latency - How fast is the response?\n  Accuracy - Overall pass rate on the test set.\n  The Result\n  Went from 78% to 96% accuracy. But more importantly, we had a repeatable system - every bad response became data for the next version. That\u2019s what makes evaluation systems actually work.\n  Key Insight\n  \u201cI\u2019d start with eval harnesses FIRST, not architecture. Understanding how to measure quality is more important than understanding how systems work. Once you know how to evaluate, you can test different approaches empirically instead of debating architecturally.\u201d\n",
    "source_id": "2d3a97f0-fe30-80e5-87e4-c3babce9a099",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Claude chat (1)"
    ],
    "title": "Claude chat (1)"
  },
  {
    "text": "## Who built the eval harness?\n  Your AI engineer built it. You didn't write the Python code. But you spec'd it - you told him what it needed to do.\n  The eval harness is just a script that:\n  Reads a list of test queries from a file (like a CSV or JSON)\n  Runs each query through your RAG system\n  Captures the response\n  Compares it to the expected answer\n  Logs the results (pass/fail, what was returned, what was expected)\n  Your engineer wrote the code. You defined the inputs - the test queries and the expected answers.\n  How did you use the eval harness?\n  Here's the actual workflow, step by step:\n  Step 1: You created the test set\n  You and your PM gathered questions from customer interviews. You documented them somewhere - probably Notion or a spreadsheet. Questions like \"what's the valuation of Tesla\" or \"can you predict X.\"\n  Then you researched and wrote down what the correct answer should be for each query. This is the ground truth. You became the domain expert because you had to verify what \"right\" looked like.\n  So now you have a file with two columns: QUERY and EXPECTED_ANSWER. Maybe 50 rows, maybe 200. This is your test set.\n  Step 2: Engineering runs the harness\n  Your engineer takes that test set file, plugs it into the eval harness, and runs it. The script loops through every query, hits your RAG system, captures what comes back, and compares it to the expected answer.\n  At the end, you get a report. Something like:\n  78% accuracy (39 out of 50 passed)\n  Here are the 11 that failed\n  For each failure: the query, what we expected, what we got\n  Step 3: You analyze the failures\n  This is where you come back in. You look at the 11 failures and ask: why did these fail?\n  You're not looking at code. You're looking at the output. You see:\n  Query: \"What's Tesla's current market cap?\"\n  Expected: \"$800B as of Q3 2024\"\n  Got: \"Tesla Energy reported revenue of $2.3B\"\n  That's obviously a retrieval problem - we pulled the wrong documents entirely.\n  Or you see:\n  Query: \"What's Tesla's current market cap?\"\n  Expected: \"$800B as of Q3 2024\"\n  Got: \"$650B based on 2023 data\"\n  That's a ranking problem - we had the right data but surfaced the outdated version.\n  Or you see:\n  Query: \"What's Tesla's current market cap?\"\n  Expected: \"$800B as of Q3 2024\"\n  Got: \"Tesla is a leading electric vehicle manufacturer founded by Elon Musk\"\n  That's a generation problem - we probably retrieved the right docs but the LLM ignored the question and gave a generic summary.\n  Step 4: You categorize and prioritize\n  You go through all 11 failures and tag them: retrieval, ranking, generation, data quality. Now you can say to your engineer:\n  \"7 of these are ranking issues - we're surfacing old data over new data. 3 are retrieval misses - wrong documents entirely. 1 is a generation hallucination.\"\n  Step 5: Engineering fixes, you re-run\n  Engineering makes a change - maybe they adjust the date weighting in the ranking algorithm so newer documents score higher. Then they re-run the harness with the same test set.\n  Now you get: 85% accuracy (42 out of 50). You fixed 3 of the ranking issues. Repeat until you hit your target.\n  What you actually did, in plain language\n  You didn't build the harness. You didn't run the harness. But you:\n  Created the test cases (the queries and correct answers)\n  Reviewed the failure reports\n  Diagnosed what kind of failure each one was\n  Prioritized which failures mattered most to customers\n  Brought that analysis to engineering so they knew what to fix\n  The harness was the tool. You were the one who made it useful by defining what to test and interpreting the results.\n  One more thing\n  When a customer flagged something in production (not in your test set), you'd add that query to your test set. Now your harness tests for that case going forward. That's the feedback loop - every real-world failure becomes a new test case.\n  Does this make the handoff clearer? Can you walk me through it in your own words now?\n",
    "source_id": "2d3a97f0-fe30-80e5-87e4-c3babce9a099",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Claude chat (1)"
    ],
    "title": "Claude chat (1)"
  },
  {
    "text": "## Retrieval vs Generation - what's the difference\n  Think of your RAG system as a two-step process:\n  Step 1: Retrieval\n  The user asks a question. The system searches your database to find relevant documents or chunks of data. This is the \"R\" in RAG - Retrieval.\n  At Kunik, you had financial data from publishers. When someone asked \"what's the valuation of Tesla,\" the retrieval step would search across your connected data sources and pull back chunks that might contain the answer.\n  Retrieval problems look like:\n  Wrong documents came back (you asked about Tesla stock, got Tesla Energy)\n  Right documents exist but weren't found (the answer is in your database but search missed it)\n  Right documents found but ranked too low (answer was result #47 instead of result #1)\n  Step 2: Generation\n  Once you have the retrieved documents, you pass them to an LLM (like Claude) with a prompt that says something like: \"Based on this context, answer the user's question.\"\n  The LLM reads the retrieved chunks and generates an answer.\n  Generation problems look like:\n  LLM hallucinated (made up information that wasn't in the context)\n  LLM misunderstood the data (pulled the wrong number from a table)\n  LLM gave a confident answer when it should have said \"I don't know\"\n  LLM summarized poorly (lost important nuance)\n  Why this matters for debugging\n  When your eval harness flagged a wrong answer, you had to figure out which step broke.\n  If it was retrieval: The LLM never had a chance. You gave it bad inputs, so it gave bad outputs. The fix is on the search side - adjusting embeddings, tuning hybrid search weighting, improving chunking.\n  If it was generation: The LLM had good inputs but still produced bad outputs. The fix is on the prompt side - better instructions, clearer formatting, maybe few-shot examples.\n  What you did vs what engineering did\n  Here's how the handoff worked based on what you told me:\n  Your job as PM:\n  Define what \"wrong\" means (you verified answers, became the domain expert)\n  Identify patterns in failures (are we seeing mostly retrieval problems or generation problems?)\n  Prioritize which failures matter most (a wrong valuation number is worse than a slightly outdated date)\n  Bring findings to engineering with evidence (\"these 15 queries failed, and 12 of them look like ranking issues\")\n  Engineering's job:\n  Diagnose the technical root cause (why is the ranking off? is it the BM25 weighting? the embedding model?)\n  Implement the fix (adjust parameters, change the chunking strategy, modify the prompt)\n  Re-run the eval to validate\n  You weren't writing the fix. But you were the one who figured out what was broken and why it mattered. Engineering couldn't prioritize without you telling them which failures hurt customers most.\n  On your ranking point\n  You said accuracy wasn't always the issue - it was more about ranking. That's a retrieval problem, and it's actually a sophisticated insight.\n  Your sources were trustworthy (connected directly to publisher APIs). So the data itself was good. But if the system ranked an outdated document higher than a recent one, or surfaced a less relevant chunk before a more relevant one, the answer would be wrong even though the right data existed.\n  This is why you tuned hybrid search. You were balancing:\n  Semantic search (finds conceptually similar content, good for understanding intent)\n  Keyword search / BM25 (finds exact matches, good for specific terms like company names or dates)\n  If keyword search was weighted too heavily, you'd miss conceptual matches. If semantic search was weighted too heavily, you'd get vague results that weren't specific enough.\n  What you should know for tomorrow\n  You don't need to explain the math behind BM25 or how embeddings work. But you should be able to say:\n  \"When we found wrong answers, I'd first figure out if it was a retrieval problem or a generation problem. If the right data wasn't being surfaced - or was ranked too low - that's retrieval. If the right data was there but the LLM misinterpreted it, that's generation. Most of our issues were retrieval - specifically ranking. We'd tune the hybrid search balance and re-run evals to see if scores improved.\"\n  Does this click now? Any part still fuzzy?\n",
    "source_id": "2d3a97f0-fe30-80e5-87e4-c3babce9a099",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Claude chat (1)"
    ],
    "title": "Claude chat (1)"
  },
  {
    "text": "## What you actually built (and learned)\n  You taught yourself to write eval harnesses in Python. This was new for you as a PM - you weren't an engineer, but you learned enough to set up the infrastructure.\n  An eval harness is basically:\n  A test set - A collection of queries with known correct answers (your ground truth)\n  A runner - Code that takes each query, runs it through your RAG system, and captures the output\n  Scoring logic - Code that compares the output to the expected answer and calculates metrics (precision, recall, etc.)\n  Logging - Tracking which queries passed, which failed, and why\n  The point is automation. Instead of you and your engineer manually reviewing every query, you could run 50 or 100 queries at once, get a score, make a change, run again, see if the score improved.\n  Let me help you remember the workflow\n  Based on what you told me earlier:\n  Step 1: You gathered test queries from customer interviews\n  Real questions like \"what's the valuation of X\" - categorized by public vs private markets.\n  Step 2: You created ground truth answers\n  You and the PM researched what the correct answers should be. This was manual - you had to become the domain expert.\n  Step 3: You ran queries through the harness\n  Your engineer would run the eval. The harness would:\n  Take a query\n  Pass it to the RAG system\n  Capture the response\n  Compare it to ground truth\n  Score it (correct/incorrect/partial)\n  Log the result\n  Step 4: You'd analyze the failures\n  When something scored wrong, you'd look at:\n  Did we retrieve the right documents?\n  Was the ranking correct?\n  Did the LLM synthesize correctly?\n  Step 5: You'd tune and re-run\n  Engineering would make a fix (adjust hybrid search weighting, change chunking, modify the prompt), then you'd re-run the eval to see if scores improved.\n  The key insight you learned\n  You said: \"I'd start with eval harnesses FIRST (Week 1), not RAG architecture. Understanding how to measure quality is more important than understanding how systems work. Once you know how to evaluate, you can test different approaches empirically instead of debating architecturally.\"\n  This is gold for tomorrow. Ram is hiring someone to build evaluation systems. You learned - the hard way - that eval is the foundation. You can't improve what you can't measure.\n  Now let me ask you again\n  Question 1: When your harness flagged a query as \"wrong,\" what made it wrong? Was it checking exact match? Semantic similarity? Did a human review it?\n  Question 2: What were the actual metrics your harness calculated? You mentioned precision and recall - do you remember what those meant in your context?\n  Question 3: The repo is currently just a placeholder. Do you want to actually flesh it out before tomorrow so you can show Ram something concrete? Or is the story enough?\n",
    "source_id": "2d3a97f0-fe30-80e5-87e4-c3babce9a099",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Claude chat (1)"
    ],
    "title": "Claude chat (1)"
  },
  {
    "text": "## Let's rebuild your mental model of what you did\n  The setup:\n  You had a RAG system that took questions about financial data and returned answers. The quality was inconsistent - sometimes great, sometimes wrong. You needed a way to measure accuracy so you could improve it.\n  The team:\n  Just you, one PM you managed, and an AI engineer. Scrappy.\n  Where the test questions came from:\n  Customer interviews. Real questions people actually wanted answered - \"what's the valuation of X,\" \"can you predict Y,\" questions about public and private market investments. This is important because you weren't testing on synthetic queries - you were testing on real use cases.\n  The evaluation process you ran\n  Let me walk you through what you did, step by step:\n  Step 1: You gathered real questions\n  From customer interviews, you documented the kinds of questions users would actually ask. You categorized them - public markets vs private markets, valuation questions vs prediction questions, etc.\n  Step 2: You ran the questions through the system\n  Your AI engineer would take the documented questions and run them through Kunik. The system would return answers.\n  Step 3: You manually reviewed the outputs\n  This is where you became the domain expert. You and the engineer would look at each answer and ask: is this right? You'd research what the correct answer should be - checking against the source data, validating the logic, confirming the citations were accurate.\n  Step 4: You scored the responses\n  For each query-response pair, you'd mark it as correct, incorrect, or partially correct. You were building a labeled dataset - ground truth that you could measure against.\n  What happened when something was wrong\n  This is the part you're fuzzy on. Let me help you think through it.\n  When you found a wrong answer, you had a few options:\n  Option A: It was a retrieval problem\n  The system pulled the wrong documents. Maybe it searched for \"Tesla valuation\" but returned data about Tesla Energy instead of Tesla stock. The fix here is on the search/retrieval side - adjusting how the system finds relevant content.\n  Option B: It was a ranking problem\n  The right document was in the results, but it was ranked too low. The system surfaced less relevant content first. This is where you'd tune the hybrid search weighting - maybe keyword search was too dominant, or semantic search wasn't capturing the right meaning.\n  Option C: It was a generation problem\n  The system retrieved the right documents but the LLM synthesized a bad answer. Maybe it hallucinated, or it misunderstood the data, or it gave a confident answer when it should have said \"I don't know.\"\n  Option D: It was a data problem\n  The source data itself was incomplete or wrong. No amount of retrieval or generation tuning would fix it - you needed better data.\n  The feedback loop\n  When a customer flagged something, here's what likely happened:\n  The flag came in (via your product, or they told you directly)\n  Your engineer would log it in Notion - the query, the response, and what was wrong\n  You'd research what the correct answer should be\n  You'd add that query-response pair to your labeled dataset (now you have one more example of ground truth)\n  You'd diagnose which part of the pipeline failed\n  Engineering would make a fix\n  You'd re-run your test set to see if accuracy improved\n  Each bad response became training data for the next iteration. That's the loop.\n  Now let me ask you some questions to make sure you've got this\n  Question 1: When you and your engineer were reviewing answers, what specifically made you say \"this is wrong\"? Was it factually incorrect? Missing information? Wrong source cited?\n  Question 2: You said you were optimizing for accuracy over speed because it was regulated industries. What did \"accurate\" mean in your context? Was it about the number being right? The source being trustworthy? Both?\n  Question 3: When you found a retrieval problem vs a generation problem, did you handle them differently? Who fixed what?\n  Take a minute and actually answer these - not for me, but so you can articulate this tomorrow without sounding like you memorized something.\n",
    "source_id": "2d3a97f0-fe30-80e5-87e4-c3babce9a099",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Claude chat (1)"
    ],
    "title": "Claude chat (1)"
  },
  {
    "text": "## What Zoom is actually hiring you to do\n  Ram runs the Zoom Virtual Agent team. A Virtual Agent is basically a chatbot that handles customer support - someone types \"how do I reset my password\" and the AI tries to resolve it without escalating to a human.\n  The problem with these systems is: how do you know if they're actually good? A customer might get an answer, but was it the right answer? Was the tone appropriate? Did they actually solve their problem or did they just give up?\n  That's what this role is about. You're building the systems that measure and improve the quality of AI conversations at scale. Not just dashboards - the actual engine that evaluates whether the Virtual Agent is doing its job.\n  What an evaluation framework actually is\n  Think of it like this: you have an AI system producing outputs. Those outputs could be good or bad. An evaluation framework is the system you build to programmatically assess quality so you can improve over time.\n  It has three core components:\n  1. Ground truth (what \"good\" looks like)\n  You need labeled examples. At Kunik, you had 1,200 query-response pairs that were scored by domain experts. Someone looked at a query, looked at the response, and said \"this is correct\" or \"this is wrong\" or \"this is partially correct.\"\n  Without this, you're flying blind. You can't measure accuracy if you don't know what the right answer is.\n  Question for you: At Kunik, who did the labeling? How did you decide what criteria to score on?\n  2. Metrics (how you quantify quality)\n  You need numbers that tell you if things are getting better or worse. At Kunik you used:\n  Precision - of the results returned, how many were actually relevant?\n  Recall - of all the relevant results that exist, how many did we find?\n  Latency - how fast is the response?\n  Cost per query - how much does each request cost us?\n  For Zoom's Virtual Agent, the metrics would be different. They care about things like:\n  Containment rate - did the Virtual Agent resolve the issue without escalating to a human?\n  First contact resolution (FCR) - was the problem solved on the first try?\n  CSAT - did the customer rate the interaction positively?\n  Average handle time (AHT) - how long did it take?\n  Question for you: Why did you pick precision and recall specifically? What were you trying to optimize for?\n  3. Feedback loops (how the system improves)\n  This is the part most people miss. An evaluation framework isn't a one-time thing - it's a system that continuously improves the AI.\n  At Kunik, when a customer flagged a bad response, that feedback went back into your labeled dataset. Now you have a new example of what \"bad\" looks like. You can retrain, retune, or adjust based on real user signals.\n  Question for you: Walk me through exactly how that feedback loop worked. Customer flags something - then what happens?\n  How you worked with engineering\n  You weren't writing the code. But you were the one who figured out where failures were happening.\n  The Kunik system had a pipeline:\n  Query comes in\n  Retrieval - the system searches for relevant documents (this is where hybrid search happens - combining semantic/vector search with keyword/BM25 search)\n  Chunking - the retrieved documents get broken into pieces\n  LLM synthesis - Claude or GPT takes the chunks and generates a response\n  When accuracy was 78%, you didn't know which step was broken. The evaluation framework helped you isolate it. You'd look at failed queries and trace them through the pipeline:\n  Did we retrieve the right documents? (retrieval problem)\n  Did we chunk them correctly? (chunking problem)\n  Did the LLM synthesize a good answer from good inputs? (generation problem)\n  You found most failures were in the hybrid search layer - the balance between semantic and keyword search was off. You brought that finding to engineering, showed them which queries were failing and why, and they adjusted the weighting.\n  Question for you: Can you explain what hybrid search is in your own words? Why do you need both semantic and keyword search?\n  What Ram is going to want to hear tomorrow\n  He's not going to quiz you on technical definitions. He wants to know:\n  Have you actually built evaluation systems, or did you just manage people who did?\n  Do you understand why this is hard?\n  Can you think through the problem from first principles?\n  The hardest part of evaluation isn't the math - it's defining what \"good\" means. For a Virtual Agent, is a partial answer acceptable? What if the customer got their answer but the tone was robotic? What if the AI was accurate but the customer still escalated because they wanted to talk to a human?\n  These are judgment calls. You have to make them, document them, and be consistent. That's what makes evaluation systems actually work.\n  Now - answer my questions above so we can make sure you actually understand this, not just have answers memorized.\n",
    "source_id": "2d3a97f0-fe30-80e5-87e4-c3babce9a099",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Claude chat (1)"
    ],
    "title": "Claude chat (1)"
  },
  {
    "text": "## PART 7: IF HE ASKS ABOUT FIRST 30 DAYS\n  Week 1-2: Learn the current system - how do they evaluate today, what metrics exist, what's the data infrastructure\n  Week 2-3: Talk to stakeholders - engineering, customer success, support teams - understand where quality issues are actually surfacing\n  Week 3-4: Identify gaps and start proposing a roadmap for what to build or improve\n",
    "source_id": "2e6a97f0-fe30-8018-9618-f99c46120c7b",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Old"
    ],
    "title": "Old"
  },
  {
    "text": "## PART 8: IF SALARY COMES UP\n  Range posted is $146K-$339K. I want $230K+.\n  If he asks: \"I'm targeting base in the $230-250K range depending on the full comp package.\"\n  Don't negotiate in first call. Just set the anchor and move on.\n",
    "source_id": "2e6a97f0-fe30-8018-9618-f99c46120c7b",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Old"
    ],
    "title": "Old"
  },
  {
    "text": "## PART 9: MY QUESTIONS FOR HIM\n  \"What does the evaluation infrastructure look like today - are you building from scratch or improving something that exists?\"\n  \"What's the biggest quality challenge the Virtual Agent team is facing right now?\"\n  \"How does this role work with engineering and data science day-to-day?\"\n",
    "source_id": "2e6a97f0-fe30-8018-9618-f99c46120c7b",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Old"
    ],
    "title": "Old"
  },
  {
    "text": "## PART 10: KEY CONCEPTS TO KNOW COLD\n",
    "source_id": "2e6a97f0-fe30-8018-9618-f99c46120c7b",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Old"
    ],
    "title": "Old"
  },
  {
    "text": "## Retrieval vs Generation\nRetrieval: System searches database and pulls back documents. Problems = wrong docs, right docs ranked too low.\nGeneration: LLM reads retrieved docs and creates response. Problems = hallucination, misinterpretation, ignoring the question.\nWhen diagnosing failures, first figure out which step broke. If retrieval was bad, LLM never had a chance.\n",
    "source_id": "2e6a97f0-fe30-8018-9618-f99c46120c7b",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Old"
    ],
    "title": "Old"
  },
  {
    "text": "## Contact Center Metrics\nContainment rate: Did AI resolve without escalating to human? (Core metric)\nFirst contact resolution (FCR): Did customer have to come back with same issue?\nCSAT: Did customer rate interaction positively?\nAHT (Average handle time): How long did it take?\nDeflection rate: At what point did customers give up and ask for human?\n",
    "source_id": "2e6a97f0-fe30-8018-9618-f99c46120c7b",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Old"
    ],
    "title": "Old"
  },
  {
    "text": "## Offline vs Online Evaluation\nOffline: Test suite before deployment. Controlled, repeatable, catches problems before customers see them. Limitation: doesn't cover everything real users do.\nOnline: Measuring in production with real users. A/B tests, live metrics. Tells you real-world performance. Limitation: can't experiment freely without risking customer experience.\nYou need both. Offline is your gate before shipping. Online is your feedback loop after.\n",
    "source_id": "2e6a97f0-fe30-8018-9618-f99c46120c7b",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Old"
    ],
    "title": "Old"
  },
  {
    "text": "## How to Evaluate Subjective Things (like tone)\nBreak \"tone\" into specific dimensions (empathetic, professional, clear)\nCreate rubric describing good vs bad for each\nCalibrate with humans until scoring is consistent\nConsider using LLMs to scale (validate against human scoring)\nConnect to outcomes - does low empathy correlate with low CSAT?\n",
    "source_id": "2e6a97f0-fe30-8018-9618-f99c46120c7b",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Old"
    ],
    "title": "Old"
  },
  {
    "text": "## Eval notes\n  Technical Understanding: How to Think About Evaluation\n  \"How would you approach building an evaluation framework for conversational AI?\"\n  Start from the customer problem and work backwards.\n  First question you ask yourself: What does \"good\" look like for this product? For a Virtual Agent, \"good\" probably means the customer got their issue resolved, didn't have to escalate, and didn't hate the experience. That's your north star.\n  Second question: How do I measure that? You can't just ask \"was this conversation good?\" - that's too vague. You break it down into components you can actually measure. Did the AI understand the question? Did it retrieve relevant information? Did it generate an appropriate response? Did the customer's behavior indicate success (they stopped asking, they didn't escalate, they rated it well)?\n  Third question: What's my ground truth? For factual answers, you can verify against documentation - \"how do I share my screen\" has a correct answer. For subjective things like tone, you need rubrics - criteria that humans can score consistently.\n  Fourth question: How do I make this repeatable? You're not manually reviewing every conversation. You need automation - a harness that can run thousands of test cases, score them, surface failures, and let you track improvement over time.\n  Fifth question: How does this connect to real customer outcomes? Your eval metrics need to correlate with business metrics. If your eval says quality is 95% but customers are still escalating at the same rate, your eval is measuring the wrong thing.\n  That's the thinking process. Start with what \"good\" means, break it into measurable components, define ground truth, automate it, and validate against real outcomes.\n  \"What metrics would you prioritize for a Virtual Agent?\"\n  Think about what a Virtual Agent is trying to do. Its job is to resolve customer issues without human intervention. So you're measuring: did it work, how well did it work, and what was the experience like.\n  Did it work? That's containment rate - the percentage of conversations where the AI resolved the issue without escalating to a human. This is the core metric. If containment is low, the AI isn't doing its job.\n  How well did it work? That's first contact resolution - did the customer have to come back with the same issue? High containment but low FCR means the AI is giving answers that seem right but don't actually solve the problem. The customer leaves, tries the solution, it doesn't work, they come back.\n  What was the experience like? That's CSAT (customer satisfaction) - did they rate the interaction positively? And AHT (average handle time) - how long did it take? Fast and wrong is bad. Slow and right might also be bad if customers get frustrated waiting.\n  Where does it break? Deflection rate tells you at what point customers give up and ask for a human. If they bail after the first message, maybe the AI didn't understand the question. If they bail after three exchanges, maybe the AI understood but couldn't help.\n  When someone asks you to prioritize, think about what matters most to the business. Containment is probably first - that's the whole point of a Virtual Agent. Then CSAT - containment doesn't matter if customers hate the experience. Then the diagnostic metrics like FCR and deflection that help you understand where to improve.\n  \"How do you evaluate something subjective like tone?\"\n  This is harder than accuracy because there's no single \"right answer.\" But you can still make it systematic.\n  First, define what you're evaluating. \"Tone\" is vague. Break it into specific things you can observe: Was the response empathetic? Was it professional? Was it clear? Was it appropriately confident (not hedging too much, not overconfident)?\n  Second, create a rubric. For each dimension, describe what \"good\" and \"bad\" look like. Empathetic might mean: acknowledges the customer's frustration, uses phrases like \"I understand\" or \"I'm sorry you're experiencing this.\" Not empathetic might mean: jumps straight to the solution without acknowledging the problem, sounds robotic or dismissive.\n  Third, calibrate with humans. Have multiple people score the same conversations using your rubric. If they disagree a lot, your rubric isn't clear enough. Refine until you get consistency.\n  Fourth, consider using LLMs to scale. Once you have a calibrated rubric, you can prompt an LLM to score conversations using the same criteria. It won't be perfect, but it lets you evaluate thousands of conversations instead of dozens. You validate the LLM scoring against human scoring to make sure it's reliable.\n  Fifth, connect to outcomes. Ultimately, you want to know if tone affects customer satisfaction. Do conversations scored as \"low empathy\" correlate with lower CSAT? If not, maybe tone doesn't matter as much as you thought - or your rubric isn't capturing what actually matters to customers.\n  \"What's the difference between offline and online evaluation?\"\n  Think of it like rehearsal vs live performance.\n  Offline evaluation is running tests before you deploy. You have a test set with known scenarios, you run them through the system, you measure performance. This is what you did at Kunik - 1,200 queries with expected answers, run the harness, see what passes. It's controlled, repeatable, and lets you catch problems before customers see them.\n  The limitation: your test set might not cover everything real users do. Customers ask weird questions, misspell things, have context you didn't anticipate. Offline eval tells you the system works on cases you thought of.\n  Online evaluation is measuring in production with real users. A/B tests, live metrics, actual customer behavior. This tells you how the system performs in the real world, not just in controlled tests.\n  The limitation: you can't experiment freely without risking customer experience. And you need enough traffic to get statistical significance.\n  You need both. Offline eval is your gate before shipping - nothing goes to production unless it passes your test suite. Online eval is your feedback loop after shipping - are real customers actually having a better experience?\n  When you make a change, the flow is: run offline eval to make sure you didn't break anything, deploy to a small percentage of users, measure online metrics, if they look good, roll out wider.\n  Process and Collaboration: How to Work With Engineering\n  \"How do you work with engineering on ambiguous problems?\"\n  The key is knowing what you own vs what they own.\n  You own the \"what\" and \"why\" - what problem are we solving, why does it matter, how do we know if we've solved it. You define success criteria, you prioritize which failures matter most, you bring the customer perspective.\n  They own the \"how\" - how do we technically implement this, what architecture, what algorithms, what trade-offs between approaches.\n  The overlap is where collaboration happens. You might say \"we need to improve ranking so recent documents surface first.\" That's a requirement. They might come back with three options: weight by date, use a decay function, or add recency as a retrieval signal. They explain trade-offs. You ask questions: \"Which one is fastest to implement? Which one gives us the most control to tune later?\" You decide together based on priorities.\n  When things are ambiguous - and in AI they often are - you use experiments to resolve it. Instead of debating whether approach A or B is better, you say \"let's test both and see what the eval says.\" Data beats opinions.\n  The worst thing you can do is either: (a) tell engineering exactly how to build it when you don't have the technical depth, or (b) just say \"make it better\" and give them no direction. You need to be specific about the problem and success criteria, but leave room for them to figure out the solution.\n  \"How do you prioritize what to fix?\"\n  Start with customer impact, not frequency.\n  A failure that happens 100 times a day but barely bothers customers is lower priority than a failure that happens 10 times a day but makes customers furious or causes them real harm.\n  Think about severity. For a Virtual Agent: giving the wrong answer to a billing question could cost someone money - high severity. Sounding slightly robotic but giving the right answer - low severity. Not understanding a question and asking for clarification - medium severity, depends on how often it happens.\n  Think about volume. All else equal, fix things that affect more customers first.\n  Think about effort. If two problems have similar impact but one is a quick fix and one is a major architecture change, do the quick fix first. Ship improvements incrementally.\n  Think about dependencies. Some fixes unlock other fixes. If your retrieval is fundamentally broken, there's no point fine-tuning generation prompts - you're polishing garbage. Fix retrieval first.\n  When you bring priorities to engineering, be explicit about your reasoning. Don't just say \"fix these 10 things in this order.\" Say \"I ranked these by customer impact. The top 3 are billing-related failures because they cost customers money. The next 4 are high-volume issues affecting thousands of conversations. The last 3 are edge cases we should track but aren't urgent.\"\n  \"How do you handle disagreements about what 'good' looks like?\"\n  This is one of the hardest parts of evaluation. Two smart people can look at the same AI response and disagree about whether it's good.\n  First, get explicit about what you're disagreeing on. Is it factual accuracy? Completeness? Tone? Different people prioritize different things. Name the dimension.\n  Second, look at examples together. Pull 10 responses you disagree on. Go through each one and articulate why you think it's good or bad. Often you'll find you're applying different criteria, and making the criteria explicit resolves the disagreement.\n  Third, write it down. Create a rubric that documents the criteria. \"A good response must include X, Y, and Z. Partial credit if it has X and Y but not Z. Fail if it's missing X.\" Now you're not debating vibes, you're debating whether the rubric makes sense.\n  Fourth, test against customer outcomes. If you still disagree about the rubric, look at real data. Do responses that score high on your criteria correlate with higher customer satisfaction? If your rubric says something is great but customers hate it, your rubric is wrong.\n  The goal isn't to \"win\" the disagreement. The goal is to build a shared understanding of quality that you can measure consistently.\n  Situational: How to Handle Real-World Problems\n  \"You discover accuracy dropped 5% after a model update. What do you do?\"\n  Don't panic, and don't guess. Diagnose systematically.\n  Step one: Confirm it's real. Is this a measurement issue or a real regression? Did something change in how you're calculating accuracy? Did your test set change? Rule out false alarms.\n  Step two: Isolate the change. What exactly was updated? Was it the retrieval model, the generation model, the prompts, the underlying data? You need to know what variable changed.\n  Step three: Run your eval suite and compare before/after. Don't just look at the overall number - look at the breakdown. Which query types regressed? Maybe accuracy dropped 5% overall but it's because one category went from 90% to 60% while everything else stayed flat. That tells you where to focus.\n  Step four: Diagnose the failures. Look at the specific queries that used to pass and now fail. What changed in the output? Is it a retrieval problem (different documents coming back) or a generation problem (same documents, different response)?\n  Step five: Decide whether to roll back or fix forward. If the regression is severe and affecting customers now, roll back to the previous version while you investigate. If it's minor or you can fix it quickly, push a fix.\n  Step six: Add the failing cases to your test suite so this doesn't happen again. The regression happened because your eval didn't catch it - strengthen your eval.\n  \"A customer complains the AI gave a wrong answer. How do you investigate?\"\n  Treat it like debugging. Trace through the system step by step.\n  Step one: Get the specific conversation. Not a summary, the actual logs - what the customer said, what the AI responded, any intermediate steps.\n  Step two: Reproduce if you can. Take the customer's query and run it through your system now. Do you get the same bad answer? If so, the problem is still there. If you get a correct answer now, something might have changed, or there might be inconsistency in the system.\n  Step three: Trace the pipeline. What documents were retrieved? Were they relevant? If bad documents were retrieved, it's a retrieval problem - figure out why search returned the wrong results. If good documents were retrieved but the answer was still wrong, it's a generation problem - the LLM misinterpreted or hallucinated.\n  Step four: Determine root cause. Why did this happen? Is it a gap in your data? A limitation of the search algorithm? A prompt that doesn't handle this case well? An edge case the model wasn't trained on?\n  Step five: Fix it and verify. Implement a fix, run your eval to make sure you didn't break anything else, confirm the specific failing case now passes.\n  Step six: Add it to your test set. This customer surfaced a case you weren't testing for. Now you are. That's the feedback loop.\n  \"How would you measure if a new feature actually improved the experience?\"\n  You need a controlled experiment, not just before/after comparison.\n  The problem with before/after: lots of things change over time - customer mix, seasonality, other features shipping. If you launch a feature and metrics go up, you can't be sure the feature caused it.\n  The solution: A/B test. Split users into two groups randomly. Group A gets the new feature, Group B doesn't. Compare metrics between groups. If Group A does better, the feature caused it.\n  What metrics to compare depends on what the feature is supposed to do. If you're improving answer accuracy, look at containment rate - does Group A escalate less? Look at CSAT - does Group A rate conversations higher? Look at FCR - does Group A come back with the same issue less often?\n  Watch for statistical significance. If Group A has 80.1% containment and Group B has 80.0%, that's probably noise, not a real improvement. You need enough data to be confident the difference is real.\n  Watch for unintended consequences. Maybe your feature improves accuracy but tanks latency - customers get better answers but have to wait longer. You need to look at multiple metrics, not just the one you're optimizing for.\n  Watch for segment differences. Maybe the feature helps one type of customer but hurts another. Break down results by user segment to see if the improvement is universal or localized.\n  The Meta-Skill: Thinking From First Principles\n  Across all of these situations, the pattern is the same:\n  Start with the customer problem. What are we trying to achieve? What does success look like?\n  Break it down into measurable components. Don't try to measure vague things like \"quality.\" Define specific dimensions you can observe.\n  Use data to resolve ambiguity. When you're not sure which approach is better, test both. When you disagree with someone, look at evidence together.\n  Close the loop. Whatever you learn should feed back into your system - new test cases, refined metrics, updated rubrics.\n  That's what a PM who owns AI quality does. You're not writing code. You're defining what good looks like, building systems to measure it, and using data to continuously improve.\n  Does this help you understand the thinking, not just the answers? Any of these situations you want to go deeper on?\n",
    "source_id": "2e6a97f0-fe30-8018-9618-f99c46120c7b",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Old"
    ],
    "title": "Old"
  },
  {
    "text": "## Follow-ups on your Kunik story\n  \"How did you decide what metrics to use?\"\n  He wants to know if you think from first principles or just copy what others do\n  Your answer: \"Started with what customers complained about. Wrong answers and slow responses. That gave us accuracy and latency. Precision and recall came from standard retrieval system practice - my engineer recommended those and they made sense for measuring search quality.\"\n  \"Who actually built the eval harness?\"\n  He's checking if you did the work or just managed people who did\n  Your answer: \"My engineer wrote the code. I spec'd it - defined the test cases, the scoring criteria, what metadata to capture on failures. I owned the inputs and interpreted the outputs. He owned the implementation.\"\n  \"What was the hardest part?\"\n  He wants to see if you understand why eval is genuinely difficult\n  Your answer: \"Defining what 'correct' means. Is a partial answer acceptable? How recent does data need to be? These are judgment calls. We had to document our criteria and be consistent, otherwise you can't trust the scores.\"\n  \"What would you do differently?\"\n  He's checking for self-awareness and learning\n  Your answer: \"Start with eval earlier. We built the product first and retrofitted evaluation. If I'd set up even basic accuracy tracking from day one, we would've caught problems faster.\"\n  Follow-ups on why Zoom\n  \"What do you know about Zoom Virtual Agent?\"\n  He's checking if you did any homework\n  Your answer: \"It's your AI chatbot for customer support, part of Zoom Contact Center. Handles tier-1 questions so human agents can focus on complex issues. I know evaluation for conversational AI is different from what I did at Kunik - more subjective, multi-turn conversations, tone matters not just accuracy. That's part of what interests me.\"\n  \"Contact center is new to you. How would you get up to speed?\"\n  He wants to know your learning approach\n  Your answer: \"Same way I got up to speed on financial data at Kunik. Talk to customers, look at real conversations that failed, understand what the support team is hearing. The domain is new but the approach isn't.\"\n  Add the ones that feel most likely to your sheet. You don't need scripted answers - just know what he's really asking so you're not caught off guard.\n  i feeel like i might get asked behavioral questions, hes gonna wanna assess lightgly right?\n",
    "source_id": "2e6a97f0-fe30-8018-9618-f99c46120c7b",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "Old"
    ],
    "title": "Old"
  },
  {
    "text": "Kunik pipeline: Query Analysis \u2192 Retrieval \u2192 Relevance Check \u2192 Reranking \u2192 Synthesis\nZoom pipeline: Transcription \u2192 Intent Recognition \u2192 Retrieval \u2192 Response Generation \u2192 Speech Synthesis\n  9. Your 60-second intro\n",
    "source_id": "2d3a97f0-fe30-80e3-a7d9-cc557380e709",
    "path": [
      "Interview Prep",
      "Zoom Prep"
    ],
    "title": "Zoom Prep"
  },
  {
    "text": "## ONE-PAGER (WITH ZOOM CONNECTIONS)\n  Part 10: When You Need an Evaluation System\n  Part 11: Defining What \"Correct\" Means \u2014 The Rubric\n  Part 12: Building the Test Set \u2014 Ground Truth\n  Part 13: Running Evaluation and Measuring\n  Part 14: Diagnosing Failures\n  Part 15: Implementing Fixes\n  Part 16: Validating Improvements\n  Part 17: Data Schema for Evaluation\n  Part 18: Calibration \u2014 LLM as Judge\n  Part 19: Ongoing Monitoring\n",
    "source_id": "2d3a97f0-fe30-80e3-a7d9-cc557380e709",
    "path": [
      "Interview Prep",
      "Zoom Prep"
    ],
    "title": "Zoom Prep"
  },
  {
    "text": "## How do you measure precision and recall at scale with millions of conversations?\"\n  \"You can't manually review millions of conversations. So you use an LLM-as-judge.\n  You define a rubric\u2014what does accurate mean, what does relevant mean, what does good tone mean. Then you prompt the LLM: 'Here's the conversation, here's the rubric, score each dimension 1-3 and explain why.'\n  But you can't just trust the LLM blindly. You calibrate it. You take 100-200 conversations, have humans score them, then run the LLM on the same set. You measure alignment\u2014does the LLM agree with humans 85% or more? If not, you analyze the disagreements, adjust the prompts, and iterate until you hit that threshold.\n  Once calibrated, you run the LLM-as-judge on all conversations at scale. Then you aggregate: what percentage scored 3 on accuracy, what percentage scored 2, what percentage scored 1. You can slice by topic, by channel, by customer.\n  The LLM scores are proxies. You validate them against real outcomes. If conversations that score high on quality also have lower repeat contact and higher CSAT, your evaluation is measuring something real. If not, your rubric is wrong and you go back to square one.\"\n",
    "source_id": "2eaa97f0-fe30-8035-acb8-c93ba08defca",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "more questions"
    ],
    "title": "more questions"
  },
  {
    "text": "## \"How would you build an evaluation system from scratch?\"\n  \"I'd start by defining what quality means for a Virtual Agent. It's not one thing\u2014a conversation can be accurate but have bad tone, or have great tone but not resolve the issue. So I'd identify the dimensions that matter: accuracy, relevance, tone, and resolution.\n  For each dimension, I'd build a rubric. Pull 50-100 sample conversations, sort into piles\u2014clearly good, clearly bad, somewhere in the middle. Examine what makes the good ones good. Write scoring criteria with specific examples. Test with multiple raters until we hit 85% agreement.\n  Then I'd create a test set\u2014maybe 200 conversations across different topics, channels, and outcomes. Have humans score them to create ground truth.\n  Then I'd work with data science to implement the LLM-as-judge. They translate my rubrics into prompts. We run the LLM against the ground truth and calibrate\u2014measure alignment, analyze disagreements, adjust prompts, iterate until we hit 85%.\n  After deployment, I'd set up ongoing monitoring. Weekly sample checks, monthly alignment tracking. And I'd connect evaluation scores to business outcomes\u2014if accuracy scores are high but repeat contact is also high, something's wrong with the evaluation.\"\n",
    "source_id": "2eaa97f0-fe30-8035-acb8-c93ba08defca",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "more questions"
    ],
    "title": "more questions"
  },
  {
    "text": "## \"What metrics would you put on the customer dashboard?\"\n  \"I'd organize in layers based on who's looking.\n  Executive layer: outcome metrics only. Containment rate, CSAT, trends over time, benchmark comparisons. One screen, 30 seconds, red/yellow/green. They want to know: is this working?\n  Operations layer: outcomes plus diagnostics by topic. 'Containment is 68% overall, but billing is 54% while password resets are 91%.' They want to know: where should I focus?\n  Technical layer: all three\u2014outcomes, diagnostics, and system health. Accuracy scores, intent recognition rates, retrieval relevance, transcription accuracy for voice. Drill-down to specific conversations. They want to know: what's technically broken?\n  The dashboard should let users move between layers. An executive sees containment is down, clicks to see it's billing, clicks again to see retrieval relevance dropped for billing. They can drill as deep as they want.\"\n",
    "source_id": "2eaa97f0-fe30-8035-acb8-c93ba08defca",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "more questions"
    ],
    "title": "more questions"
  },
  {
    "text": "## \"How would you measure first contact resolution for unauthenticated voice callers?\"\n  \"FCR means the customer's issue got solved in one interaction without calling back. To measure it, I need to link calls from the same person about the same issue.\n  For unauthenticated voice, I'd use probabilistic identity matching. Phone number is the primary signal\u2014if the same number calls back within 48 hours, likely the same person. I'd supplement with conversation topic matching\u2014if both calls are about 'refund for order #12345,' that strengthens the link.\n  FCR would be: total calls minus repeat contacts, divided by total calls. If we had 1,000 calls and 200 were followed by a repeat contact from the same number about the same topic within 48 hours, FCR is 80%.\n  It's not perfect. People use different phones, caller ID can be blocked. But it gives a directionally accurate measure I can track over time. If FCR trends down, resolution quality is degrading, even if the absolute number isn't perfectly accurate.\"\n",
    "source_id": "2eaa97f0-fe30-8035-acb8-c93ba08defca",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "more questions"
    ],
    "title": "more questions"
  },
  {
    "text": "## \"What's the relationship between traditional contact center metrics and AI evaluation metrics?\"\n  \"Traditional metrics tell you WHAT happened. AI metrics tell you WHY.\n  Traditional metrics like containment, CSAT, and FCR measure business outcomes. They've existed for decades. They tell you: are we saving money? Are customers happy?\n  AI metrics like accuracy, intent recognition, retrieval relevance, transcription accuracy measure the AI system itself. They're diagnostic. When containment drops, AI metrics tell you which pipeline stage is failing.\n  You need both. If I only have traditional metrics, I know something is broken but not what to fix. If I only have AI metrics, I might optimize technical scores that don't connect to customer outcomes.\n  The flow is: traditional metrics identify WHERE the problem is\u2014containment dropped for billing questions. AI metrics diagnose WHY\u2014retrieval relevance dropped for billing topics. Now I know what to fix.\"\n",
    "source_id": "2eaa97f0-fe30-8035-acb8-c93ba08defca",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "more questions"
    ],
    "title": "more questions"
  },
  {
    "text": "## \"Containment is high but customers are complaining. What's going on?\"\n  \"High containment means conversations aren't escalating to humans. But containment doesn't mean the customer was actually helped. They might have hung up frustrated, gotten a wrong answer they didn't realize was wrong, or given up.\n  I'd look at repeat contact rate. If containment is 72% but repeat contact is 35%, a third of customers are calling back. The bot is 'containing' calls by giving answers, but those answers aren't solving problems.\n  I'd dig into which topics have high repeat contact. Maybe claims status inquiries have 45% repeat contact. I'd pull sample conversations. Maybe the bot is giving accurate status\u2014'your claim is in review'\u2014but not giving customers what they actually want: 'When will I hear back? What's the next step?'\n  The bot is technically accurate but not helpful. It's answering the literal question without addressing the underlying need. The fix is updating the bot's responses to include next steps and timelines, not just status.\"\n",
    "source_id": "2eaa97f0-fe30-8035-acb8-c93ba08defca",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "more questions"
    ],
    "title": "more questions"
  },
  {
    "text": "## \"How do you handle it when a customer says your evaluation scores are wrong?\"\n  \"I take it seriously. If customers don't trust the evaluation, they don't trust the product.\n  I ask for the specific conversations they think are scored wrong. I pull them and review myself, comparing the bot's responses to source documents.\n  Usually I find a mix. Some are real evaluation failures\u2014I investigate why. Some the customer misunderstood what we're measuring. Some are genuinely ambiguous.\n  For real failures, I dig into root causes. Maybe retrieval pulled wrong articles and the LLM-as-judge only saw wrong articles, so it scored 'accurate' against bad information. Maybe incomplete answers were scored as accurate because the rubric didn't distinguish completeness.\n  For each root cause, I identify a fix. Add retrieval relevance as a separate check. Update the rubric to distinguish complete from incomplete.\n  Then I communicate back to the customer. Acknowledge their concern was valid, explain what I found, share the fixes. They feel heard and understand the nuance.\"\n",
    "source_id": "2eaa97f0-fe30-8035-acb8-c93ba08defca",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "more questions"
    ],
    "title": "more questions"
  },
  {
    "text": "## \"Voice accuracy is lower than chat. Why?\"\n  \"Voice has two additional pipeline stages: transcription at the start and speech synthesis at the end. That's where I'd focus.\n  I'd sample voice failures and trace through the pipeline: transcription, intent recognition, retrieval, generation, synthesis. For each failure, which stage broke?\n  Maybe I find 31% are transcription errors\u2014the system misheard the customer. I'd dig deeper: what's causing them? Background noise, accents, speaking too fast, technical jargon, audio quality?\n  If accents are the biggest category, I'd propose mitigations. When transcription confidence is low, have the bot ask for clarification. Build custom vocabulary for each customer's product names. Add transcription accuracy as a voice-specific metric on the dashboard so customers can see it.\n  I'd also add voice-specific evaluation dimensions\u2014transcription accuracy, speech clarity, pacing\u2014so customers can diagnose voice-specific issues separately from overall quality.\"\n",
    "source_id": "2eaa97f0-fe30-8035-acb8-c93ba08defca",
    "path": [
      "Interview Prep",
      "Zoom Prep",
      "more questions"
    ],
    "title": "more questions"
  },
  {
    "text": "## Metal models \n  Here are all your mental models in one place. Save this for reference.\n  MENTAL MODEL 1: THE DIAGNOSTIC FUNNEL\n  MENTAL MODEL 2: THE RUBRIC DEVELOPMENT CYCLE\n  MENTAL MODEL 3: THE CALIBRATION PROCESS\n  MENTAL MODEL 4: THE SCHEMA QUESTION\n  MENTAL MODEL 5: THE STAKEHOLDER PYRAMID\n  Use when: Deciding what to show in the dashboard and how to organize it.\n  Executive Layer (Top) \u2192 Operations Layer (Middle) \u2192 Technical Layer (Bottom)\n  Top Layer: Executive View. Who: VP, C-level. What they ask: \"Is this working? Should we invest more?\" What they see: Outcome metrics only. Containment rate, CSAT, trends over time, comparison to benchmarks. What they don't need: Diagnostic details. How often: Monthly or quarterly. Design: One screen, 30-second glance, big numbers, trend arrows, red/yellow/green.\n  Middle Layer: Operations View. Who: Operations managers, team leads. What they ask: \"What's working, what's broken, where should we focus?\" What they see: Outcome metrics PLUS diagnostics broken down by topic, channel, time period. \"Containment is 68% overall, but billing is 54% while password resets are 91%.\" What they don't need: Deep system-level metrics. How often: Weekly, sometimes daily. Design: Summary at top, ability to break down, highlight problem areas.\n  Bottom Layer: Technical View. Who: Product managers, engineering, data science. What they ask: \"What's technically broken and how do we fix it?\" What they see: All three layers. Outcomes, diagnostics, and system health. Transcription accuracy, intent recognition rates, retrieval relevance, latency, confidence distributions, drill-down to specific conversations. How often: Continuously when debugging, weekly for monitoring.\n  How Information Flows: Up the pyramid: Information gets aggregated. Down the pyramid: Information gets detailed. The dashboard should enable movement in both directions.\n  MENTAL MODEL 6: THE QUALITY LOOP\n  Use when: Thinking about how the evaluation system drives continuous improvement.\n  Measure \u2192 Surface \u2192 Diagnose \u2192 Fix \u2192 Measure Again\n  Step 1: Measure. The evaluation system scores every conversation on multiple dimensions. These scores are the raw measurement of quality. This happens automatically, at scale, continuously.\n  Step 2: Surface. Raw scores aren't useful until they're surfaced in a way people can act on. The dashboard aggregates scores into metrics, highlights problem areas, shows trends. Surfacing means making the data visible, understandable, and actionable.\n  Step 3: Diagnose. Once a problem is surfaced, someone investigates. Using the Diagnostic Funnel, they segment the data, form hypotheses, and validate with specific examples. Diagnosis ends with a root cause.\n  Step 4: Fix. Once you know the root cause, you implement a fix. Engineering fixes a bug. A customer updates their knowledge base. You adjust retrieval configuration. You update the rubric. The fix is specific and has an owner.\n  Step 5: Measure Again. After the fix is deployed, you measure again. Did accuracy go back up? Did the specific failure pattern stop? If yes, move to the next problem. If no, go back to Step 3.\n  Why This Is a Loop: It never ends because the product changes, customer behavior changes, the world changes, new problems emerge as you fix old ones. Quality is not a destination. It's a continuous process.\n  MENTAL MODEL 7: PRIORITIZATION\n  Use when: You have multiple things competing for attention and need to decide what to work on first.\n  Filter by Urgency \u2192 Score Impact \u2192 Score Effort \u2192 Use the Matrix \u2192 Communicate\n  Step 1: Filter by Urgency. Some things can't wait. Major customer threatening to churn. Critical bug causing data loss. CEO needs something for board meeting tomorrow. Urgent items get handled first. But ask: what happens if this waits one week? If nothing catastrophic, it's not truly urgent.\n  Step 2: Score Impact. For non-urgent items, estimate impact. How many customers does this affect? How much does it affect them? Does it affect revenue, retention, or strategic goals? Does it unblock other important work? High/Medium/Low.\n  Step 3: Score Effort. Estimate how much work. Multiple sprints and cross-team coordination? One sprint, straightforward? A few days? High/Medium/Low. Talk to engineering and data science if unsure.\n  Step 4: Use the Matrix. High impact + Low effort: Do first (quick wins). High impact + High effort: Do second, plan carefully (big bets). Low impact + Low effort: Do if spare capacity (filler). Low impact + High effort: Don't do (traps).\n  Step 5: Communicate. Once prioritized, communicate the decision and reasoning. Stakeholders who asked for deprioritized items need to understand why. \"We're prioritizing X over Y because X affects 80% of customers while Y affects 10%, and X is less effort.\"\n  The Key Insight: Prioritization isn't just about picking what's most important. It's about making tradeoffs explicit and defensible.\n  MENTAL MODEL 8: TRADEOFFS\n  Use when: You face a decision where optimizing for one thing means sacrificing another.\n  Name Both Sides \u2192 Who Bears the Cost \u2192 What's Reversible \u2192 What Will You Learn \u2192 Decide and Own It\n  Step 1: Name Both Sides. \"We could launch now with 80% accuracy or wait two weeks for 90%.\" Option A (launch now): Gain speed, start learning. Lose quality, risk trust. Option B (wait): Gain quality, reduce risk. Lose time, delay learning.\n  Step 2: Ask Who Bears the Cost. If you launch at 80%, customers bear the cost. If you wait, the business bears the cost. This matters for how you decide.\n  Step 3: Ask What's Reversible. If you launch and it's bad, can you quickly improve? Or will customers churn and never come back? Irreversible decisions deserve more caution. Reversible decisions can be made faster.\n  Step 4: Ask What You'll Learn. Sometimes one option gives you information the other doesn't. Launching lets you see real usage. Waiting means you might optimize the wrong things. If learning is valuable and downside is recoverable, bias toward action.\n  Step 5: Make the Call and Own It. Don't waffle. State the decision and reasoning. \"We're launching at 80% because the deadline is real, downside is recoverable, and we'll learn more from real usage. We commit to 90% within four weeks.\"\n  The Key Insight: Tradeoffs aren't about finding the \"right\" answer. Your job is to make the tradeoff explicit, consider context, and make a defensible decision.\n  MENTAL MODEL 9: WORKING WITH ENGINEERING AND DATA SCIENCE\n  Use when: You need to partner with technical teams to get something built.\n  Understand Ownership \u2192 Bring Problems Not Solutions \u2192 Clear Handoffs \u2192 Stay Engaged \u2192 Close the Loop\n  Step 1: Understand Who Owns What. You (PM): What to build and why. Problem definition, requirements, success criteria, prioritization. Engineering: How to build it. Architecture, code, infrastructure, technical decisions. Data science: How to make the AI work. Model selection, prompt engineering, calibration, statistical analysis.\n  Step 2: Bring Problems, Not Solutions. Don't tell them HOW to do their job. Tell them WHAT problem you're solving. Bad: \"Build me a database table with these 15 fields.\" Good: \"I need to answer these questions. What data would we need, and what's the best way to structure it?\"\n  Step 3: Define Clear Handoffs. Be clear about what you're handing off and what you need back. \"Here's the rubric with scoring criteria and examples. Here are 100 conversations with human scores. I need you to implement the LLM-as-judge and get to 85% alignment. Let me know if anything is unclear.\"\n  Step 4: Stay Engaged Without Micromanaging. After handoff, don't disappear. Check in regularly. Ask if they're blocked. Be available for decisions. But don't hover or second-guess their technical decisions.\n  Step 5: Close the Loop. When delivered, review it. Does it meet requirements? If not, give clear feedback. Not \"this isn't good enough.\" Instead: \"Alignment is 70%, we need 85%. The LLM seems too generous on resolution. Can we add more examples of partial resolution?\"\n  The Key Insight: Partnership works when everyone understands their role, handoffs are clear, and communication is specific.\n  MENTAL MODEL 10: HANDLING AMBIGUITY\n  Use when: You don't have enough information to make a confident decision.\n  Name It \u2192 Reduce If Possible \u2192 Cost of Being Wrong \u2192 Make a Bet \u2192 Create Feedback Loops\n  Step 1: Name What You Don't Know. Be specific. Not \"I don't know what to do.\" Instead: \"I don't know whether customers want feature A or feature B.\" Or: \"I don't know if 80% accuracy is good enough to launch.\"\n  Step 2: Ask If You Can Reduce Ambiguity Quickly. Talk to customers? Look at data? Run a quick test? Ask an expert? If you can reduce ambiguity quickly and cheaply, do it. But don't let research become an excuse to avoid deciding.\n  Step 3: Ask What Happens If You're Wrong. If cost is low and reversible: Decide quickly, learn, adjust. If cost is high and irreversible: Invest more time reducing ambiguity.\n  Step 4: Make a Bet and State Your Assumptions. \"I'm prioritizing feature A over B. My assumption is customers care more about accuracy than speed. If we learn that's wrong, we'll pivot to B next quarter.\" This forces clarity and tells others what would change your mind.\n  Step 5: Create Feedback Loops. What metric would tell you if you were right? When will you check? What would make you reconsider?\n  The Key Insight: Most PM decisions are made with incomplete information. The skill isn't avoiding ambiguity. It's making reasonable decisions despite ambiguity, and learning quickly whether you were right.\n  MENTAL MODEL 11: EXPLAINING TECHNICAL CONCEPTS\n  Use when: You need to help a non-technical person understand something complex.\n  Start with Why They Care \u2192 Use Analogies \u2192 Build Simple to Complex \u2192 Connect Back\n  Step 1: Start with the Problem, Not the Solution. Don't start with \"LLM-as-judge is a system that uses large language models...\" Start with: \"Customers want to know if their Virtual Agent is doing a good job. We handle millions of conversations. We can't have humans review every one. So we need an automated way to evaluate quality at scale.\" Now the solution makes sense.\n  Step 2: Use Analogies. Connect to something familiar. \"The LLM-as-judge is like having a really smart intern trained on our quality rubric. We show them a conversation and ask 'how would you rate this?' Except instead of one intern reviewing a few conversations, we have a system that can review millions instantly.\"\n  Step 3: Build from Simple to Complex. Don't explain everything at once. Layer 1: \"We have an AI that reads conversations and scores them on quality.\" Layer 2 (if they want more): \"It scores on multiple dimensions: accuracy, tone, resolution.\" Layer 3 (if they want more): \"We calibrate it against human judgment.\" Stop when they have what they need.\n  Step 4: Connect Back to What They Care About. For an executive: \"This means we can give customers real-time visibility into quality without scaling our QA team.\" For a customer: \"This means you can see quality scores for every conversation, not just a sample.\"\n  The Key Insight: Technical explanations fail when they're too detailed too fast, or when they don't connect to what the listener cares about.\n  MENTAL MODEL 12: BEHAVIORAL STORIES (STAR FRAMEWORK)\n  Use when: You're asked \"tell me about a time\" questions.\n  Situation \u2192 Task \u2192 Action \u2192 Result\n  Situation: Set context briefly. Where were you? What was the project? What was the challenge? Two to three sentences.\n  Task: What was YOUR specific responsibility? Not the team. What were you accountable for? One to two sentences.\n  Action: What did YOU do? This is the meat. Be specific about steps you took, decisions you made, how you worked with others. 60% of your answer time.\n  Result: What happened? Quantify if possible. What did you learn? One to three sentences.\n  Structure: Total time 90 seconds to 2 minutes. Situation + Task: 20%. Action: 60%. Result: 20%.\n  Common Mistakes: Too much situation (get to the action). Saying \"we\" instead of \"I\" (be specific about your contribution). No result (always land with an outcome). Vague action (give specifics, not generalities).\n  The Key Insight: Behavioral stories need to be prepared in advance. You need to know which experiences map to which question types, and you need to have practiced telling them.\n  END OF MENTAL MODELS\n  Save this. Reference it when preparing. Ready to move to item 6 (making evaluation explainable) or skip to Kunik scenarios?\n",
    "source_id": "2d3a97f0-fe30-80e3-a7d9-cc557380e709",
    "path": [
      "Interview Prep",
      "Zoom Prep"
    ],
    "title": "Zoom Prep"
  },
  {
    "text": "## My questions and notes\n  What a rubric actually is:\n  A customer's containment rate dropped 10% this month. Walk me through how you'd use traditional metrics and AI metrics together to diagnose and fix the problem.\n  Ram asks you: \"We have a voice Virtual Agent with no authentication. How would you measure first contact resolution?\"\n  The Right Way: Acknowledge and Pivot\n  The Job Scope\n  The Complete Picture of What You Touch\n  For the Virtual Agent, the \"subjects\" are:\n   1.  Zoom scenarios (internalized for hypotheticals)\n  2. Terminology definitions \n  3. Traditional vs AI metrics relationship\n  4. How to organize metrics for different stakeholders\n  5. How to track unregistered customers on voice / identity resolution\n  6. Data schema thinking (what data to capture, how to structure it)\n  7. Making evaluation explainable to customers\n",
    "source_id": "2d3a97f0-fe30-80e3-a7d9-cc557380e709",
    "path": [
      "Interview Prep",
      "Zoom Prep"
    ],
    "title": "Zoom Prep"
  },
  {
    "text": "## 8. Kunik scenarios (for behavioral questions)\n  LAYER 1: Foundational Concepts\n  LAYER 2: Building the Kunik System\n  LAYER 2 Cont: \ud83d\udea8 STARTING HERE RELEVANT TO ZOOM \ud83d\udea8\n  LAYER 3: The Publisher Dashboard (With Real Unit Economics)\n",
    "source_id": "2d3a97f0-fe30-80e3-a7d9-cc557380e709",
    "path": [
      "Interview Prep",
      "Zoom Prep"
    ],
    "title": "Zoom Prep"
  },
  {
    "text": "## 10. Questions to ask Dhivya\n  1. \"What does success look like for the evaluation system in the next 6-12 months?\"\n  This shows you're thinking about impact, not just tasks. Her answer tells you what she's prioritizing\u2014is it improving LLM-as-judge accuracy? Launching new dimensions? Scaling to more customers? It also gives you a chance to connect your experience to what she needs.\n  2. \"What are the biggest challenges the team is facing right now with evaluation quality or calibration?\"\n  This is a genuine question that shows you understand the work is hard. Her answer tells you where the pain is\u2014maybe alignment is drifting, maybe customers don't trust the scores, maybe certain dimensions are harder to calibrate. You can briefly connect to similar challenges you faced at Kunik if relevant.\n  3. \"How does the evaluation team work with the rest of product and engineering? What does that collaboration look like day-to-day?\"\n  This shows you care about how the work actually gets done, not just what the work is. Since she's data science and you'd be PM, understanding the collaboration model matters. It also signals you know how to work with technical teams, which is what she's evaluating.\n",
    "source_id": "2d3a97f0-fe30-80e3-a7d9-cc557380e709",
    "path": [
      "Interview Prep",
      "Zoom Prep"
    ],
    "title": "Zoom Prep"
  },
  {
    "text": "## 11. Mock interview practice\n  Tell me about a time you had to define what \"quality\" means for a system.\n  Tell me about a time you worked with engineering or data science to fix a technical problem.\n  Question 1: How would you build a rubric for a subjective dimension like tone?\n  \"Tone is tricky because it's subjective. If I just score 'tone: good or bad,' two people will disagree because they have different standards. So the way I'd approach it is to break tone into specific, observable sub-dimensions.\n  When I look at a conversation with bad tone, I ask: what specifically is wrong? Is the bot ignoring that the customer is upset? Is it too cheerful for a serious problem? Is it using jargon the customer doesn't understand? Each of those is a different failure mode.\n  So I'd break tone into four sub-dimensions: Empathy\u2014did the bot acknowledge the customer's emotional state when appropriate? Professionalism\u2014was the language appropriate for a business context? Tone match\u2014did the tone fit the situation? Clarity\u2014was the response easy to understand?\n  For each sub-dimension, I'd go through the same rubric process. Pull 50-100 conversations, sort into piles, examine what makes the good ones good, write scoring criteria with specific examples. Test with multiple raters, refine where we disagree until we hit 85% agreement. Then hand to data science to implement the LLM-as-judge and calibrate until LLM matches humans.\n  The key is you're not scoring something vague like 'tone.' You're scoring specific, observable behaviors that raters can agree on.\"\n  Question 2: How would you calibrate an LLM-as-judge?\n  \"Calibration is making sure the LLM-as-judge scores conversations the same way humans would. The target is 85% or higher alignment.\n  The process: First, I create a ground truth set. I take 100-200 conversations, have three humans score them independently using the rubric, and where two or more agree, that's the consensus score.\n  Then I run the LLM-as-judge against the same conversations. I compare LLM scores to human scores and measure alignment\u2014what percentage match?\n  If alignment is below 85%, I analyze the disagreements. Is the LLM too generous? Too harsh? Confused by specific patterns? I categorize the disagreements.\n  Then I work with data science to adjust the prompts. If the LLM is too generous on empathy, we add examples of what partial empathy looks like\u2014not just good and bad, but the middle. If the LLM misses multi-turn context, we add instructions to evaluate the full conversation.\n  Re-run, re-measure. It usually takes three to five rounds. At Kunik, we went from 70% to 78% to 86% over three iterations.\n  After deployment, calibration isn't finished. Weekly, I'd pull 20 random conversations, have a human score them, compare to LLM. Monthly, calculate alignment on a larger sample. If alignment drops below 80%, investigate and recalibrate.\"\n  Question 3: What do you do when the LLM-as-judge and humans disagree?\n  \"First, I don't assume either is right. I investigate the specific disagreements.\n  I pull every conversation where they disagreed and look for patterns. Is the LLM consistently too generous on a specific dimension? Is it confused by a specific type of conversation? Is it interpreting a criterion differently than intended?\n  Sometimes the LLM is wrong\u2014the prompt isn't clear enough or doesn't have the right examples. I work with data science to adjust the prompt and re-run.\n  Sometimes the human is wrong\u2014they weren't following the rubric correctly, or the rubric itself is ambiguous. I refine the rubric language and retrain the human raters.\n  Sometimes it's genuinely ambiguous\u2014reasonable people could disagree. For those cases, I make a judgment call on how we want to handle it, document that decision, and add it to the rubric as guidance.\n  The key is categorizing the disagreements. Random disagreements are fine\u2014that's noise. Systematic disagreements mean something is broken and needs fixing.\"\n  Question 4: Accuracy dropped 5% across multiple customers. How would you investigate?\n  \"If it's across multiple customers, that suggests a systemic issue\u2014something in Zoom's platform, not one customer's configuration.\n  First, I segment to find WHERE the problem is. Is it all channels or just voice? Is it all topics or specific topics? Is it all time periods or did it start on a specific date? I'm looking for the pattern that isolates the problem.\n  Let's say I find it's voice only, and it started Tuesday. Now I have a hypothesis: something changed in the voice pipeline on Tuesday.\n  Next, I trace through the voice pipeline to find WHY. The stages are transcription, intent recognition, retrieval, response generation, speech synthesis. I sample failed conversations and trace each one. Did transcription mishear the customer? Did intent recognition misunderstand? Did retrieval pull the wrong articles?\n  Let's say I find transcription errors spiked\u2014certain words are being misheard. I check with engineering: did anything change Tuesday? Maybe they deployed a transcription update to improve speed, but it degraded accuracy for certain accents.\n  Once I've validated the root cause, I identify the fix and owner. This is an engineering fix\u2014roll back the update or tune it. I document the impact, push for prioritization, and add this failure mode to our monitoring so we catch it faster next time.\"\n  Question 5: A customer says your evaluation scores are wrong. How do you handle that?\n  \"First, I take it seriously. If customers don't trust the evaluation system, they don't trust the product.\n  I ask them to share the specific conversations they think are scored wrong. I pull those conversations and review them myself, comparing the bot's responses to the source documents.\n  Usually I find a mix. Some conversations the customer is right\u2014there are real evaluation failures. Some the customer misunderstood what we're measuring. Some are genuinely ambiguous.\n  For the real failures, I dig into why. At Kunik, when a customer said our accuracy scores were wrong, I found three root causes: retrieval failures where the wrong articles were pulled and the LLM-as-judge only saw the wrong articles, incomplete answers being scored as fully accurate because the rubric didn't distinguish completeness, and stale knowledge base content where the bot was technically accurate against the source but the source was outdated.\n  For each root cause, I identify a fix. Add retrieval relevance as a separate check. Update the rubric to distinguish complete from incomplete. Add guidance that accuracy is measured against the knowledge base, not the real world.\n  Then I communicate back to the customer. I acknowledge their concern was valid, explain what I found, share the fixes I'm implementing. They feel heard, they understand the nuance, and they're willing to give the system another chance.\"\n  Question 6: How would you measure resolution quality for a Virtual Agent?\n  \"Resolution is tricky because it's not just 'did the bot give a correct answer'\u2014it's 'did the customer's problem actually get solved.'\n  The challenge is you can't always tell from the transcript. The bot might say 'I've submitted your refund request' but you don't know if the refund actually happened.\n  So I'd reframe the dimension. I'm not measuring whether the real-world problem was solved\u2014I can't know that from the transcript. I'm measuring whether the bot did everything within its power to address the customer's need.\n  I'd define three outcomes: Fully resolved\u2014the bot addressed the customer's stated need completely. The customer would have no reason to call back. Partially resolved\u2014the bot addressed some but not all of the need, or started a process but didn't confirm completion. The customer might need to follow up. Not resolved\u2014the bot failed to address the need entirely.\n  I'd also add 'appropriate escalation' as a fourth category. If the issue genuinely required human judgment and the bot transferred to an agent, that's not a failure\u2014that's correct behavior.\n  Then I'd validate with a proxy metric: repeat contact rate. If resolution scores are high but customers are calling back at a high rate, the evaluation is wrong. High resolution plus low repeat contact means we're measuring the right thing.\"\n  Question 7: What's the relationship between traditional contact center metrics and AI evaluation metrics?\n  \"Traditional metrics tell you WHAT happened. AI metrics tell you WHY.\n  Traditional metrics like containment rate, CSAT, and first contact resolution measure business outcomes. Did customers get helped? Did they escalate? Were they satisfied? These have existed for decades because contact centers have always needed to measure operational performance.\n  AI metrics like accuracy, intent recognition, retrieval relevance, and transcription accuracy measure the quality of the AI system itself. They're diagnostic. When containment drops, AI metrics tell you which stage of the pipeline is failing.\n  You need both. If I only have traditional metrics, I know containment dropped 10% but I don't know what to fix. Is it a transcription problem? A retrieval problem? A generation problem? I'm guessing.\n  If I only have AI metrics, I might be optimizing technical scores that don't actually connect to customer outcomes. Accuracy could be 95% but if the bot isn't resolving issues, customers are still unhappy.\n  The flow is: traditional metrics identify WHERE the problem is, AI metrics diagnose WHY. Containment dropped for billing questions\u2014that's the WHERE. Intent recognition for billing is fine, but retrieval relevance dropped\u2014wrong knowledge articles are being pulled. That's the WHY. Now I know what to fix.\"\n  Question 8: How would you organize metrics for different stakeholders?\n  \"Different stakeholders care about different things, so I'd organize in layers.\n  Executive layer\u2014VPs and C-level. They ask 'Is this working? Should we invest more?' They see outcome metrics only: containment rate, CSAT, trends over time, benchmark comparisons. One screen, 30-second glance, big numbers with red/yellow/green indicators. They don't need diagnostic details.\n  Operations layer\u2014contact center managers and team leads. They ask 'What's working, what's broken, where should we focus?' They see outcomes plus diagnostics broken down by topic, channel, time period. 'Containment is 68% overall, but billing is 54% while password resets are 91%.' That tells them where to focus. They don't need deep system metrics.\n  Technical layer\u2014product managers, engineering, data science. They ask 'What's technically broken and how do we fix it?' They see all three: outcomes, diagnostics, and system health. Transcription accuracy, intent recognition rates, retrieval relevance, confidence distributions, drill-down to specific conversations.\n  The dashboard should let users move between layers. An executive sees containment is down, clicks to see it's billing, clicks again to see retrieval relevance dropped for billing topics. They can drill as deep as they want, but they start at the level appropriate for them.\"\n  Question 9: Voice accuracy is consistently lower than chat. How would you diagnose that?\n  \"Voice has two additional pipeline stages that chat doesn't have: transcription at the start and speech synthesis at the end. That's where I'd focus.\n  I'd pull the data. Let's say voice accuracy is 76% and chat accuracy is 89%. That's a 13-point gap. I'd segment voice failures by failure type.\n  I trace through the voice pipeline for a sample of failures: transcription, intent recognition, retrieval, response generation, speech synthesis. For each failure, which stage broke?\n  Let's say I find: 31% of voice failures are transcription errors\u2014the system misheard the customer. 28% are intent recognition\u2014heard correctly but misunderstood. 22% are response generation. 19% are speech synthesis\u2014the response was correct but sounded wrong.\n  Now I dig into transcription since that's my biggest category. What's causing the errors? I sample 100 transcription failures and categorize: background noise 23%, customer accent 34%, speaking too fast 18%, technical jargon 15%, audio quality 10%.\n  Accent is the biggest. I'd propose mitigations: when transcription confidence is low, have the bot ask for clarification rather than guessing. Build custom vocabulary for each customer's product names and terms. Add voice-specific evaluation dimensions to the dashboard so customers can see transcription accuracy separately.\n  The diagnosis tells me where to focus engineering effort and what voice-specific improvements would have the biggest impact.\"\n  Question 10: How would you know if your evaluation system is working correctly?\n  \"Three ways: alignment monitoring, outcome correlation, and customer feedback.\n  First, alignment monitoring. Weekly, I pull 20 random conversations, have a human score them, compare to LLM-as-judge scores. Is alignment holding at 85%+? If it drops, something changed\u2014maybe conversation patterns shifted, maybe the rubric needs updating. Monthly, I do this on a larger sample to track trends.\n  Second, outcome correlation. If the evaluation system is working, scores should correlate with real outcomes. High accuracy scores should correlate with lower repeat contact rates. High resolution scores should correlate with higher CSAT. If accuracy scores are high but customers keep calling back, the evaluation is measuring the wrong thing.\n  Third, customer feedback. Are customers trusting the scores? Are they using the dashboard to make decisions? Or are they saying 'your scores don't match what we see'? Customer complaints are a signal that something might be off, even if my metrics look fine. When a customer pushes back, I investigate\u2014sometimes they're wrong, sometimes they reveal a blind spot in the evaluation.\n  If all three are healthy\u2014alignment holding, outcomes correlating, customers trusting\u2014the evaluation system is working. If any of the three breaks down, I investigate.\"\n  That's 10 high-probability technical questions with polished answers. These are the kinds of questions Ram and Daniel asked, so Dhivya will likely go in similar directions.\n  Want to do rapid-fire practice where I ask and you answer in your own words?\n  Behavioral Question 1: \"Tell me about a time you had to define quality metrics or build an evaluation system.\"\n  \nSituation:\n\"At Kunik, we built a RAG pipeline for financial data\u2014developers would ask questions in plain English and get structured JSON back. We had a working system, but I was eyeballing results. Some looked good, some looked off, and I couldn't answer basic questions: What's our accuracy? Which query types fail most? Without measurement, I was guessing.\"\nTask:\n\"I needed to define what 'accurate' actually meant so we could measure systematically. Without that definition, quality was subjective\u2014I'd say something looked fine, my engineer would disagree, and we had no way to resolve it.\"\nAction:\n\"I built a rubric from scratch. I pulled 50-100 query results and just looked at them without scoring. Then I sorted them into piles\u2014clearly good, clearly bad, somewhere in the middle\u2014using my gut.\nI examined the piles. What made the good ones good? All the right companies, correct data, matched the query criteria. What made the bad ones bad? Wrong companies, wrong values, missing results. I wrote scoring criteria based on those patterns. Score 3 means all records match criteria with correct data. Score 2 means minor issues. Score 1 means significant errors.\nI tested the criteria on 20 new results to see if they held up. Then I had Dharani, my PM support, score 50 results independently using the rubric. We compared scores. Where we disagreed\u2014about 29% of the time\u2014we discussed why. Usually the rubric wasn't specific enough for that situation. I refined the language, added edge case guidance, and we re-scored until we hit 86% agreement.\"\nResult:\n\"That rubric let us measure systematically for the first time. Our baseline was 78% accuracy\u2014which told us we had real work to do. But more importantly, we could now see the problem and diagnose where failures were happening. That rubric was the foundation for everything that came after.\"\n\n  Behavioral Question 2: \"Tell me about a time you worked with engineering or data science to solve a problem.\n  \nSituation:\n\"At Kunik, after building the evaluation system, we measured 78% accuracy. I diagnosed the failures and found 70% happened before the AI even generated anything\u201446% were retrieval problems like keyword mismatches, 24% were chunking problems where context was split across chunks. I needed engineering to fix these, but I had to give them clear direction.\"\nTask:\n\"My job was to define what was broken and what success looked like. Their job was to figure out the technical solution. I couldn't just say 'fix retrieval'\u2014that's too vague to act on.\"\nAction:\n\"I wrote requirements for my engineer Sajal. For each fix, I described the problem specifically: '36% of failures are keyword mismatch. The system requires exact matches but the same concept can be expressed different ways\u2014APAC versus Asia-Pacific, healthtech versus medical technology.'\nI gave concrete examples: 'This query for APAC companies should have returned Company C, but didn't because the chunk said Asia-Pacific instead of APAC.' I quantified the impact: 'This affects roughly 95 queries, 36% of all failures.' I defined success criteria: 'After the fix, queries for APAC should find documents mentioning Asia-Pacific, Asia Pacific, APAC region.'\nSajal implemented hybrid search\u2014combining keyword and semantic search. I validated by re-running the queries I had diagnosed. Company C was now found. But Company D was still missing.\nI dug in and found the semantic search had found the right chunk, but it was ranking too low. I brought that back to Sajal: 'Company D is still missing. Semantic search found it but the ranking is off.' He adjusted the weighting. I tested again. Now Company G was incorrectly appearing\u2014semantic search was too broad.\nWe went back and forth like this\u2014test, find gaps, adjust, test again\u2014until we found the right balance.\"\nResult:\n\"After all four fixes\u2014hybrid search, chunking optimization, reranking, and routing\u2014we went from 78% to 96% accuracy. The key learning was that iteration is normal. Fixes rarely work perfectly the first time. My job was to validate and bring clear feedback. His job was to figure out the technical solution. That division of responsibility made the collaboration work.\"\n\n  Behavioral Question 3: \"Tell me about a time you had to diagnose why something wasn't working.\"\n\n  Situation:\n\"At Kunik, we measured 78% accuracy on our RAG pipeline. I knew we needed 90%+ for financial customers to trust the data. But 78% is just a number\u2014it doesn't tell you what to fix. I had 264 failed queries out of 1,200 and no idea why they were failing.\"\nTask:\n\"I needed to figure out where in the pipeline failures were happening. Without diagnosis, we'd be guessing at fixes\u2014maybe wasting engineering time on the wrong things.\"\nAction:\n\"I sampled 50 failed queries across different query types. For each one, I traced through our five-stage pipeline: Query Analysis, Retrieval, Relevance Check, Reranking, Synthesis.\nFor each failure, I asked: Did Query Analysis understand the query correctly? I'd check\u2014yes, it extracted the right entities and filters. Were the right chunks retrieved? I'd look at what came back\u2014no, Company C should have been found but wasn't. Why? The query said 'APAC' but the chunk said 'Asia-Pacific.' Keyword mismatch.\nI categorized failures as patterns emerged. After 50 queries, I had a clear breakdown: 46% were retrieval problems\u2014keyword mismatches and wrong sources searched. 24% were chunking problems\u2014relevant information existed but was split across chunks so the system couldn't connect company names to valuations. 18% were generation problems\u2014the AI hallucinated or extracted the wrong values from correct chunks.\nI extrapolated to the full failure set: roughly 120 failures from retrieval, 63 from chunking, 48 from generation.\"\nResult:\n\"The insight changed everything. 70% of failures happened before the AI even generated anything. That told me to focus engineering on retrieval and chunking, not on tweaking AI prompts. Without that diagnosis, we might have spent weeks on prompt engineering when the real problem was upstream. That's what enabled us to go from 78% to 96%.\"\nThose are your three behavioral STAR stories for the most likely behavioral questions in a technical interview. They're ready to go.\n",
    "source_id": "2d3a97f0-fe30-80e3-a7d9-cc557380e709",
    "path": [
      "Interview Prep",
      "Zoom Prep"
    ],
    "title": "Zoom Prep"
  },
  {
    "text": "## THE FRAMEWORK FOR ANY ANSWER\n1. Headline first - What's the point? Say it in one sentence.\n2. Context - What do they need to know to understand this?\n3. Specifics - What actually happened? Numbers, examples, details.\n4. So what - Why did it matter? What was the outcome or learning?\n",
    "source_id": "2f6a97f0-fe30-8088-b13c-c9e6361189f3",
    "path": [
      "Interview Prep",
      "Wellfoundary on demand interivew"
    ],
    "title": "Wellfoundary on demand interivew"
  },
  {
    "text": "## WELLFOUND INTERVIEW ONE-PAGER\nINTRO\nI'm a product manager with about 12 years of experience, primarily in AI and data products.\nMost recently I was Founder in Residence at Temasek's venture studio where I built Kunik from the ground up. Before that I co-founded Imbr - we reduced songwriter payment cycles from two years to 30 days. And at Warner Chappell I managed royalty platforms processing about $2B annually across 40 territories.\nThe common thread is building products in complex data environments where measuring quality is difficult and the output has to work for real people. I've done the zero-to-one work twice as a founder. That's where I operate best.\nWHAT ARE YOU LOOKING FOR\nI want to own something end to end. Direct access to customers, a team that makes decisions and ships. Early stage or growth can both work - what matters is the problem is substantive, the team is strong, and there's momentum.\nDEAL BREAKERS\nPlaces where product doesn't have real ownership. Where you're assembling tickets from stakeholder requests instead of solving problems. I recognize that environment quickly - it's not where I do good work.\nEARLY STAGE VS GROWTH\nEarly stage I like being in the room when you're determining what to build. Talking to customers, making bets, learning fast. Growth I like seeing whether something works at scale - real numbers, real patterns, connecting what you built to outcomes.\nWhat I want to avoid is the middle - large enough that everything moves slow, small enough that nothing you ship has real impact.\nACHIEVEMENT\nI improved accuracy from 78% to 96% on a RAG system for financial data. That's what got us to $150K in design partner commitments - developers wouldn't pay until they could trust the data for investment decisions.\nContext: Kunik was a two-sided marketplace connecting AI developers with licensed financial data. Developers query in plain English - \"AI healthtech startups in APAC over $500M\" - and get structured data back. They're making investment decisions on this. Wrong data is worse than no data.\n78% wasn't good enough. The assumption was AI hallucinations - tune the prompts, adjust the model. I wasn't convinced. I pulled 50 failed queries and traced each one through the pipeline stage by stage. 70% of failures were retrieval - the system was pulling wrong documents before the AI ever processed them. Keyword mismatches, chunking that split relevant context.\nOnce I had that diagnosis, I wrote specific requirements - not \"improve retrieval\" but concrete examples of failures with clear success criteria. Worked with my engineer in tight cycles. I'd validate, identify what's still failing, bring it back with specifics.\nI owned diagnosis and measurement. He owned implementation. That's how we reached 96% in months.\nENVIRONMENT\nI need ownership over something real - not executing decisions someone else made. Direct access to customers so I can validate whether what I'm building works. And momentum.\nOn team dynamics - I work closely with engineers. Collaborative and iterative, not handoffs. I bring the problem with specifics, they determine the approach, we iterate. At Kunik my engineer and I went back and forth constantly - I'd test, find what's broken, he'd adjust. That's how we moved fast.\nCOLLABORATION\nI owned diagnosis and measurement. My engineer owned implementation.\nIn practice: I traced failures, identified patterns, documented specific examples with clear success criteria. He built the technical solution. I validated against my test set - what's resolved, what's still failing, why. Back to him with specifics. Two week cycles.\nThat structure worked because there was no ambiguity. He wasn't guessing what \"fix accuracy\" meant. I wasn't waiting months for a release. We knew what was broken, so we fixed that.\nPRECISION VS RECALL\nFor financial data, precision takes priority. Incorrect data leads to bad investment decisions - that's harmful. Incomplete data is frustrating but doesn't mislead.\nSo I prioritized precision first. Tightened relevance filtering - fewer results, but reliable. The tradeoff was recall dropped. We were missing valid companies because \"APAC\" wouldn't match \"Asia-Pacific\" in the documents.\nThen we implemented hybrid search to recover recall without compromising precision. That sequence mattered - establish precision first, then recover recall systematically. That's how we reached 96%.\nBIGGEST HURDLE TO 96%\nDetermining where the problem actually was. The assumption was AI hallucinations - prompts, model issues. I couldn't confirm otherwise until I traced through the failures myself.\nI pulled 50 failed queries, went stage by stage. 70% were retrieval - wrong documents before the AI ever saw them.\nOnce I knew that, the path was clear. The hurdle wasn't fixing it - it was knowing what to fix.\nWHAT I DELIVERED\nThe diagnosis was my work. Traced failures, identified patterns, found the 70% retrieval issue.\nWrote requirements for each fix - specific, with examples and success criteria. Defined the test set. Ran validation after each fix. Identified what's still broken, brought it back.\nMy engineer built the technical solutions to my specs. I validated whether they worked.\nI owned the what and the measurement. He owned the how.\nTIME BREAKDOWN\nDiagnosis was front-loaded - about two weeks tracing failures, categorizing patterns.\nThen iterative. He ships a fix, I validate for a day or two. Identify what's passing, what's broken. Find the pattern, document it, bring it back.\nRoughly 30% diagnosis, 50% validation cycles, 20% requirements. Not clean - some days debugging specific failures, some days documenting patterns, some days running queries.\nThe key was staying close to actual failures. In the data constantly, not managing from a distance.\nTYPICAL DAY / TOOLS / SKILLS\nRight now I'm consulting - market mapping, customer discovery calls, synthesizing for clients.\nWhen I was building Kunik it was different. Mornings usually customer calls, afternoons building or with my engineer. Constant loop - talk to customer, learn something, change the product, validate.\nTwo skills I developed:\nEvaluation systems. Before Kunik I didn't have a framework for measuring whether AI was working. Now I've built rubrics, understand precision and recall tradeoffs, calibration methodology, alignment between human and LLM scoring. At Kunik I had to define what \"accurate\" means for financial data. That's not straightforward.\nI build directly now. Not just specifications and handoffs. I built a contract review tool on the side - Claude for the AI layer, Claude Code for development, Vercel for deployment, GitHub for version control. Concept to working prototype in a weekend. That changes my conversations with engineering - I can demonstrate rather than describe, and I understand constraints because I've encountered them.\nTools: Claude for research, analysis, writing, code. Claude Code and Cursor for building. Vercel and GitHub for deployment. Notion for organization. Figma when I need mockups.\nMAPPING MARKETS TIME SINK\nStructuring. Getting data isn't difficult anymore. The difficult part is making sense of it - what categories matter, what's signal versus noise, how to frame it so someone can decide.\nTOUGH TRADEOFF\nPausing Kunik. We had $150K committed, 96% accuracy, publishers signing LOIs. Going to market with public data and some proprietary partnerships.\nThe core value was paywalled data in one place. Then Anthropic announced MCP - commoditized public finance data that's already structured for RAG. Anyone can plug that in now.\nSo our differentiation depends entirely on proprietary partnerships. Long sales cycles. We had LOIs but needed more locked in. Six months of runway.\nThe choice: continue burning capital hoping partnerships close in time, or pause and preserve what's left.\nI paused. Hardest decision I've made. But exhausting runway while pretending the market hasn't shifted - that's worse.\nSKILLS I DEVELOPED\nEvaluation systems. Before Kunik I didn't have a framework for measuring whether AI was working. Now I've built rubrics, understand precision and recall, calibration, alignment between human and LLM scoring. Defining what \"accurate\" means for financial data isn't obvious. That discipline is central to how I approach AI products now.\nI build directly. Not just specifications. I built a contract review tool - Claude, Claude Code, Vercel, GitHub. Concept to working prototype in a weekend. Changes how I work with engineering - I demonstrate rather than describe, and I understand constraints because I've hit them.\nTOOLS\nClaude for research, analysis, writing, code. Claude Code and Cursor for building. Vercel and GitHub for deployment. Notion for organization. Figma for mockups.\n",
    "source_id": "2f6a97f0-fe30-8088-b13c-c9e6361189f3",
    "path": [
      "Interview Prep",
      "Wellfoundary on demand interivew"
    ],
    "title": "Wellfoundary on demand interivew"
  },
  {
    "text": "##  HYPOTHETICALS \u2014 ZILLOW BRIDGES\n  \"How would you apply what you learned at Kunik to what we're building?\"\n  Core transfer: building AI that takes actions in high-stakes decisions requires earning trust incrementally\n  At Kunik, accuracy was the trust threshold \u2014 users didn't trust it until 96%\n  At Zillow, same principle but different threshold \u2014 users won't let AI schedule tours or draft offers until they've seen it get smaller things right first\n  Also: compliance as foundation, not afterthought. Built data provenance into Kunik from day one. At Zillow, fair housing considerations need the same treatment \u2014 baked into architecture, not bolted on\n  \"Where would you start if you joined this team tomorrow?\"\n  First 30 days: diagnose. Where is the team on the trust escalation ladder with users? Where are the biggest failure modes right now?\n  Understand the agent relationship \u2014 is AI seen as augmenting agents or threatening them? That shapes everything\n  Look for high-frequency, lower-stakes actions as starting point \u2014 want lots of reps to learn from, but room to make mistakes\n  Tour scheduling might be a good early bet. Offer submission would not \u2014 too high stakes before trust is established\n  \"What failure modes would you be most worried about?\"\n  Confidence miscalibration \u2014 AI acts confident when it should be uncertain, user trusts it and gets burned. Most dangerous because users can't catch it\n  Multi-party coordination failures \u2014 AI promises something to buyer that agent didn't confirm. Now you have broken expectations across relationships\n  Timing failures \u2014 AI acts too fast (before user was ready) or too slow (misses opportunity)\n  Framework for prioritizing: frequency \u00d7 severity \u00d7 detectability. Fix the high-frequency, high-severity, low-detectability failures first\n  \"How would you think about trust with users in this domain?\"\n  Trust is earned gradually, can't be assumed\n  Start with suggestions, not actions. \"Here are three properties you might like\" before \"I scheduled tours for these\"\n  Let users verify before acting. \"I drafted this message to the agent \u2014 want me to send it?\"\n  Trust escalation ladder: awareness \u2192 trial \u2192 verification \u2192 delegation \u2192 reliance\n  Design for the anxious user first \u2014 more checkpoints, more transparency. Let power users dial down friction later\n  \"How would you measure success for agentic AI?\"\n  Ultimate outcome is months away (did they buy a home, did AI help)\n  Need leading indicators: task completion rate, user acceptance rate (accept/modify/reject AI suggestions), error rate, recovery rate (when AI makes mistake, does user recover or abandon), trust signals (are users giving AI more autonomy over time)\n  Also qualitative: user feedback, support tickets, session recordings\n  Feedback loops critical \u2014 need infrastructure to connect AI actions to outcomes so the system learns\n  \"How do you think about the agent relationship?\"\n  Two-sided marketplace dynamics \u2014 can't ignore supply side\n  If agents see AI as threatening their value, they'll push back. If AI makes them more effective, they'll embrace it\n  Key question: is AI replacing agent work or augmenting agent work?\n  Frame as augmentation: AI handles tedious coordination, frees agent to focus on relationship and expertise\n  Similar to Kunik \u2014 had to serve both developers and data providers. Can't optimize for one side without considering the other\n",
    "source_id": "2d3a97f0-fe30-80e9-95a2-cc8f7ed6828e",
    "path": [
      "Interview Prep",
      "Zillow interview prep"
    ],
    "title": "Zillow interview prep"
  },
  {
    "text": "## Imbr (Backup for \"Own It\" or \"0\u21921 in Ambiguity\") \n  The Core Story\n  Context:\nSongwriters typically wait 730 days (two years) to get paid royalties. The money flows through a chain \u2014 DSPs report usage, collection societies aggregate, publishers process, matching happens, disputes get resolved, then finally payment. Each step adds delay. This is an industry-wide problem that everyone accepts as \"just how it works.\"\n  The Insight:\nAt Imbr, you and your co-founder (the EVP of Digital from Warner, so he knew the contracts and partner side) realized the bottleneck wasn't the payment infrastructure \u2014 it was the validation process. Everyone waits for usage data to trigger validation of ownership. But ownership changes less frequently than usage. What if you validated ownership claims upfront, before usage data even arrived?\n  What You Built:\nA preclaim system. Rights holders register their claims with supporting documentation. You validate these in advance. When usage data arrives, payments can flow immediately to pre-validated recipients. You reduced payment cycles from 730 days to 30 days.\n  Your Role:\nYour co-founder handled contracts and partners. You built the entire product \u2014 giving engineers guidance, designing the matching tool (similar functionality to what you'd built at Warner), architecting the system.\n  Why This Works for \"Own It\" or \"0\u21921\"\n  For Own It: You took full ownership of the product side of a startup. Not \"contributed to\" \u2014 you built it.\n  For 0\u21921 in Ambiguity: You looked at a broken system everyone accepted and reimagined it from first principles. Instead of optimizing the existing flow, you inverted it entirely.\n  The Natural Way to Tell It (Short Version)\n  When to Use This\n  Use Imbr if Teresa asks:\n  \"Tell me about another time you owned something end-to-end\"\n  \"Tell me about a time you tackled an ambiguous problem\"\n  \"Tell me about a time you challenged conventional thinking\"\n  Don't use Imbr as your primary \"Own It\" story \u2014 Kunik is stronger because it's more recent and more directly relevant to AI product work. But Imbr is a solid backup that shows you've done this pattern multiple times.\n",
    "source_id": "2d3a97f0-fe30-80e9-95a2-cc8f7ed6828e",
    "path": [
      "Interview Prep",
      "Zillow interview prep"
    ],
    "title": "Zillow interview prep"
  },
  {
    "text": "## Fox Programmatic Revenue (Backup for \"Influence Through Data\" or \"Delivering Results\")\n  Story 4: Fox Programmatic Revenue (Backup for \"Influence Through Data\" or \"Delivering Results\")\n  The Core Story\n  Context:\nAt Fox Entertainment, you managed digital properties serving 30 million users. You identified an opportunity in programmatic advertising optimization.\n  What You Did:\nYou used data to identify where revenue was being left on the table in the ad stack. You built the case, aligned the stakeholders (ad ops, engineering, business teams), and drove the optimization.\n  The Outcome:\n$2M in incremental annual revenue within 6 months.\n  The Harvey/TMZ Twist (if they ask about stakeholder conflict):\nAt TMZ, you increased incremental daily revenue by $2-3K. Harvey Levin wanted more engagement features \u2014 comments, polls \u2014 that would push ads down the page. You predicted this would hurt revenue. You suggested compromises, he didn't want them, they implemented his way, revenue dropped as you predicted, then they went back to your approach.\n  The Natural Way to Tell It (Short Version)\n",
    "source_id": "2d3a97f0-fe30-80e9-95a2-cc8f7ed6828e",
    "path": [
      "Interview Prep",
      "Zillow interview prep"
    ],
    "title": "Zillow interview prep"
  },
  {
    "text": "Your intro (keep it under 60 seconds):\n\"I'm a Principal PM with 12 years in product, last 5 focused on AI/ML. Most recently I was Founder in Residence at Temasek's venture studio where I built an AI data marketplace from scratch - RAG pipelines, evaluation frameworks, took accuracy from 78% to 96%. Before that I spent nearly 3 years at Warner Chappell building royalty and media processing platforms handling $2B+ across 40 territories. I've also founded two companies. I saw Coactive and reached out to Satya because the multimodal AI space is exactly where I want to be - I just wanted to make sure we're aligned on level since the posted role looked a bit junior for my background.\"\nTalking points to have ready:\nWhy Coactive:\nMultimodal AI is where the market is going\nUnstructured data (images/video) is a hard problem I find interesting\nSeries B, strong investors, small enough to have impact\nYour relevant experience:\nBuilt AI data pipelines (Kunik)\nML matching systems (Warner Chappell - 85% to 95% precision)\nEnterprise customers in media/entertainment (their target vertical)\n0-to-1 product building twice as founder\nQuestions to ask her:\nWhat level is the team thinking for this role?\nWhat does the PM team look like today?\nWhat's the timeline for the interview process?\nIs there flexibility to work from the SF office some days?\nIf she asks about comp:\n\"For SF-based at this level, I'm targeting $300K total comp.\"\nDon't overcomplicate it. Be direct, confident, conversational. You've done this before.\nCoactive AI in plain terms:\nCompanies have millions of images and videos sitting in storage - product photos, user-generated content, surveillance footage, media libraries. Right now that stuff is basically unsearchable unless someone manually tagged it.\nCoactive lets you search and analyze all that visual content using natural language. \"Show me all images with a red dress near a beach\" - and it just works, no tagging required.\nTheir customers:\nFandom (45M wiki pages of visual content moderation)\nMedia & entertainment companies managing content libraries\nRetail (searching product catalogs)\nTrust & safety teams (finding policy violations in images/video at scale)\nWhat the PM would do:\nWork with enterprise customers to understand their pain points with visual data\nDefine what features to build next (better search? new integrations? faster processing?)\nCollaborate with ML/engineering on how to ship it\nFigure out how to make a deeply technical product feel simple to users\nProbably own a specific surface area - could be the search experience, the API, the integrations, the tagging system\nYour relevant experience:\nKunik: built AI that let people query complex data with natural language\nWarner Chappell: massive catalog management, matching systems\nMedia & entertainment customers are their target vertical\nOn the call, you're feeling out:\nWhat's the actual scope of this PM role?\nIs there flexibility on level and comp?\nDog-friendly office?\nSF vs San Jose flexibility?\n$300K base is your target. Be yourself. You got this.\n",
    "source_id": "2fba97f0-fe30-8042-8cc4-ed924e1395f6",
    "path": [
      "Interview Prep",
      "Coactive AI"
    ],
    "title": "Coactive AI"
  }
]