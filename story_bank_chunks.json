[
  {
    "company": "Kunik",
    "story": "RAG Accuracy (78% \u2192 96%)",
    "beat": "Beat 1: Why accuracy mattered",
    "tags": [
      "accuracy",
      "evaluation",
      "debugging",
      "technical depth",
      "engineering collaboration",
      "prioritization"
    ],
    "text": "Kunik | RAG Accuracy (78% \u2192 96%) | Beat 1: Why accuracy mattered\n\n## Beat 1: Why accuracy mattered\nKunik was selling financial data to AI developers through a pay-per-query API. Financial services clients don't tolerate errors. A wrong answer about a fund's performance or a compliance requirement isn't just a bad UX, it's a liability. Design partners were interested but wouldn't commit until accuracy was proven. Accuracy wasn't a quality metric, it was the sales blocker.\nProbe: Why was this your problem and not engineering's?\nBecause accuracy was a business problem, not just a technical one. Engineering could optimize the pipeline but someone had to define what \"accurate\" meant for each query type, build the measurement system, and connect the results to what customers actually needed. That was product work.",
    "path": [
      "Kunik",
      "RAG Accuracy (78% \u2192 96%)",
      "Beat 1: Why accuracy mattered"
    ]
  },
  {
    "company": "Kunik",
    "story": "RAG Accuracy (78% \u2192 96%)",
    "beat": "Beat 2: Setting up the evaluation framework",
    "tags": [
      "accuracy",
      "evaluation",
      "debugging",
      "technical depth",
      "engineering collaboration",
      "prioritization"
    ],
    "text": "Kunik | RAG Accuracy (78% \u2192 96%) | Beat 2: Setting up the evaluation framework\n\n## Beat 2: Setting up the evaluation framework\nThere was no systematic way to measure accuracy when I started. Queries would fail and nobody knew where in the pipeline the failure happened. I defined what \"correct\" meant for each query type: factual lookups needed exact match, analytical queries needed directional accuracy with correct source attribution. Built a rubric with the engineering team. Set up a labeled test set of queries with known-correct answers so we could measure consistently.\nProbe: How big was the test set?\nStarted with about 200 labeled queries. Enough to see patterns. We expanded it as we found new failure modes.\nProbe: What would you do differently?\nBuild the eval framework on day one, not after we were already at 78% trying to diagnose why. We lost time shipping without systematic measurement.",
    "path": [
      "Kunik",
      "RAG Accuracy (78% \u2192 96%)",
      "Beat 2: Setting up the evaluation framework"
    ]
  },
  {
    "company": "Kunik",
    "story": "RAG Accuracy (78% \u2192 96%)",
    "beat": "Beat 3: Tracing failures through the pipeline",
    "tags": [
      "accuracy",
      "evaluation",
      "debugging",
      "technical depth",
      "engineering collaboration",
      "prioritization"
    ],
    "text": "Kunik | RAG Accuracy (78% \u2192 96%) | Beat 3: Tracing failures through the pipeline\n\n## Beat 3: Tracing failures through the pipeline\nTagged every failed query and traced it back through five stages: document retrieval, chunking/extraction, reranking, context assembly, generation. This was the key decision. Instead of trying to improve the model or prompt engineering, I instrumented the pipeline so we could see exactly where each failure originated. Most teams jump straight to \"make the LLM better.\" We asked \"is the LLM even getting the right inputs?\"\nProbe: How did you actually tag failures? Manually?\nStarted manual. Reviewed failed queries myself with the engineer, tagged each one. After about 200 labeled examples, we had enough pattern recognition to prioritize. Couldn't automate it first because we didn't know what categories we'd find.",
    "path": [
      "Kunik",
      "RAG Accuracy (78% \u2192 96%)",
      "Beat 3: Tracing failures through the pipeline"
    ]
  },
  {
    "company": "Kunik",
    "story": "RAG Accuracy (78% \u2192 96%)",
    "beat": "Beat 4: The diagnosis",
    "tags": [
      "accuracy",
      "evaluation",
      "debugging",
      "technical depth",
      "engineering collaboration",
      "prioritization"
    ],
    "text": "Kunik | RAG Accuracy (78% \u2192 96%) | Beat 4: The diagnosis\n\n## Beat 4: The diagnosis\n58% were retrieval issues (wrong documents pulled entirely). 24% were chunking problems (right document but wrong section extracted). 18% were generation errors (right context but bad synthesis). The insight that changed our whole approach: this wasn't an AI problem. It was a search problem.\nProbe: Why categorize failures that way?\nBecause those are the stages of a RAG pipeline. A query can fail at retrieval, chunking, or generation. Those are the only places it can break. Categorizing by pipeline stage tells you where to invest engineering effort.",
    "path": [
      "Kunik",
      "RAG Accuracy (78% \u2192 96%)",
      "Beat 4: The diagnosis"
    ]
  },
  {
    "company": "Kunik",
    "story": "RAG Accuracy (78% \u2192 96%)",
    "beat": "Beat 5: The fixes",
    "tags": [
      "accuracy",
      "evaluation",
      "debugging",
      "technical depth",
      "engineering collaboration",
      "prioritization"
    ],
    "text": "Kunik | RAG Accuracy (78% \u2192 96%) | Beat 5: The fixes\n\n## Beat 5: The fixes\nPrioritized retrieval first because it was the highest-leverage fix at 58% of failures. Worked with engineering on hybrid search combining BM25 keyword matching with vector similarity. Added query decomposition so complex multi-part questions got broken into sub-queries. For chunking, moved from fixed-size chunks to semantic chunking that respected document structure. For generation, added source attribution so every answer pointed back to the specific passage it came from.",
    "path": [
      "Kunik",
      "RAG Accuracy (78% \u2192 96%)",
      "Beat 5: The fixes"
    ]
  },
  {
    "company": "Kunik",
    "story": "RAG Accuracy (78% \u2192 96%)",
    "beat": "Beat 6: Working with engineering",
    "tags": [
      "accuracy",
      "evaluation",
      "debugging",
      "technical depth",
      "engineering collaboration",
      "prioritization"
    ],
    "text": "Kunik | RAG Accuracy (78% \u2192 96%) | Beat 6: Working with engineering\n\n## Beat 6: Working with engineering\nFive-person team, two engineers. I wasn't writing the code but I was defining what to measure, what \"good\" looked like, and the priority order of fixes. Pushed back when engineering wanted to fine-tune the model before fixing retrieval. That was a real disagreement. They saw it as an AI problem, I saw it as a data problem. The failure analysis proved it.\nProbe: Why not just fine-tune the model?\nThat was engineering's instinct. I pushed back because the failure analysis showed the model was mostly getting bad inputs, not producing bad outputs. Fine-tuning is expensive and slow. Fixing retrieval was faster, cheaper, and addressed 58% of failures in one shot.\nProbe: How did you resolve the disagreement?\nWith data. The failure categorization made it clear. When 58% of problems are \"wrong documents pulled,\" fine-tuning the model that reads those documents doesn't help. The evidence won the argument.",
    "path": [
      "Kunik",
      "RAG Accuracy (78% \u2192 96%)",
      "Beat 6: Working with engineering"
    ]
  },
  {
    "company": "Kunik",
    "story": "RAG Accuracy (78% \u2192 96%)",
    "beat": "Beat 7: The result",
    "tags": [
      "accuracy",
      "evaluation",
      "debugging",
      "technical depth",
      "engineering collaboration",
      "prioritization"
    ],
    "text": "Kunik | RAG Accuracy (78% \u2192 96%) | Beat 7: The result\n\n## Beat 7: The result\n78% to 96% accuracy. 3,000 daily queries at p95 900ms latency. Design partner commitments of $150K came after accuracy crossed 90%. The evaluation framework became a selling point itself. Publishers wanted to see proof that their data was being used accurately.",
    "path": [
      "Kunik",
      "RAG Accuracy (78% \u2192 96%)",
      "Beat 7: The result"
    ]
  },
  {
    "company": "Kunik",
    "story": "Building the Marketplace (0-to-1)",
    "beat": "Beat 1: The starting point",
    "tags": [
      "0-to-1",
      "marketplace",
      "discovery",
      "developer experience",
      "founder",
      "prioritization"
    ],
    "text": "Kunik | Building the Marketplace (0-to-1) | Beat 1: The starting point\n\n## Beat 1: The starting point\nTemasek's venture studio had a thesis: paywalled data was getting locked out of AI. Publishers were sitting on valuable datasets but had no way to monetize them for AI use cases. AI developers needed domain-specific data but hated bulk licensing. No product existed, no target market was defined. I was brought in as Founder in Residence to figure out what to build.",
    "path": [
      "Kunik",
      "Building the Marketplace (0-to-1)",
      "Beat 1: The starting point"
    ]
  },
  {
    "company": "Kunik",
    "story": "Building the Marketplace (0-to-1)",
    "beat": "Beat 2: Discovery",
    "tags": [
      "0-to-1",
      "marketplace",
      "discovery",
      "developer experience",
      "founder",
      "prioritization"
    ],
    "text": "Kunik | Building the Marketplace (0-to-1) | Beat 2: Discovery\n\n## Beat 2: Discovery\nRan 50+ user interviews across both sides of the market. Talked to data publishers at financial institutions, media companies, research firms. Talked to AI developers building RAG applications. Publishers' core fear: they'd been burned before. Given away content to platforms that scraped it, no visibility into usage, no revenue. Developers' core pain: bulk licensing meant paying six figures before knowing if the data was any good. They wanted to test quality before committing.\nProbe: How did you pick financial services as the vertical?\nStarted broader, looked at entertainment, media, and financial data. Financial services had the clearest willingness to pay and the strongest accuracy requirements. Entertainment data was messier and the buyers weren't ready.",
    "path": [
      "Kunik",
      "Building the Marketplace (0-to-1)",
      "Beat 2: Discovery"
    ]
  },
  {
    "company": "Kunik",
    "story": "Building the Marketplace (0-to-1)",
    "beat": "Beat 3: The product decision",
    "tags": [
      "0-to-1",
      "marketplace",
      "discovery",
      "developer experience",
      "founder",
      "prioritization"
    ],
    "text": "Kunik | Building the Marketplace (0-to-1) | Beat 3: The product decision\n\n## Beat 3: The product decision\nDesigned a pay-per-query API. Developers send a natural language query, get back answers sourced from licensed data. Usage-based pricing so they could test with $50 before committing $50K. For publishers: full transparency on how their data was accessed, query-level revenue attribution, automated 70% rev-share payouts. The key insight was that both sides needed trust built into the product. Publishers needed to see exactly how their data was used. Developers needed to verify quality before buying.\nProbe: Why pay-per-query instead of subscription?\nBecause the developer pain point was committing money before knowing if the data was good. Pay-per-query removed that risk. They could evaluate quality with real queries before scaling spend.",
    "path": [
      "Kunik",
      "Building the Marketplace (0-to-1)",
      "Beat 3: The product decision"
    ]
  },
  {
    "company": "Kunik",
    "story": "Building the Marketplace (0-to-1)",
    "beat": "Beat 4: Developer experience",
    "tags": [
      "0-to-1",
      "marketplace",
      "discovery",
      "developer experience",
      "founder",
      "prioritization"
    ],
    "text": "Kunik | Building the Marketplace (0-to-1) | Beat 4: Developer experience\n\n## Beat 4: Developer experience\nOwned the DX end-to-end. API design, authentication, SDK documentation, onboarding flow. Reduced time-to-first-query from hours to minutes. Developer tools live or die on first experience. If a developer can't get a working response in under 10 minutes, they leave. Built the onboarding to get them to a successful query as fast as possible.",
    "path": [
      "Kunik",
      "Building the Marketplace (0-to-1)",
      "Beat 4: Developer experience"
    ]
  },
  {
    "company": "Kunik",
    "story": "Building the Marketplace (0-to-1)",
    "beat": "Beat 5: The publisher side",
    "tags": [
      "0-to-1",
      "marketplace",
      "discovery",
      "developer experience",
      "founder",
      "prioritization"
    ],
    "text": "Kunik | Building the Marketplace (0-to-1) | Beat 5: The publisher side\n\n## Beat 5: The publisher side\nShipped a publisher portal with catalog management, usage analytics, and revenue attribution. Publishers could see which queries hit their data, how often, and exactly what they earned. Automated the 70% rev-share payouts. Onboarded publishers in 30 days vs. prior 6-month approval cycles in traditional data licensing. This was the unlock for supply side. Publishers who'd been burned before could now see everything.",
    "path": [
      "Kunik",
      "Building the Marketplace (0-to-1)",
      "Beat 5: The publisher side"
    ]
  },
  {
    "company": "Kunik",
    "story": "Building the Marketplace (0-to-1)",
    "beat": "Beat 6: Building the team",
    "tags": [
      "0-to-1",
      "marketplace",
      "discovery",
      "developer experience",
      "founder",
      "prioritization"
    ],
    "text": "Kunik | Building the Marketplace (0-to-1) | Beat 6: Building the team\n\n## Beat 6: Building the team\nRecruited and led a 5-person team. Two engineers, one data scientist, one designer, me on product. As a founder, I was doing product, fundraising, user research, and partner sales simultaneously. Had to be ruthless about prioritization because we had no room for wasted cycles.\nProbe: How did you prioritize with such a small team?\nEvery feature had to either close a design partner or unblock a technical blocker. If it didn't do one of those two things, it didn't make the sprint.",
    "path": [
      "Kunik",
      "Building the Marketplace (0-to-1)",
      "Beat 6: Building the team"
    ]
  },
  {
    "company": "Kunik",
    "story": "Building the Marketplace (0-to-1)",
    "beat": "Beat 7: The result",
    "tags": [
      "0-to-1",
      "marketplace",
      "discovery",
      "developer experience",
      "founder",
      "prioritization"
    ],
    "text": "Kunik | Building the Marketplace (0-to-1) | Beat 7: The result\n\n## Beat 7: The result\n$100K revenue, $150K in design partner commitments, 3,000 daily queries at p95 900ms. Seven months from zero to working product with paying customers. $1M term sheet offered from the venture studio.",
    "path": [
      "Kunik",
      "Building the Marketplace (0-to-1)",
      "Beat 7: The result"
    ]
  },
  {
    "company": "Kunik",
    "story": "Killing the Entertainment Vertical",
    "beat": "Full story",
    "tags": [
      "hard decisions",
      "data-driven",
      "pivot",
      "prioritization"
    ],
    "text": "Kunik | Killing the Entertainment Vertical\n\nKUNIK | Killing the Entertainment Vertical\nTags: hard decisions, data-driven, pivot, prioritization\nTO DRAFT: Story about deciding to cut entertainment data vertical and focus on financial services. Data-driven reasoning, stakeholder pushback, what the analysis showed, the result of narrowing focus.",
    "path": [
      "Kunik",
      "Killing the Entertainment Vertical"
    ]
  },
  {
    "company": "Kunik",
    "story": "Pausing the Company",
    "beat": "Full story",
    "tags": [
      "founder decisions",
      "market timing",
      "integrity",
      "leadership"
    ],
    "text": "Kunik | Pausing the Company\n\nKUNIK | Pausing the Company\nTags: founder decisions, market timing, integrity, leadership\nTO DRAFT: $1M term sheet offered, leadership change altered deal terms, market moved faster than enterprise sales allowed. Made the call to pause rather than burn capital. Responsible founder decision, not a failure.",
    "path": [
      "Kunik",
      "Pausing the Company"
    ]
  },
  {
    "company": "Warner",
    "story": "Inheriting the Legacy Platform",
    "beat": "Full story",
    "tags": [
      "legacy systems",
      "modernization",
      "stakeholder management",
      "strategy"
    ],
    "text": "Warner | Inheriting the Legacy Platform\n\nWARNER | Inheriting the Legacy Platform\nTags: legacy systems, modernization, stakeholder management, strategy\nTO DRAFT: Inherited royalty platform held together with Excel and prayers. $2B+ annually, 40 territories. Songwriters couldn't read their own statements. Modernization strategy, prioritization, how she approached a massive legacy system.",
    "path": [
      "Warner",
      "Inheriting the Legacy Platform"
    ]
  },
  {
    "company": "Warner",
    "story": "ML Matching (85% \u2192 95%)",
    "beat": "Full story",
    "tags": [
      "accuracy",
      "ML systems",
      "data science collaboration",
      "scale"
    ],
    "text": "Warner | ML Matching (85% \u2192 95%)\n\nWARNER | ML Matching (85% \u2192 95%)\nTags: accuracy, ML systems, data science collaboration, scale\nTO DRAFT: Royalty matching across 12M song catalog. Working with data science team. How matching works in music (compositions vs recordings vs performances). What improved accuracy from 85% to 95%.",
    "path": [
      "Warner",
      "ML Matching (85% \u2192 95%)"
    ]
  },
  {
    "company": "Warner",
    "story": "Writer Portal Adoption (30% \u2192 75%)",
    "beat": "Full story",
    "tags": [
      "user research",
      "UX",
      "adoption",
      "non-technical users",
      "stakeholder management"
    ],
    "text": "Warner | Writer Portal Adoption (30% \u2192 75%)\n\nWARNER | Writer Portal Adoption (30% \u2192 75%)\nTags: user research, UX, adoption, non-technical users, stakeholder management\nTO DRAFT: Songwriters couldn't understand their own statements. Redesigned portal UX. How she researched the problem, what the solution looked like, driving adoption from 30% to 75% with 1,000+ songwriters.",
    "path": [
      "Warner",
      "Writer Portal Adoption (30% \u2192 75%)"
    ]
  },
  {
    "company": "Warner",
    "story": "Managing 12-Person Squad",
    "beat": "Full story",
    "tags": [
      "leadership",
      "cross-functional",
      "team management",
      "stakeholders"
    ],
    "text": "Warner | Managing 12-Person Squad\n\nWARNER | Managing 12-Person Squad\nTags: leadership, cross-functional, team management, stakeholders\nTO DRAFT: Led squad of 12 across product, engineering, data science. How she structured the team, ran sprints, managed stakeholders across 40 territories. Leadership style, conflict resolution.",
    "path": [
      "Warner",
      "Managing 12-Person Squad"
    ]
  },
  {
    "company": "Imbr",
    "story": "Co-founding + Payments (730 \u2192 30 days)",
    "beat": "Full story",
    "tags": [
      "founder",
      "0-to-1",
      "music",
      "payments",
      "ML matching",
      "disruption"
    ],
    "text": "Imbr | Co-founding + Payments (730 \u2192 30 days)\n\nIMBR | Co-founding + Payments (730 \u2192 30 days)\nTags: founder, 0-to-1, music, payments, ML matching, disruption\nTO DRAFT: Identified the problem (songwriters giving up half their royalties to publishers and PROs). Built rights platform, cut out middlemen, connected creators directly to DSPs. Payments from 730 days to 30 days. Songwriter share from 47% to 90%. 99% ML matching accuracy.",
    "path": [
      "Imbr",
      "Co-founding + Payments (730 \u2192 30 days)"
    ]
  },
  {
    "company": "Fox",
    "story": "Manager Quit 2 Weeks In",
    "beat": "Full story",
    "tags": [
      "adversity",
      "leadership",
      "ownership",
      "resilience",
      "ad tech"
    ],
    "text": "Fox | Manager Quit 2 Weeks In\n\nFOX | Manager Quit 2 Weeks In\nTags: adversity, leadership, ownership, resilience, ad tech\nTO DRAFT: Joined Fox as Principal PM. Manager quit 2 weeks in. Absorbed her responsibilities without the title or comp. Led digital platform serving 30M users across 6 properties. Drove $2M incremental ad revenue. AI-powered comment moderation. How she handled the situation.",
    "path": [
      "Fox",
      "Manager Quit 2 Weeks In"
    ]
  },
  {
    "company": "Cross-cutting",
    "story": "Disagreeing with Leadership",
    "beat": "Full story",
    "tags": [
      "conflict",
      "influence",
      "data-driven",
      "stakeholder management"
    ],
    "text": "Cross-cutting | Disagreeing with Leadership\n\nCROSS-CUTTING | Disagreeing with Leadership\nTags: conflict, influence, data-driven, stakeholder management\nTO DRAFT: Multiple examples available. Kunik: pushing back on fine-tuning vs fixing retrieval. Warner: disagreements on modernization approach. Pick 2-3 concrete examples with specific outcomes.",
    "path": [
      "Cross-cutting",
      "Disagreeing with Leadership"
    ]
  },
  {
    "company": "Cross-cutting",
    "story": "Earning Engineer Trust",
    "beat": "Full story",
    "tags": [
      "engineering collaboration",
      "credibility",
      "technical depth",
      "influence without authority"
    ],
    "text": "Cross-cutting | Earning Engineer Trust\n\nCROSS-CUTTING | Earning Engineer Trust\nTags: engineering collaboration, credibility, technical depth, influence without authority\nTO DRAFT: How she earns trust with engineers as a non-technical PM. Specific examples from Kunik (sitting in on architecture decisions, learning the pipeline), Warner (working with data science team on ML matching). The pattern across roles.",
    "path": [
      "Cross-cutting",
      "Earning Engineer Trust"
    ]
  }
]